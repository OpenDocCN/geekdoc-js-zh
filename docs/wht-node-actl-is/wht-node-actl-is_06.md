# 什么是缓冲区？

> 原文：[`www.thenodebook.com/buffers/what-is-buffer`](https://www.thenodebook.com/buffers/what-is-buffer)

我建议你喝杯咖啡，为接下来的几章做好准备。我们将揭开 Node.js 最基本、坦白说也是被误解最多的一个部分的神秘面纱。如果你曾经在自己的控制台中盯着 `<Buffer ...>` 感到一丝不安，那么你就在正确的位置。

嗯，我可以用简单的谷歌搜索到的信息来引用，比如说一个**“缓冲区是一个固定大小的内存块，bla... bla... bla...”**，然后就算完成了，但这样真的能理解吗？这不仅仅关于学习一个 API，这是关于重新连接你大脑的一部分，JavaScript 已经训练它只以文本的方式思考。

💡提示

掌握缓冲区不仅仅是针对 Node.js。字节数组、编码和内存管理的概念在系统编程中是通用的。如果你将来与 Go、Rust、C++ 或 Java 等语言一起工作，这些知识将给你带来显著的优势。

## 字符串之外的世界

作为 JavaScript 开发者，我们生活在一个舒适、明亮的世界里。我们的数据是有结构的。它以 JSON 的形式到来，我们将其解析为对象，操作字符串，然后再以 JSON 的形式发送出去。这是一个文本的世界，是 Unicode 字符的世界，是可读信息的世界。即使我们处理的是数字数组，它们通常也仅仅是数字，代表数量、分数或坐标。这就是我们的舒适区。

所以，想象一下你的老板来找你，带着一个新的项目。“我们需要一个高性能的 TCP 代理，”她说。“它只需要将一个连接上接收到的每一个字节原封不动地转发到另一个连接。没有检查，没有修改，只是纯粹、快速的数据传输。”或者可能是，“我们正在构建一个图像处理服务。第一步是读取上传的 JPEG 文件的前 512 个字节以提取 EXIF 元数据。”

你点头，打开你的编辑器，然后...你停了下来。

你如何表示那股原始图像数据流？那些 TCP 数据包？JavaScript 中哪种数据类型可以持有... *那种*？

任何经验丰富的 JavaScript 开发者头脑中首先浮现的想法可能是一个字符串。这是我们用来表示...嗯，一系列“东西”的唯一原始数据类型。它感觉像是完成这项工作的正确工具。

这就是我们在第一个、巨大的“哦哦”时刻遇到的。这不仅仅是在 JavaScript 中缺失的功能；这是语言设计的基本哲学与当前任务之间的根本不匹配。JavaScript 是在浏览器中诞生和成长的。它的整个世界观是由文档对象模型、用户事件和 AJAX 请求塑造的——一个由 HTML、CSS 和基于文本的数据格式主导的世界。

相反，Node.js 将 JavaScript 从那个舒适的浏览器沙盒中拉了出来，扔进了寒冷、严峻的服务器间现实。这是一个文件系统、网络套接字、加密操作和底层系统调用的世界。在这个世界里，通用的语言不是文本。它是字节。原始的、未经解释的、辉煌的字节。

本章是关于这种冲突的。我们将探讨 JavaScript 原生工具为什么不仅效率低下，而且处理二进制数据时实际上是有害的。我们不仅将学习`Buffer` API。我们将构建一个深入的基础心理模型，解释为什么`Buffer`最初必须被发明。我们将亲身体验这个问题，揭示使解决方案成为可能的巧妙内存架构，并看到 Node 的原始、专有的解决方案如何优雅地与现代 JavaScript 标准相结合。

但在我们看到字符串失败之前，我们必须非常清楚它们失败的原因。我们已经几次提到了“字节”这个词，但它实际上意味着什么？让我们暂停一下，进行必要的快速课程。忘记 JavaScript 一分钟。让我们深入到底层。

## 比特和字节

现代计算机中的所有东西，从你正在阅读的文本到最复杂的 3D 游戏，都是建立在极其简单的基石之上的：一个可以开或关的开关。就是这样。没有魔法。我们用 0 表示“关”，用 1 表示“开”。这个单一的信息片段，这个 0 或 1，被称为**比特**。它是计算中最小的数据单位。

```js
A single bit:
[ 0 ]  (Off)   or   [ 1 ]  (On)
```

单个比特相当有限。你用不了多少东西——是或否，真或假。要完成任何有用的任务，我们需要将它们组合在一起。按照数十年的计算机历史中形成的惯例，我们将它们分成八位的集合。

一个**8 位集合称为字节**。

```js
A single byte:
[0][1][0][0][1][0][0][1]
```

这是我们将要处理的基本构建块。当我们谈论“二进制数据”时，我们是在谈论这些字节的序列。一个 1 兆字节的文件简单地说就是大约一百万个这样的 8 位模式。

现在，最重要的概念是：**一个字节本身没有固有的意义**。它只是一个模式。字节`01001001`本身并不是字母'I'或蓝色或音符。它只是一个模式。要将这个模式变成有意义的，我们必须应用一个**解释**。这是我们即将看到的所有问题的根源。

### 字节作为数字

📌重要

这是本章最重要的概念。数据只是一系列数字。是您的代码通过应用解释（例如，“将这个数字视为 UTF-8 字符”或“将这个数字视为像素的颜色强度”）赋予了它意义。所有二进制数据错误都源于应用了错误的解释。

字节最直接的解释是作为数字。我们如何从 1 和 0 的模式中获得一个数字？我们使用二进制（基-2）数制。它的工作方式就像你每天使用的十进制（基-10）系统一样，但每个位置代表的不是 10 的幂（1s，10s，100s 等），而是 2 的幂。

让我们看看字节的结构。从右到左读取，位置具有递增的值：

```js
┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐
│  7  │  6  │  5  │  4  │  3  │  2  │  1  │  0  │
├─────┼─────┼─────┼─────┼─────┼─────┼─────┼─────┤
│  2⁷ │  2⁶ │  2⁵ │  2⁴ │  2³ │  2² │  2¹ │  2⁰ │
├─────┼─────┼─────┼─────┼─────┼─────┼─────┼─────┤
│ 128 │ 64  │ 32  │ 16  │  8  │  4  │  2  │  1  │
└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘
```

要找到字节所表示的数字，你只需将其中包含`1`的位置的值相加。让我们以前面的例子为例：`01001001`。

```js
128    64    32    16     8     4     2     1
┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐
│  0  │  1  │  0  │  0  │  1  │  0  │  0  │  1  │
└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘
 Value = (0 * 128) + (1 * 64) + (0 * 32) + (0 * 16) + (1 * 8) + (0 * 4) + (0 * 2) + (1 * 1)
 = 64 + 8 + 1
 = 73
```

因此，二进制模式`01001001`代表整数`73`。

使用 8 位，我们可以制作的最小数字是`00000000`，即 0。最大的是`11111111`，即`128 + 64 + 32 + 16 + 8 + 4 + 2 + 1 = 255`。这是一个需要记住的关键范围：一个字节可以表示从**0 到 255**的任何整数。这就是为什么你经常在低级编程中看到这个数字，比如 RGB 颜色值。

ℹ️注意

范围 0-255 是基本的。当你看到处理单个字节的 API（如通过索引访问缓冲区`buf[i]`）时，你读取和写入的值将始终在这个范围内。

### 字节作为字符

这正是事情变得有趣并直接与我们本章相关的地方。如果我们同意一个标准映射，我们就可以创建一个表格，其中说，“每当您看到数字 65，将其解释为字符'A'。当您看到 66，它是'B'。”

这正是**ASCII**（美国信息交换标准代码）的含义。它是一种解释方案。它是一种合同。

```js
A small slice of the ASCII "contract":
 Decimal | Binary   | Character
-------------------------------
 65    | 01000001 |    'A'
 66    | 01000010 |    'B'
 67    | 01000011 |    'C'
 ...   | ...      |    ...
 97    | 01100001 |    'a'
 98    | 01100010 |    'b'
 ...   | ...      |    ...
 32    | 00100000 |  (space)
```

因此，在 ASCII 解释下，我们的字节`01001001`（即数字 73）代表大写字母**'I'**。

这是关键。位没有改变。基础数字没有改变。只是我们对那个数字的解释改变了。这就是**字符编码**：一组将数字映射到字符的规则。ASCII 是一个简单的例子。我们很快就会看到的 UTF-8，是一套更复杂但功能更强大的规则集，可以表示世界上任何语言的几乎任何字符。

**解释 3：字节作为...其他任何事物**

同一个字节，`01001001`（数字 73），根据上下文可能代表无数其他事物：

+   **在图像文件中，**它可能代表单个像素的蓝色成分的强度。

+   **在声音文件中，**它可能是一个波形样本，定义了在时间微秒级时的扬声器位置。

+   **在网络上，**它可能是 IP 地址的一部分。

+   **在程序中，**它可能是一条机器代码指令，告诉 CPU 执行特定操作。

计算机不知道也不关心。它只看到`01001001`。提供上下文并应用正确解释的是软件，即程序——*您的代码*。我们即将见证的“字符串灾难”是应用错误解释的直接结果。

**扩展：序列和缩写**

我们很少单独处理一个字节。我们按顺序处理成千上万、数百万甚至数十亿个字节。用二进制形式写出它们是非常繁琐的。

```js
The word "HELLO" in binary (ASCII):
01001000 01000101 01001100 01001100 01001111
 'H'     'E'     'L'     'L'     'O'
```

这对人类来说难以阅读。为了使我们的生活更简单，我们使用缩写：**十六进制**（基数为 16）。十六进制使用 16 个符号：0-9 和 A-F。十六进制的魔力在于一个十六进制数字可以精确表示四个比特（一个“半字节”）。这意味着任何字节（8 比特）都可以由恰好两个十六进制数字完美表示。

```js
Binary -> Hex Mapping
0000 -> 0    1000 -> 8
0001 -> 1    1001 -> 9
0010 -> 2    1010 -> A
0011 -> 3    1011 -> B
0100 -> 4    1100 -> C
0101 -> 5    1101 -> D
0110 -> 6    1110 -> E
0111 -> 7    1111 -> F
```

让我们转换我们的“HELLO”序列：`01001000` -> `0100` 是 `4`，`1000` 是 `8` -> `48` `01000101` -> `0100` 是 `4`，`0101` 是 `5` -> `45` `01001100` -> `0100` 是 `4`，`1100` 是 `C` -> `4C`

因此，“HELLO”的十六进制表示为：`48 45 4C 4C 4F`。

这就是当你使用 Node 的 `console.log` 打印 Buffer 时你所看到的**确切**内容：`<Buffer 48 45 4c 4c 4f>`。Node 给你的是原始字节序列的方便、可读的十六进制表示形式。这不是不同的格式；这只是显示相同底层二进制数据的不同方式。

最后，关于大于 255 的数字怎么办？我们只需使用更多的字节。一个 16 位整数（两个字节）可以存储高达 65,535 的值。一个 32 位整数（四个字节）可以存储高达约 42 亿的值。但这引入了一个新的问题：如果一个 16 位数字由两个字节组成，比如说 `0x12` 和 `0x34`，我们在内存中按什么顺序存储它们？

`[0x12][0x34]` 或 `[0x34][0x12]`？

这是**字节序**的问题。存储最高有效字节先的系统（`[0x12][0x34]`）被称为**大端**。存储最低有效字节先的系统（`[0x34][0x12]`）被称为**小端**。网络通常使用大端（常称为“网络字节序”），而许多现代 CPU（如 Intel/AMD x86）是小端。这只是另一个约定，另一个软件必须同意以正确通信的解释规则。这就是为什么你会在后面看到像 `buf.readInt16BE()` 和 `buf.readInt16LE()` 这样的方法——你必须告诉 Node 使用哪种字节序进行解释。

ℹ️注意

字节序是处理来自不同来源的二进制数据（如网络流与本地文件）时的经典“陷阱”。如果你读取一个多字节数字并得到一个看起来非常不正确的值，不匹配的字节序是你应该首先检查的事情之一。

### 记忆十六进制字符的“A-C-F”技巧

不要试图一次性记忆所有六个字母及其对应的数字。你的大脑只需要锁定其中三个。**最佳方法是只记住 A、C 和 F**。

将这三个字母视为你的“锚点”。所有其他字母都围绕它们排列。**A 是 10**。这个很简单。它是第一个字母，紧**A**在数字 9 的后面。即 **A = 10**。

**C 是 12**。想象一个**时钟**（C 代表 Clock）。一个标准的时钟面有 **12** 个小时。即 **C = 12**。

**F 是 15**。用 **F** 来表示 **F**ifteen。或者，把它看作是单个十六进制数字可以持有的 **F**inal 或 **F**ull 值。所以，**F = 15**。

**那么，如何得到其他的（B、D、E）呢？** 它们只是你的锚点之间的 **数字**！

**B 是什么？** 它在 A（10）和 C（12）之间。它们之间的唯一数字是 **11**。所以，**B = 11**。或者更简单，**B** 是 `A + 1`，或者 `11`。**D 和 E 是什么？** 它们在 C（12）和 F（15）之间。对于 D，记住它是在 **C** 之后，而 **C** 是 12，所以 **D** 是 13。对于 **E**，它是在 **F** 之前，所以是 **14**。

就这样！只需记住三个关键字母及其简单的关联，你就可以立即找出所有其他的字母。

| 你的锚点 | 如何记忆 | “中间的” |
| --- | --- | --- |
| **A = 10** | 第一个字母，**A**fter 9 |
|  |  | **B = 11**（它在 A 和 C 之间） |
| **C = 12** | 一个 **C**lock 有 12 个小时 |  |
|  |  | **D = 13**（它在 C 之后） |
|  |  | **E = 14**（它在 F 之前） |
| **F = 15** | **F** 是 **F**ifteen / **F**ull |

### 如何理解十六进制字节

一个十六进制字节，如 `B7` 或 `4E`，实际上是在 **基数为 16** 的系统中表示的数字。虽然我们日常使用的数字是基数为 10（使用数字 0-9），但十六进制使用十六个符号（0-9 和 A-F）。理解它的关键是认识到一个字节 **总是由两个十六进制数字表示**，并且每个数字都有一个特定的位值。

#### 两个位值

将一个两位数的十六进制数字看作有两个列或“位置”。右边的数字在 **“个位”**（16⁰）中。它的值乘以 1。左边的数字在 **“十六位”**（16¹）中。它的值乘以 16。

| 十六位（值乘以 16） | 个位（值乘以 1） |
| --- | --- |
| 左边的数字 | 右边的数字 |

要找到总值，你需要计算每个位置的值并将它们相加。让我们将十六进制字节 **`A9`** 转换为常规的十进制数字。

`步骤 1` 将字节分解为其两个数字。左边的数字是 `A`。右边的数字是 `9`。

`步骤 2` 计算左边的数字（“十六位”）的值。首先，将十六进制字符转换为它的十进制数字：`A = 10`。现在，将这个数字乘以 16 - `10 × 16 = 160`。

`步骤 3` 计算右边的数字（“个位”）的值。十六进制字符 `9` 已经是十进制数字：`9`。将这个数字乘以 1 - `9 × 1 = 9`。

`步骤 4` 将两个值相加。`160 + 9 = 169`。

因此，十六进制字节 **`A9`** 表示的十进制数字是 **169**。别担心，你会习惯的，而且大脑开始找到转换十六进制值的模式可能只需要几周，甚至几天。

#### 另一个例子，转换 `C5`

让我们再做一个，让它更清晰。

`步骤 1` 分解数字。`C`（左）和`5`（右）。

`步骤 2` 将 `C` 转换为十进制：`C = 12`。乘以其位值：`12 × 16 = 192`。

`步骤 3` 将 `5` 转换为十进制：`5`。乘以其位值：`5 × 1 = 5`。

`步骤 4` 将它们加起来，即 `192 + 5 = 197`。

因此，十六进制字节**`C5`**是十进制数**197**。

**为什么它被使用？**这个系统对计算机来说效率非常高。一个字节由 8 位（0s 和 1s）组成。一个十六进制数字完美地代表 4 位，所以两个十六进制数字完美地代表一个字节的全部 8 位。对于一个人来说，阅读和写入`C5`比二进制的等效值`11000101`要容易得多。

### 将所有内容串联起来

好的，快速课程到此结束。让我们将其与 Node.js 联系起来。

当 `fs.readFileSync('logo.png')` 运行时，Node 从操作系统获取的是一系列原始的字节。它是一串 `01001001`s 和 `11101010`s。这些字节代表像素颜色、图像尺寸和压缩元数据，所有这些都遵循 PNG 文件格式规范。它们**不是**根据 ASCII 或 UTF-8 的规则被解释为文本。

我们即将探讨的核心问题是告诉 JavaScript 对这组数据进行错误解释规则应用的灾难性后果。我们即将要求它使用为人类文字设计的字典来读取用像素语言写的情书。结果，正如你将看到的，是混乱。

现在，有了对字节真正是什么的坚实基础，让我们看看它如何全部出错。

## 一个真实的为什么文本失败的演示

假设我们在项目目录中有一个简单的 PNG 图像文件，比如 `logo.png`。它是一个二进制文件。我们的任务很简单：将其读入内存，然后将其写回到一个新的文件中，`logo-copy.png`。这是一个简单的文件复制操作。

根据我们对 Node 的`fs`模块的现有知识，这种天真尝试看起来完全合理。

```js
// naive-copy.js
import fs from "fs";
import path from "path";
 const sourcePath = path.resolve("logo.png");
const destPath = path.resolve("logo-corrupted.png");
 console.log(`Reading from: ${sourcePath}`);
 try {
 // Let's try the obvious. Read the file into a string.
 // We have to provide an encoding, right? 'utf8' is standard.
 const data = fs.readFileSync(sourcePath, "utf8");
 console.log("File read into a string. Here is a sample:");
 console.log(data.slice(0, 50)); // Let's see what it looks like
 console.log(`\nWriting data back to: ${destPath}`);
 fs.writeFileSync(destPath, data);
 console.log("Copy complete. Or is it?");
} catch (err) {
 console.error("An error occurred:", err);
}
```

现在，让我们运行这个程序。你需要在同一目录下有一个 `logo.png` 文件。输出将类似于以下内容：

```js
Reading from: /path/to/your/project/logo.png
File read into a string. Here is a sample:
�PNG
�
����JFIFHH���ICC_PROFILE�0
 Writing data back to: /path/to/your/project/logo-corrupted.png
Copy complete. Or is it?
```

第一个表明事情严重错误的线索是样本输出。它是一堆奇怪的符号的混乱，最值得注意的是那些菱形问号：`�`。这不仅仅是一个随机字符；它是一个具有非常重要意义的特定 Unicode 字符，我们将在稍后解释。

但真正令人信服的证据出现在你检查文件系统时。你会发现一个新的文件，`logo-corrupted.png`。将其文件大小与原始的 `logo.png` 进行比较。损坏的版本几乎肯定会更小。如果你尝试用图像查看器打开它，它将失败。它已经损坏了。我们没有复制数据；我们实际上破坏了它。

那么，刚才发生了什么？这不是 Node.js 的 bug。这是一个对我们要求它做什么的根本性误解。

为了解开这个谜团，我们需要回顾并问一个关键问题：JavaScript 字符串究竟是什么？把它想象成一个字节数组很诱人，但正如我们刚才所确立的，那是不正确的。**JavaScript 字符串是一个不可变的字符序列**。内部，V8 引擎使用通常为 UTF-16 的格式来表示这些字符。关键的一点是，字符串是一个抽象层。它不是原始字节；它是根据一组语言和符号规则（Unicode）对原始字节的解释。

这把我们带到了问题的核心：我们传递给 `readFileSync` 的 `'utf8'` 参数。当我们提供这个编码时，我们不仅仅是告诉 Node 读取文件。我们发出了一个命令：“从 `logo.png` 读取一系列原始字节，并且我希望你将它们解码，将它们解释为有效的 UTF-8 文本序列。”

这是一个 UTF-8 的陷阱。PNG 文件是一个高度结构的二进制格式。它的字节表示像素、压缩元数据、调色板和校验和。它们不是用来表示文本的。让我们看看几乎任何 PNG 文件的第一个四个字节，被称为“魔数”，它标识了它是一个 PNG 文件。用十六进制表示，它们是 `89 50 4E 47`。

当 Node 的 UTF-8 解码器遇到字节 `0x89` 时，它会立即遇到问题。在 UTF-8 中，任何大于 `0x7F`（127）的字节值都表示多字节字符序列的开始。特定的值 `0x89` 在 UTF-8 规范中不是任何多字节序列的有效起始字节。解码器现在陷入了困境。它遇到了在 UTF-8 语言中没有意义的字节。

当解码器发现无效的字节序列时，它会做什么？它不能只是崩溃。它必须产生某种东西。所以，它发出 `U+FFFD`，这是官方的 Unicode “替换字符”。那就是你在控制台看到的 `�`。

⚠️警告

替换字符 `�` 的出现是一个红旗。它表示发生了不可逆的、有损的转换。解码器无法理解的原始字节序列已被丢弃并替换。您的数据现在永久损坏。

这是一个不可逆的、有损的转换。解码器丢弃了原始的 `0x89` 字节，并用表示 UTF-8 中 `�` 的三个字节（`EF BF BD`）替换了它。原始信息已经消失。永远消失。它对文件中每个不符合 UTF-8 严格规则的单独字节或字节序列都这样做。这就是为什么我们的 `logo-corrupted.png` 大小不同，为什么它充满了垃圾。我们没有存储文件的数据；我们存储了失败解码尝试的残骸。

现在，一个聪明的开发者可能会问，“但是等等，其他编码怎么办？如果我使用了 `'latin1'` 或旧的 `'binary'` 编码呢？”

这是一个很好的问题，它引出了更深层次的见解。让我们尝试使用`'latin1'`。`latin1`（或 ISO-8859-1）编码是特殊的，因为它定义了从 0 到 255 的字节值到前 256 个 Unicode 代码点的一对一映射。如果你尝试使用`'latin1'`复制脚本，往返可能实际上*会工作*。生成的文件可能与原始文件完全相同。

那么，问题解决了吗？绝对没有。这是一个危险且误导性的技巧。尽管它可能看起来保留了数据，但你仍然迫使 JavaScript 引擎将你的二进制数据*作为文本*处理。现在它是一个字符串。这意味着 V8 可能会对其执行针对文本设计的内部优化，而不是任意二进制数据。更重要的是，这在语义上是不正确的。你向运行时撒谎，关于你的数据代表什么。你拿着一系列像素数据，告诉引擎，“这是一系列欧洲语言字符的序列。”当你将这个“字符串”传递给期望实际文本的其他 API 时，可能会导致微妙的、令人恐惧的错误。

🚨警告

使用`'latin1'`来“保留”字符串中的二进制数据是一种脆弱的技巧，应该避免使用。这在语义上是不正确的，并且可能导致与其他 API 或未来 JavaScript 引擎优化不预期的行为。正确的解决方案是完全不使用字符串来处理二进制数据。

问题不在于编码的选择。问题在于解码的行为本身。我们不想将字节解释为文本。我们想要保留字节，原始且未被污染。我们需要一个表示纯、未解释的字节序列的数据结构。而 JavaScript 本身并没有这样的结构。

## 为什么 Node 需要自己的内存

好的，所以我们已经确定字符串不是完成这项工作的正确工具。我们需要一个新的工具，一个本质上只是字节数组的数据库结构。在我们介绍 Node 的解决方案，即`Buffer`之前，我们必须理解一个关键的系统架构问题。问题不仅仅是`Buffer`是什么，而是它在内存中的位置。答案是真正巧妙的。

让我们快速回顾一下 V8 的世界，我们在上一章中提到了它。V8 引擎在其称为 V8 堆的区域中管理其内存。这是一个高度复杂的环境，由世界级的垃圾收集器（GC）不断监控和清理。GC 针对一个非常特定的负载进行了优化：管理许多小型、高度互联的 JavaScript 对象的生命周期。它在清理字符串、对象、数组和闭包方面非常出色，这些对象具有短到中等的生命周期。

现在，让我们介绍一个对垃圾收集器来说的噩梦场景。想象一下，我们的图像处理服务不仅需要读取 512 字节，而是需要将整个 500MB 的视频文件加载到内存中进行分析。

如果我们要设计一个新的“字节数组”数据类型，它位于 V8 堆上，分配那个 500MB 的对象将是第一个问题。但真正的灾难将在垃圾回收期间发生。V8 的 GC，尤其是在主要收集周期中，需要遍历整个活动对象图，并在许多情况下移动它们以压缩内存并防止碎片化。

想象 GC 遇到我们的 500MB 视频对象。它必须扫描它，确定是否有任何东西指向它，然后可能将那个*整个半 GB 的内存块*从一个位置复制到另一个位置。这将触发一次巨大的、冻结应用程序的“停止世界”暂停。如果主线程被锁定在内存管理中，那么你的服务器将完全无响应数秒。如果主线程被锁定在内存管理中，那么事件循环的所有巧妙之处都将无用。V8 的堆及其垃圾回收器根本不适合处理大型、连续的静态数据块。

这就是 Node.js 做出卓越架构决策的地方。

Node 的`Buffer`是在一个完全不同的内存空间中分配的。它们存在于 V8 管理的堆*之外*。

📌重要

这是关键架构决策，使得在 Node.js 中实现高性能的二进制数据处理成为可能，而不会损害垃圾回收器。

那么，我们如何从 JavaScript 代码中与之交互呢？

这是巧妙设计的第二部分。你在 JS 代码中操作的`Buffer`对象本身并不是内存块。它只是一个小型、轻量级的 JavaScript 对象，充当一个*句柄*或*指针*。这个小的句柄对象*确实*存在于 V8 堆上，并由垃圾回收器管理。它包含有关数据的元数据，如其长度，以及最关键的是，指向实际数据块内存地址的内部指针，该数据块位于 V8 之外。

让我们构建这个心理模型：

**原始数据块**，位于你电脑 RAM 中的某个地方的一个大、连续的内存块，由 Node 的 C++核心管理，而不是 V8。这是你的文件或网络数据包实际字节所在的地方。

**JS 句柄**，一个位于 V8 堆上的小型 JavaScript 对象。它易于创建，并且对于 GC 来说易于跟踪。它持有原始数据块的地址。

当垃圾回收器运行时，它只能看到小的句柄对象。它可以跟踪它，移动它，并以极高的效率最终将其垃圾回收。它永远不需要触及那个庞大的 500MB 数据块。当 JS 句柄最终被回收时，Node 的 C++层通过一种特殊机制（弱引用）得到通知，然后它知道可以安全地释放相关的原始内存块，将其返回给操作系统。

这种设计是两者的最佳结合。我们得到了在 JavaScript 中处理对象的安全性和便利性，而大型二进制数据的内存管理则由更适合它的系统来处理。

但这种设计并非魔法，它也有其权衡。直接从操作系统分配内存通常比 V8 为其堆上的小对象高度优化的“跳跃指针”分配操作慢。更重要的是，通过跳出 V8 的完全自动化的内存管理，我们引入了一个新的潜在问题类别。我们现在正在与一个行为不同的内存系统交互，理解这个边界是编写高性能、无泄漏的 Node 应用程序的关键。我们将在后面的章节中更详细地探讨这些性能和内存泄漏的影响，但就目前而言，关键的认识是这个双堆模型。它是使其他一切成为可能的基础。

ℹ️注意

双堆模型有性能影响。创建许多小的缓冲区可能比创建许多小的 JS 对象慢，因为调用 C++分配内存的开销。Node 有一些优化（如内存池）来减轻这一点，我们将在后面的章节中介绍。

## `Buffer` - Node 的解决方案

现在我们已经理解了问题（字符串是危险的）和架构解决方案（堆外内存），我们终于可以谈论这个工具本身了：`Buffer`。

你可能正在问的一个问题，尤其是如果你有现代浏览器 API 的经验，是“为什么 Node 没有直接使用`ArrayBuffer`和像`Uint8Array`这样的类型数组？”这是一个非常好的问题，答案是简单的历史。当 Node.js 在 2009 年由 Ryan Dahl 创建时，`ArrayBuffer`和类型数组套件并不是 JavaScript 语言或 V8 引擎的稳定、标准化的部分。它们是实验性提案，离可靠到足以用于生产使用还有好几年。

但是 Node 有一个紧迫的需求。Node 的整个目的就是实现服务器端 I/O。如果你无法处理原始请求体，你怎么能构建一个 HTTP 服务器呢？如果你无法持有文件数据，你怎么能与文件系统交互呢？Node 从第一天起就必须解决二进制数据问题。因此，Ryan Dahl 和其他早期贡献者做了任何实用工程师都会做的事情：他们发明了自己的解决方案。`Buffer`就是在这种纯粹的需求下诞生的。

那么，`Buffer`本质上是什么？它是一个固定大小的、可变的字节序列。把它想象成一个直接的低级视图，可以看到一块内存。它表现得就像一个字节数组，其中每个元素都是一个从 0 到 255 的整数（单个字节的范围）。

让我们看看我们如何创建和使用它们。创建缓冲区（`new Buffer()`）的旧方法已经被弃用，因为它非常模糊，具有危险性。现代 API 更安全，也更明确。

⚠️警告

你可能会在旧代码库或在线示例中看到`new Buffer()`。这个构造函数已被弃用，不应在现代代码中使用。它的行为取决于其参数的类型，这导致了严重的安全漏洞（例如，意外暴露未初始化的内存）。始终使用`Buffer.alloc()`或`Buffer.from()`。

你将使用两个主要的静态方法：`Buffer.alloc()`和`Buffer.from()`。

`Buffer.alloc(size)`是创建一个新“干净”缓冲区的方法。你告诉它你需要多少字节，它就会给你一个相应大小的缓冲区，并用零填充。

```js
// create-buffer.js
 // Allocate a new Buffer of 10 bytes.
const buf1 = Buffer.alloc(10);
console.log(buf1);
// -> <Buffer 00 00 00 00 00 00 00 00 00 00>
```

它被零填充的事实很重要。这被称为“零化”内存。出于安全考虑，`Buffer.alloc()`默认会这样做。当 Node 从操作系统请求内存时，操作系统可能会给它一块之前由其他进程使用的内存。那块内存可能包含敏感数据——密码、私钥，等等。通过用零覆盖整个块，`Buffer.alloc()`确保你从一个干净的起点开始，并且不会意外泄露旧数据。有一个“不安全”版本，`Buffer.allocUnsafe()`，出于性能原因跳过了这个零填充步骤，但只有当你确信你将立即用你自己的数据覆盖整个缓冲区时才应使用它。

🚨注意

请务必谨慎使用`Buffer.allocUnsafe()`。它之所以更快，是因为它不会初始化分配的内存。这意味着新的 Buffer 可能包含来自应用程序其他部分或其他进程的旧敏感数据。只有在你能够保证在分配后立即完全覆盖整个内存空间时，才应使用它。

另一个工作马是`Buffer.from(thing)`。这是一个灵活的方法，可以从现有数据创建缓冲区。这就是我们解决原始字符串问题的方法。

```js
// buffer-from.js
 // 1\. From a string
const bufFromString = Buffer.from("hello world", "utf8");
console.log(bufFromString);
// -> <Buffer 68 65 6c 6c 6f 20 77 6f 72 6c 64>
// This is the correct way to convert text into its binary representation.
 // 2\. From an array of byte values
const bufFromArray = Buffer.from([0x68, 0x65, 0x6c, 0x6c, 0x6f]);
console.log(bufFromArray);
// -> <Buffer 68 65 6c 6c 6f>
console.log(bufFromArray.toString("utf8"));
// -> "hello"
 // 3\. From another Buffer (creates a copy)
const bufCopy = Buffer.from(bufFromString);
bufCopy[0] = 0x78; // Change the 'h' to an 'x'
console.log(bufCopy.toString("utf8")); // -> "xello world"
console.log(bufFromString.toString("utf8")); // -> "hello world" (original is unchanged)
```

注意`Buffer.from('hello world', 'utf8')`是如何与我们之前失败的操作的逆操作。我们不是将二进制数据破坏性地解码成字符串，而是正确地将字符串编码成其底层的二进制（UTF-8）表示形式。这是完成这项工作的正确工具。

一旦你有了缓冲区，你就可以直接与之交互。它感觉就像一个标准的 JavaScript 数组。

```js
// manipulate-buffer.js
const buf = Buffer.from("hey");
 // Read a byte at a specific index
console.log(buf[0]); // -> 104 (ASCII code for 'h')
console.log(buf[1]); // -> 101 (ASCII code for 'e')
 // Write a byte to a specific index
buf[1] = 0x6f; // 0x6f is the hex code for 'o'
console.log(buf.toString("utf8")); // -> "hoy"
```

这种直接、类似数组的访问方式简单而强大。但正是在这里，`Buffer`模块真正大放异彩，并展现了其服务器端遗产。它提供了一层符合人体工程学的辅助方法，这些方法在标准的浏览器 TypedArrays 中不可用，专为在 Node 中执行的任务而设计。

最常见的任务之一是将二进制数据转换为文本表示形式，用于记录、调试或传输。`buf.toString()`方法是你的好朋友。

```js
const secretData = Buffer.from("my-super-secret-password");
 // Represent the data as UTF-8 (the default)
console.log(secretData.toString()); // -> "my-super-secret-password"
 // Represent it as hexadecimal - very common for debugging
console.log(secretData.toString("hex"));
// -> 6d792d73757065722d7365637265742d70617373776f7264
 // Represent it as Base64 - common for transport in text-based formats like JSON or XML
console.log(secretData.toString("base64"));
// -> bXktc3VwZXItc2VjcmV0LXBhc3N3b3Jk
```

关键的是，这是一个安全、受控的数据文本表示。这不是我们之前看到的破坏性解码。我们没有丢失信息；我们只是选择如何显示它。

另一方面是写入数据。`buf.write()`方法是将字符串数据精确放置到更大的二进制结构中的强大工具。想象一下你正在手动构建一个 HTTP 响应。

```js
const responseBuffer = Buffer.alloc(128);
 // Write the first part of the header
let offset = responseBuffer.write("HTTP/1.1 200 OK\r\n");
// The write method returns the number of bytes written, which we use as the new offset
 // Write the next header
offset += responseBuffer.write("Content-Type: text/plain\r\n", offset);
 // And so on...
console.log(responseBuffer.toString("utf8", 0, offset));
/*
HTTP/1.1 200 OK
Content-Type: text/plain
*/
```

最后，我想给你快速浏览一下我们将在以后更深入探讨的内容。大多数二进制协议不仅涉及字节，还涉及多字节数字：16 位整数、32 位浮点数等。`Buffer`有一系列用于此的方法，如`buf.readInt16BE()`和`buf.writeInt16BE()`。`BE`代表大端序，它指的是字节顺序——二进制数据中的一个关键概念。这些方法允许你直接从缓冲区中提取一个两字节的数字，而无需手动位移动，这对于解析任何非平凡的二进制格式至关重要，从 JPEG 头到数据库线协议。

这丰富、人性化的 API 使得`Buffer`多年来对 Node.js 开发者来说不可或缺。它是一个为服务器端工作量身定制的定制工具。但 JavaScript 语言标准最终赶上了，这让我们来到了现在的状态。

## 缓冲区和类型化数组趋同

到目前为止，如果你一直在浏览器中使用现代 JavaScript，可能有一个想法一直在困扰着你：“这个`Buffer`东西，由于其固定长度和字节级访问……它看起来和闻起来非常像`Uint8Array`。”

你绝对是，100%正确的。

而关于现代 Node.js 中的缓冲区，这里有一个最重要的理解：**`Buffer`类*是*标准 JavaScript `Uint8Array`的子类。**

📌重要

这对于互操作性来说是一个变革。Node.js 的`Buffer`是一个`Uint8Array`。这意味着你可以将一个`Buffer`传递给任何期望`Uint8Array`的现代 API（在 Node 或浏览器兼容的库中），并且它将无缝工作。你得到了两者的最佳之处：Node 强大的、人性化的 API 和与网络标准的兼容性。

这并不是一直如此。在 Node 的早期，`Buffer`是完全独立、专有的东西。但随着`TypedArray`规范的成熟并成为 V8 和 JavaScript 的核心部分，Node.js 核心团队做出了一个英明的决定。大约从 Node.js v3 开始，他们将`Buffer`重构为从`Uint8Array`继承。

让我们来证明它。

```js
// buffer-is-a-uint8array.js
const buf = Buffer.alloc(10);
 console.log(buf instanceof Buffer); // -> true
console.log(buf instanceof Uint8Array); // -> true
```

这是拼图的关键部分。它弥合了 Node 特定世界和网络标准之间的差距。这意味着任何编写为接受`Uint8Array`的 API，无论在哪个库中，都将无缝接受 Node.js 的`Buffer`。你不需要在它们之间进行转换。缓冲区*就是*一个`Uint8Array`。

因此，如果它只是一个 `Uint8Array`，为什么我们仍然有 `Buffer` 名称和特殊 API 呢？因为它是一个 *子类*。它是一个增强的、专门的版本。一个 `Buffer` 实例免费获得你可能从浏览器中知道的所有的标准 `Uint8Array` 方法（`.slice()`、`.subarray()`、`.map()`、`.filter()` 等），*加上* 我们刚刚探索的整个用户友好、服务器端优化的 API（`.toString('hex')`、`.write()`、`.readInt16BE()` 等）。

它确实是两者的最佳结合。你既获得了与网络平台标准的兼容性，又获得了用于核心服务器开发的强大工具。

但我们还需要揭示这个内存模型的最后一层。`Buffer` 和 `Uint8Array` 本身也是抽象。它们只是对更深层次、更基本对象的视图：`ArrayBuffer`。

让我们可视化完整的层次结构 -

1.  **`ArrayBuffer`** 是原始的、不可访问的内存块本身。你不能直接从 `ArrayBuffer` 中读取或写入字节。它几乎没有方法。它不知道它应该被解释为 8 位整数、32 位浮点数还是其他任何东西。它只是 *是* 字节。它代表了资源。

1.  **`TypedArray` 视图（`Uint8Array`、`Int16Array` 等）和 `Buffer` 视图**是你在 `ArrayBuffer` 上放置的“透镜”或“窗口”，以赋予它意义并提供操作它的 API。一个 `Uint8Array` 告诉 JavaScript 引擎，“将这个底层内存块解释为一系列 8 位无符号整数。”一个 `Buffer` 做同样的事情，但添加了自己的特殊方法。

当你调用 `Buffer.alloc(10)` 时，Node 实际上在底层执行了两个步骤。首先，它分配了一个 10 字节的原始 `ArrayBuffer`（这是位于 V8 堆上的内存）。然后，它创建了一个指向该 `ArrayBuffer` 的 `Buffer` 实例（视图），并将其返回给你。

我们也可以证明这个联系。每个 `Buffer` 实例都有一个 `.buffer` 属性，它让你可以访问其底层的 `ArrayBuffer`。

```js
// buffer-and-arraybuffer.js
const buf = Buffer.from("abc");
 // Get the underlying ArrayBuffer
const arrayBuf = buf.buffer;
 console.log(arrayBuf);
// -> ArrayBuffer { [Uint8Contents]: <61 62 63>, byteLength: 3 }
 console.log(arrayBuf instanceof ArrayBuffer); // -> true
```

将内存（`ArrayBuffer`）与视图（`Buffer` 或 `Uint8Array`）分离的概念具有强大的影响：你可以对同一块内存有多个视图。

```js
// shared-memory.js
const arrayBuf = new ArrayBuffer(4); // A raw slab of 4 bytes
 // Create a view that interprets all 4 bytes as 8-bit integers
const view1_uint8 = new Uint8Array(arrayBuf);
 // Create a view that interprets all 4 bytes as a single 32-bit integer
const view2_int32 = new Int32Array(arrayBuf);
 view1_uint8[0] = 0xff; // Set the first byte to 255
view1_uint8[1] = 0xff; // Set the second byte to 255
view1_uint8[2] = 0xff; // Set the third byte to 255
view1_uint8[3] = 0x7f; // Set the fourth byte to 127
 // Now, read the *same memory* through the 32-bit integer view
// On a little-endian system, this will be interpreted as 0x7FFFFFFF
console.log(view2_int32[0]); // -> 2147483647
```

通过一个视图更改字节会立即反映在另一个视图上，因为它们只是对同一底层内存的不同解释。这是一种高级技术，但它直接源于这种内存架构，并且对于需要处理复杂二进制数据而不创建不必要的副本的高性能库来说是基本的。

💡提示

在单个 `ArrayBuffer` 上创建多个视图是一种强大的优化技术。它允许你以不同的方式解释相同的二进制数据（例如，作为混合整数和浮点数的结构），而不需要任何数据复制，这可以带来显著的性能提升。

## 结论

让我们喘口气，回顾一下我们刚刚走过的旅程。我们从一个简单、实用的任务开始——处理二进制文件——并立即陷入了一个 JavaScript 舒适的基于文本的世界和系统编程残酷的基于字节的现实之间的鸿沟。

我们亲眼目睹了显而易见的工具——字符串——如何彻底失败，不仅表现不佳，还通过试图将其强加于其上的语言解释来主动破坏我们的数据。这促使我们寻找解决方案，并在 Node 的核心架构中找到了它：一个巧妙的双堆内存模型，将大型、静态的二进制数据保持在 V8 垃圾收集器之外，防止了灾难性的性能问题。

我们遇到了`Buffer`类，这是 Node 的原始、实用的工具，它拥有丰富的、面向服务器的 API。最终，我们看到了这个曾经专有的解决方案是如何完美地集成到现代 JavaScript 生态系统中的，成为标准`Uint8Array`的一个专用子类，所有这些都建立在基本的`ArrayBuffer`之上。

如果要从整个章节中提炼一句话，那就是这句话——**Node.js 的`Buffer`是一个性能优化的、面向服务器的`Uint8Array`子类，它表示对在 V8 垃圾回收堆外分配的原始内存块的视图。**

句子中的每一部分现在都是你深刻理解的。你知道为什么它需要位于 V8 堆之外，你知道为什么它不能是一个字符串，你也知道它与你在浏览器中可能使用的现代标准之间的关系。

这种理解不仅仅是学术性的。它是你在 Node.js 中执行几乎所有高性能任务的绝对基石。现在，我们已经对 Node 如何表示静态的二进制数据块有了稳固的心理模型，我们终于可以着手处理 Node 最强大的 I/O 抽象：流。

在下一章中，我们将看到数据是如何在 Node 应用程序中流动的，不是作为一个巨大的整体，而是作为一系列 Buffer 的片段。掌握这种数据流是构建可扩展、内存高效的系统的最重要的技能。我们刚刚弄清楚**Buffer 是什么**。现在，让我们去探索你可以用它们构建什么。
