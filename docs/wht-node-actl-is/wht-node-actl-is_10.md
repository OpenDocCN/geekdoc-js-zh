# 流的基石

> 原文：[`www.thenodebook.com/streams/foundation-of-streams`](https://www.thenodebook.com/streams/foundation-of-streams)

在我们使用 Node.js 流编写任何一行代码之前，我们需要理解它们解决的基本问题。这不是 Node.js 或 JavaScript 独有的问题，甚至也不是计算机科学独有的问题。这是一个与计算机一样古老的问题：**我们如何处理大于可用内存的数据？**

这个问题可能看起来很简单，但它的答案塑造了操作系统的架构、数据库、网络协议，以及几乎处理大规模真实世界数据的每个系统的架构。Node.js 流不是一个任意的 API 设计选择。它们是对物理内存限制和 I/O 操作现实的直接、必然的回应。

## 大数据问题

让我们从一种现实场景开始。你正在构建一个需要处理上传文件的网络服务。用户可以上传图片、视频、文档——任何类型的文件。该服务必须读取这些文件，可能以某种方式对其进行转换（压缩图片、从视频中提取元数据、扫描病毒），然后存储它们或发送到其他地方。

最直接的方法——即我们首先想到的方法——是这样的：将整个文件作为一个单一的 Buffer 读入内存，在该 Buffer 上执行操作，然后写入结果。在代码中，这看起来很简单：

```js
const data = await fs.readFile("input.mp4");
const processed = transform(data);
await fs.writeFile("output.mp4", processed);
```

三行。干净。易于推理。对于小文件，这种方法完美适用。但当一个用户上传一个 2GB 的视频文件，或者一个 10GB 的数据库转储文件时，会发生什么？突然之间，你的简单程序必须分配 2GB 的内存来仅保存那个文件。如果有十个用户同时上传文件，你需要 20GB 的内存。这种方法无法扩展。

但问题不仅仅在于内存容量。即使你的服务器有 128GB 的 RAM，将整个 2GB 文件加载到内存中意味着你必须等待整个文件从磁盘（或通过网络）读取完毕，你才能开始处理它。如果读取该文件需要 5 秒，那么你的程序在处理第一个字节之前将闲置 5 秒。然后，在处理完成后，你必须将整个 2GB 写回磁盘或通过网络，再次等待整个写入操作完成。程序在数据流上本质上是同步的：先读取所有内容，然后处理所有内容，最后写入所有内容。

这种方法效率低下。当你等待磁盘提供文件的最后一个兆字节时，你本可以处理第一个兆字节。当你正在处理文件的中间部分时，你本可以将处理好的开头写入输出。读取、处理、写入这些操作可以并发发生，在时间上重叠。但“将所有内容读入内存”的方法使得这种并发变得不可能。

### 顺序处理

操作一个接一个发生——每个操作必须完成才能开始下一个

读取 5 秒处理 3 秒写入 4 秒总时间 12 秒

## 为什么分块？

基本洞察是这样的：**我们不需要一次性将整个数据集保存在内存中以便处理**。我们只需要保存我们当前正在工作的部分。

考虑另一种方法。如果我们只读取文件的一小部分——比如说，64 千字节——到内存中会怎样？我们处理这 64 千字节。我们写入结果。然后我们读取下一个 64 千字节，处理它们，写入结果，以此类推，直到整个文件被处理。

这种分块处理解决了这两个问题。首先，我们的内存使用现在由分块大小限制，而不是文件大小。处理 2GB 文件在任何给定时刻只需要 64KB 的内存。其次，操作现在可以重叠。当我们正在处理分块 N 时，操作系统可以在后台从磁盘读取分块 N+1。当我们正在将处理后的分块 N 写入输出时，我们可以同时处理分块 N+1。

### 内存使用比较

处理 2GB 文件：完整方式与分块方式

#### 将整个文件加载到内存中

一次性加载所有内容

内存使用 1200MB 峰值内存：2000 MB 状态：危险

#### 分块处理

分块处理

内存使用 64MB 峰值内存：64 MB 状态：安全效率提升 31x 分块处理需要的内存更少

但是这种分块方法引入了新的复杂性。我们必须管理分块流。我们必须决定何时读取下一个分块，何时处理它，何时写入它。我们必须处理这种情况，即分块的生产者（文件系统、网络）比消费者（我们的处理逻辑）快，或者反之亦然。我们必须确保如果在处理过程中发生错误，我们正确地清理资源。我们必须在数据流结束时发出信号。

这就是**流范式**介入的地方。流是管理分块数据流的抽象。它处理读取分块、在必要时缓冲它们，并将它们交付给您的处理逻辑的机制。它提供了一种结构化的方式来思考和实现分块、异步数据处理。

## 流的两种基本模型

在流系统中组织数据流有两种根本不同的方式。这些模型并不特定于 Node.js。它们代表了两种关于谁控制数据流的相反哲学，并且出现在许多不同的编程环境和范例中。

第一个模型是基于**推送的流**。在这个模型中，数据的生产者主动将分块推送到消费者。生产者决定何时发送数据。消费者在生产者选择发送数据时接收数据。这是事件驱动系统的模型。生产者发出事件，消费者对这些事件做出反应。

第二种模型是 **基于拉取的流**。在这个模型中，消费者主动从生产者请求数据块。消费者决定何时准备好更多数据，并明确请求它。生产者响应这些请求。这是基于迭代器的系统模型。消费者遍历数据源，一次拉取一个值。

#### **PUSH**: 生产者控制

生产者决定何时发送数据

在 **CONTROL** PRODUCER 中设置定时器 `setInterval(() => emit())` 等待... **REACT** SCONSUMER 监听 ('data', handle) 关键特性✓ 多个消费者可以监听✗ 消费者可能会过载

#### **PULL**: 消费者控制

消费者决定何时请求数据

**WAIT** SPRODUCER 函数 *generate* 空闲... **IN CONTROL** CONSUMER 迭代器.next() 关键特性✓ 自然背压控制✓ 可实现懒加载✗ 通常只有一个消费者

这两种模型具有不同的特性，不同的权衡，以及不同的用例。正如我们将看到的，Node.js 流尝试将这两种模型结合成一个单一、灵活的抽象。但在我们理解这种混合模型之前，我们必须首先理解其纯形式：推送和拉取。

## 推送架构

推送模型在软件设计中有着深厚的根源。它在 1994 年的 "Gang of Four" 书中记录的经典设计模式 **观察者模式** 中得到了形式化。观察者模式描述了对象之间的一对多依赖关系：当主题（可观察的）状态改变时，所有它的观察者（订阅者）都会自动收到通知。

在流处理的上下文中，主题是数据源，观察者是该数据消费者。当数据源有新数据可用时，它会通过将数据推送到所有已注册的消费者来通知所有注册的消费者。

在 Node.js 中，基于推送系统的基本构建块是 `EventEmitter` 类。这个类，你在学习事件循环和异步原语时已经遇到过，提供了一个简单但强大的机制来实现观察者模式。

### 推送流中的观察者模式

一个 `EventEmitter` 同时向多个监听器广播

**AUTO-PLAYING** - 下一个触发将在 2 秒后 **EVENT EMITTER**

#### 主题

`stream.emit('data', chunk)` 空闲 - 等待... **LISTENER 1** 观察者⏳ 等待... **LISTENER 2** 观察者⏳ 等待... **LISTENER 3** 观察者⏳ 等待... 代码示例

```js
// All three listeners receive the same event
stream.on('data', (chunk) => {
  console.log('Listener 1:', chunk);
});

stream.on('data', (chunk) => {
  console.log('Listener 2:', chunk);
});

stream.on('data', (chunk) => {
  console.log('Listener 3:', chunk);
});

// When stream emits, ALL listeners fire!
stream.emit('data', buffer); // → All 3 log
```

让我们从零开始构建一个简单的基于推送的流，使用 `EventEmitter`。这不会是一个生产就绪的流实现 - Node.js 已经提供了这样的实现 - 但自己构建它将阐明推送模型的机制。

```js
import { EventEmitter } from "events";
 class SimplePushStream extends EventEmitter {
 constructor(data) {
 super();
 this.data = data;
 this.index = 0;
 }
 start() {
 this._pushNext();
 }
 _pushNext() {
 if (this.index >= this.data.length) {
 this.emit("end");
 return;
 }
 const chunk = this.data[this.index++];
 this.emit("data", chunk);
 setImmediate(() => this._pushNext());
 }
}
```

这个简单的类扩展了 `EventEmitter` 并实现了推送流。它在构造函数中接受一个数据块数组。当调用 `start()` 时，它通过触发 `data` 事件开始向任何监听器推送块。当所有块都已推送后，它触发一个 `end` 事件以表示完成。

消费者通过注册事件监听器来使用此流：

```js
const stream = new SimplePushStream([1, 2, 3, 4, 5]);
 stream.on("data", (chunk) => {
 console.log("Received:", chunk);
});
 stream.on("end", () => {
 console.log("Stream ended");
});
 stream.start();
```

这就是推送模型的核心。数据流主动将数据推送给消费者。消费者不需要请求数据；它只是在数据到达时做出反应。

现在，你可能想知道在`_pushNext()`方法中使用`setImmediate()`的原因。这对逻辑来说并非绝对必要，但对行为很重要。如果没有`setImmediate()`，所有数据都会在调用`start()`时同步地在一个紧密的循环中推送。通过使用`setImmediate()`，我们确保每个数据块都在单独的事件循环中推送。这给了事件循环处理其他事件的机会，并防止我们的流独占 CPU。这是一种简单的让步形式，你将在 Node.js 的异步架构中反复看到这种模式。

## 推送模型的优点和局限性

推送模型有几个优点。首先，它在概念上很简单。生产者决定何时生产数据，消费者只是做出反应。这自然映射到事件驱动架构，这在 Node.js 中很普遍。

第二，当生产者和消费者以相似的速度运行时，推送模型可以非常高效。如果生产者可以像消费者一样快地生成数据，数据就会以最小的缓冲顺畅流动。

第三，推送模型允许有多个消费者。因为生产者发射事件，任何数量的监听器都可以订阅这些事件并接收相同的数据流。这种扇出模式在观察者模式中很自然。

然而，推送模型有一个基本问题：**背压**。如果生产者比消费者快会发生什么？在我们上面的简单实现中，生产者尽可能快地推送数据，不管消费者是否准备好。如果消费者需要时间来处理每个数据块——也许它正在向慢速磁盘写入或进行网络请求——生产者会继续推送更多数据。这些数据块必须在某处缓冲，等待消费者处理。缓冲区会无限增长，消耗内存，直到最终程序因内存耗尽而崩溃。

在基于生产的推送系统中，我们需要一个机制，让消费者向生产者发出信号：“我还没有准备好接收更多数据。请慢一点。”这就是背压。消费者通过向生产者施加反向压力来调节数据流。在基于推送的系统中实现背压并非易事。消费者必须有一种方式告诉生产者暂停，而生产者必须尊重这个信号。这需要生产者和消费者之间比仅仅发射事件更复杂的合同。

Node.js 流实现了背压，正如我们将在后面的章节中看到的。但这里的重点是，背压并不是从纯推送模型自然产生的。它必须作为额外的复杂层添加。

### 背压问题

快速生产者压倒慢速消费者——缓冲区无限增长

快速产生 发射块快速 慢速消费者 慢慢处理 缓冲状态 0 / 8 块 缓冲在安全范围内

## 拉取架构

拉取模型反转了控制流。不是生产者将数据推送到消费者，而是消费者从生产者那里拉取数据。消费者决定何时准备好下一个块，并显式请求它。

在 JavaScript 中，拉取模型在 **迭代器** 和 **可迭代** 协议中得到正式化。这些协议定义了对象按需产生值的标准方式。你可能已经使用了迭代器，但没有深入思考。当你在一个数组上写 `for...of` 循环时，你正在使用数组的内置迭代器。

让我们检查迭代器协议。迭代器是一个具有 `next()` 方法的对象。每次调用 `next()` 都会返回一个具有两个属性的对象：`value`（序列中的下一个项）和 `done`（一个布尔值，指示序列是否完成）。

这里是一个简单的基于拉取的流，实现为一个迭代器：

```js
class SimplePullStream {
 constructor(data) {
 this.data = data;
 this.index = 0;
 }
 next() {
 if (this.index >= this.data.length) {
 return { done: true };
 }
 return { value: this.data[this.index++], done: false };
 }
}
```

消费者通过显式调用 `next()` 来使用此流，以拉取每个块：

```js
const stream = new SimplePullStream([1, 2, 3, 4, 5]);
 let result = stream.next();
while (!result.done) {
 console.log("Pulled:", result.value);
 result = stream.next();
}
```

这是拉取模型的核心。消费者处于控制地位。它在准备好时拉取数据。生产者只需响应这些拉取请求。

## 生成器和可迭代协议

JavaScript 为实现迭代器提供了语法糖：**生成器函数**。生成器函数是一种特殊的函数，它可以暂停其执行并在稍后恢复，一次产生一个值。生成器函数用星号（`function*`）标记，并使用 `yield` 关键字来产生值。

这里是将我们的拉取流重新实现为生成器：

```js
function* simplePullStream(data) {
 for (const chunk of data) {
 yield chunk;
 }
}
```

此生成器产生的值序列与我们的手动迭代器相同，但语法要简洁得多。在底层，生成器函数自动实现了迭代器协议。当你调用生成器函数时，它返回一个迭代器对象。每次调用迭代器的 `next()` 方法都会恢复生成器函数的执行，直到下一个 `yield` 语句。

生成器还实现了 **可迭代** 协议。可迭代对象是一个具有键 `Symbol.iterator` 的方法的对象，该方法返回一个迭代器。数组是可迭代的。字符串是可迭代的。生成器函数返回可迭代对象。

因为生成器是可迭代的，所以我们可以用 `for...of` 循环来使用它们：

```js
for (const chunk of simplePullStream([1, 2, 3, 4, 5])) {
 console.log("Pulled:", chunk);
}
```

`for...of` 循环在幕后自动调用迭代器的 `next()` 方法，拉取值，直到 `done` 为 `true`。

### 生成器调用-产生-恢复周期

生成器暂停和恢复的交互式可视化

第 1 步/8 步 初始调用 生成器创建 `const gen = myGenerator()` 生成器函数

```js
function* myGenerator() {
  yield 1;
  yield 2;
  return;
}
```

当前状态 暂停 已产生的值：0

## 异步迭代器和 `for await...of`

生成器解决了同步序列的问题，但现实世界中的数据流是异步的。从文件中读取、从网络中获取、查询数据库——所有这些操作在 Node.js 中都是固有的异步操作。我们需要一种异步拉取数据的方法。

JavaScript 提供了 **异步迭代器** 来实现这一目的。异步迭代器类似于常规迭代器，但其 `next()` 方法返回一个解析为下一个值的 Promise。异步生成器函数用 `async function*` 标记，并且可以在其中使用 `await`。

这里是一个模拟异步数据生产的异步拉取流：

```js
async function* asyncPullStream(data) {
 for (const chunk of data) {
 await new Promise((resolve) => setImmediate(resolve));
 yield chunk;
 }
}
```

消费者使用 `for await...of` 从异步迭代器中提取数据：

```js
for await (const chunk of asyncPullStream([1, 2, 3])) {
 console.log("Pulled:", chunk);
}
```

`for await...of` 循环自动处理异步迭代器 `next()` 方法返回的 Promises。每次迭代都会在继续之前等待下一个 Promise 解析。这使得基于异步拉取的流感觉就像同步迭代一样自然。

异步迭代器是 JavaScript 中相对较新的功能（在 ES2018 中标准化），但它们非常强大。它们提供了一种干净、可组合的方式来处理异步数据序列。Node.js 流支持异步迭代，正如我们将看到的。

## 拉模型的优势和局限性

拉模型有其自身的优势。首先，**背压是隐式的**。因为消费者明确地拉取每个数据块，生产者不能压倒消费者。生产者仅在请求时才产生数据。如果消费者速度慢，它只需减少拉取频率，生产者就会空闲，等待下一次拉取。无需在生产者和消费者之间进行复杂的信号传递来调节流量。拉取机制本身提供了调节。

其次，拉模型自然映射到懒计算。生产者可以避免在消费者实际请求数据之前进行工作。如果消费者只从可能无限长的序列中拉取前几个项目，生产者就不会生成其余部分。这可以是一个显著的效率提升。

第三，拉模型具有优雅的合成性。你可以将多个基于拉取的转换链式连接起来，并且每个阶段只有在需要数据时才会从上一个阶段拉取。整个管道由最终消费者的拉取请求驱动，反向传播到链中。

然而，拉模型也有局限性。对于事件驱动系统来说，它不太自然。如果数据到达不可预测（例如，通过 WebSocket 连接的消息），它无法干净地适应拉模型。你不能拉取尚未到达的数据。当生产者可以按需生成数据时，拉模型效果最佳，而不是当生产者本身对外部事件做出反应时。

此外，拉取模型本身不支持扇出。迭代器生成一系列值，并通过拉取来消费这些值。一旦值被拉取，它就被消费了。如果你想让多个消费者接收相同的数据，你需要实现一个单独的机制来广播或 tee 流。

## Node.js 流的混合方法

Node.js 的流既不是纯粹的推送模式也不是纯粹的拉取模式。它们是一种混合模型，结合了两种方法的优点，同时缓解了它们的局限性。

在核心上，Node.js 的流是基于推送的。它们扩展了 `EventEmitter`，数据通过 `data` 事件流经它们。这使得它们非常适合 Node.js 的事件驱动架构。然而，Node.js 的流明确实现了背压。消费者可以向生产者发出信号，表明他们尚未准备好接收更多数据，而生产者必须尊重这一信号。这种背压机制为基于推送的架构添加了类似拉取的控制。

此外，Node.js 的流支持异步迭代。你可以使用 `for await...of` 来消费可读流，将其视为基于拉取的异步迭代器。在底层，异步迭代器从流的内部缓冲区拉取数据，而流管理从底层数据源的数据流。这允许你使用最适合你用例的消费模型：基于事件的（推送）或基于迭代器的（拉取）。

这种混合方法并非没有复杂性。Node.js 的流在设计上已经经历了多次迭代，API 随着时间的推移而演变，以添加新特性和解决发现的问题。我们将简要探讨这一历史，因为了解流是如何演变的有助于我们理解它们为什么今天会以这种方式工作。

## 让我们回到...过去

流一直是 Node.js 的一部分。最初的实现很简单：流发出 `data` 事件，消费者监听这些事件。没有暂停或背压的概念。如果消费者跟不上了，数据就会积累在内存中。

Node.js **版本 0.10** 引入了 Streams2，这是一次重大的重新设计，增加了对背压的显式支持。可读流获得了两种操作模式：“暂停”和“流动”。在暂停模式下，消费者明确调用 `read()` 从流的内部缓冲区拉取数据。在流动模式下，流通过 `data` 事件将数据推送到消费者，但消费者可以暂停流以发出背压信号。可写流获得了一种机制，其中 `write()` 方法返回一个布尔值，指示内部缓冲区是否已满，向生产者发出停止写入的信号，直到发出 `drain` 事件。

Node.js **v10.0** 及以后的版本进一步精炼了这一模型，增加了诸如 `stream.pipeline()` 用于健壮的错误处理、`stream.finished()` 用于检测流完成以及异步迭代器支持等特性。这些新增功能使得流在生产和使用上更加人性化且可靠。

现在，Node.js 流是一个成熟、经过实战检验的抽象。它们被广泛应用于 Node.js 生态系统 - 由 `fs` 模块用于文件 I/O，由 `http` 模块用于请求和响应体，由 `zlib` 用于压缩，由 `crypto` 用于加密，以及无数第三方库。

## 四种流类型

Node.js 定义了四种基本类型的流。每种类型代表数据流中的不同角色。

**可读流**是数据源。它们产生可以被消费的数据。例如，`fs.createReadStream()` 用于读取文件，`http.IncomingMessage` 用于服务器端的 HTTP 请求体或客户端的响应体，以及 `process.stdin` 用于读取标准输入。

**可写流**是数据的目的地。它们消费可以被写入的数据。例如，`fs.createWriteStream()` 用于写入文件，`http.ServerResponse` 用于 HTTP 响应体，以及 `process.stdout` 用于写入标准输出。

**转换流**也是可读和可写的。它们消费数据，以某种方式转换它，并产生新的数据。它们位于管道的中间，在其可写端接受输入，在其可读端发出输出。例如，`zlib.createGzip()` 用于压缩和 `crypto.createCipheriv()` 用于加密。转换流是双工流的子类，具有简化的接口，适用于可读输出直接从可写输入派生的常见情况。

**双工流**也是可读和可写的，但与转换流不同，它们的可读和可写部分是独立的。它们代表双向通信通道。最常见的一个例子是 `net.Socket`，它代表一个 TCP 连接。写入套接字的数据通过网络发送，从网络接收到的数据可以从套接字中读取。数据流的两个方向是分开的；写入套接字不会直接影响到可以从它读取的内容。

这四种类型构成了 Node.js 中流的语言。通过组合它们，你可以构建复杂的数据处理管道。一个可读流可以被连接到一个转换流，这个转换流可以连接到另一个转换流，然后连接到一个可写流。每个阶段增量地、分块地处理数据，通过反向传播的背压来确保内存使用保持在有限范围内。

## 数据流的概念化

让我们可视化数据如何通过流管道流动。想象一个有三个阶段的简单管道：

1.  一个可读流（源）从文件中读取数据块。

1.  一个转换流（处理器）将每个数据块转换为大写。

1.  一个可写流（目的地）将每个数据块写入另一个文件。

数据通过管道向前流动：从可读流到转换流再到可写流。每个数据块按顺序通过各个阶段。

但控制信号会反向流动。如果可写流的内部缓冲区满了（可能是因为磁盘写入速度慢），它会发出反压信号。转换流看到可写流不能接受更多数据，会暂停从可读流中读取。可读流看到没有人从它那里拉取数据，就会停止从文件中读取。整个管道会暂停，直到可写流的缓冲区排空并发出它准备好接收更多数据的信号。在那个时刻，管道会继续：可读流读取更多数据，转换流处理它，可写流写入它。

这种双向流动——数据向前，反压向后——是流管道中有限内存使用的关键。没有反压，快速阶段会产生比慢速阶段能消费的数据更快的数据，缓冲区会无限制地增长。有了反压，管道会自我调节，确保每个阶段都以最慢阶段的速度运行。

### 双向管道流

数据向前流动 ➜ 反压信号向后流动 ⬅

阶段 1

#### 可读流

`fs.createReadStream()` - 活跃 - 读取数据 - 阶段 2

#### 转换流

`zlib.createGzip()` - 阶段 3

#### 可写流

`fs.createWriteStream()` - 前向数据流

数据块从源 → 转换 → 目的地移动

反压信号

当缓冲区满时，暂停信号会反向传播

## 何时使用流

流并不总是正确的工具。对于小量的数据，这些数据容易放入内存，将整个数据集读入一个 Buffer 或字符串会更简单，通常也更快。流增加了开销 - 事件循环必须安排回调，数据必须分块，并且必须管理反压。如果你正在处理一个 10KB 大小的 JSON 文件，流就过度了。只需使用`fs.readFile()`或`fs.readFileSync()`，解析 JSON，然后完成。

当处理大型数据集或无界数据时，流特别有用。如果你正在处理一个多吉字节大小的日志文件，流是必不可少的。如果你正在处理一个大小未知的传入 HTTP 请求体，流是正确的抽象。如果你正在实现一个消息连续到达的网络协议，流提供了你需要的结构。

当你希望在所有数据都可用之前就开始处理数据时，流也是非常有价值的。考虑一个 HTTP 服务器响应文件下载请求的情况。如果没有流，服务器必须在开始发送响应之前将整个文件读入内存。有了流，服务器可以在从磁盘读取文件的第一部分后立即开始发送文件的第一部分，这显著减少了客户端的首次字节时间。

最后，流对于组合转换的管道非常有用。如果你需要读取一个文件，解压缩它，解析它，转换解析后的数据，并将结果写入另一个文件，流允许你将这个过程表达为一个干净、线性的管道，其中每个阶段都是一个独立的、专注的转换。这种可组合性是流抽象的主要优势。

## **常见用例**

在使用 Node.js 流工作时，会出现一些重复出现的模式。

**文件输入/输出**是最常见的用例。读取和写入大文件几乎总是应该使用流来完成。这样可以避免将整个文件加载到内存中，并允许立即开始处理。

**网络通信**本质上是流式的。HTTP 请求和响应体是流。TCP 套接字是双向流。当你通过网络发送数据时，你不会一开始就拥有所有数据；它是逐步生成或接收的。流是网络协议的自然抽象。

**数据转换管道**非常适合流。任何需要应用一系列转换到数据上的情况——解析、过滤、映射、聚合——流允许你将每个转换表达为一个独立的、可组合的阶段。这在 ETL（提取、转换、加载）工作流程、日志处理和数据分析中很常见。

**实时数据处理**通常使用流。如果你正在处理来自消息队列的事件、物联网设备的传感器数据，或者在 Web 应用中的用户交互，流提供了一种处理每个到达的事件的方法，而不会在内存中累积事件。

**代理和复用**利用套接字的复用特性。当构建代理服务器或负载均衡器时，你可以在套接字之间传输数据，转发请求和响应而不需要缓冲整个消息。这使得代理能够高效地处理非常大的请求和响应。

## **设置场景**

我们现在已经建立了流的原理基础。我们理解了它们解决的问题：在不耗尽内存的情况下处理大量或无界数据。我们理解了两种基本模型：推送和拉取，并看到了 Node.js 流如何结合两者。我们理解了四种流类型以及它们在数据流中的作用。

我们还没有做的是实现或使用真正的 Node.js 流。我们构建了一些简单的示例来阐述概念，但我们还没有探索实际的`stream.Readable`、`stream.Writable`、`stream.Transform`和`stream.Duplex`类。我们还没有研究如何实现自定义流，如何配置它们的行为，或者如何构建具有错误处理的健壮管道。

那是下一章的工作内容，我们将深入探讨可读流：它们是如何实现的，它们如何管理内部缓冲区，它们两种操作模式是如何工作的，以及如何从各种数据源创建自定义的可读流。

但所有这些都建立在我们在这里建立的基础之上。流不是魔法。它们是对内存限制和异步 I/O 现实的一种系统性的响应。它们实现了经过验证的模式 - 观察者、迭代器 - 这些模式被调整以适应 Node.js 事件驱动架构的具体需求。通过从第一原理理解流，你将能够对其行为进行推理，在出现问题时进行调试，并自信地设计你自己的流系统。

从现在开始将会变得有趣！
