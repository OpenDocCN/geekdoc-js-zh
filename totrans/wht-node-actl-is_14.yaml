- en: Modern Async Pipelines & Error Handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.thenodebook.com/streams/modern-pipelines-error-handling](https://www.thenodebook.com/streams/modern-pipelines-error-handling)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You know how individual streams work now - `Readable` producing data, `Writable`
    consuming it, `Transform` processing it in between. Each stream type has its own
    buffering, its own backpressure mechanism, its own event lifecycle. In real applications,
    you rarely work with streams in isolation. You're connecting them together, creating
    **pipelines** where data flows from source through multiple transformation stages
    to a final destination.
  prefs: []
  type: TYPE_NORMAL
- en: And this is where things get tricky. The concept's fine - piping data from one
    stream to another is straightforward enough - but actually doing it correctly?
    That's where the headaches start. Because when you connect streams, you're dealing
    with **multiple error sources**, **multiple backpressure signals**, and **multiple
    resource cleanup scenarios**. If any stream in your pipeline fails, what happens
    to the others? If backpressure occurs midway through, does it propagate correctly?
    When the pipeline completes, are all resources cleaned up properly?
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is about answering those questions. We're going to examine the
    original `pipe()` method and understand both why it exists and why it's insufficient
    for production code. Then we'll dive deep into `stream.pipeline()`, which is the
    modern, recommended approach for composing stream pipelines with proper error
    handling and cleanup. After that, we'll explore error handling patterns specific
    to streaming - because errors in pipelines behave differently from errors in synchronous
    code. We'll look at using async iteration as an alternative pipeline approach,
    and finally we'll cover advanced composition patterns for building reusable, flexible
    pipeline segments.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers how to connect streams correctly - with proper error propagation,
    resource cleanup, and backpressure handling across all stages.
  prefs: []
  type: TYPE_NORMAL
- en: The pipe() Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You''ve already learned how `pipe()` works from the Readable and Writable chapters.
    As a quick recap: `pipe()` connects a Readable stream to a Writable, automatically
    handling backpressure by calling `pause()` when `write()` returns `false` and
    `resume()` when the Writable emits `drain`. We covered this pattern extensively
    in the Writable Streams chapter when discussing backpressure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The method returns the destination stream, allowing you to chain pipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a four-stage pipeline: readable -> transform1 -> transform2 ->
    writable. Data flows through each stage sequentially, with **backpressure propagating
    backward** from the writable destination all the way to the readable source—a
    pattern we explored in detail in the Writable Streams chapter. If `writable` signals
    backpressure, the entire chain pauses; when `writable` emits `drain`, the resume
    signal propagates forward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A concrete example—compressing a log file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Three streams, two `pipe()` calls. The file reader produces chunks, the gzip
    transform compresses them, and the file writer saves the compressed data. Memory
    usage stays bounded because each stage respects backpressure signals.
  prefs: []
  type: TYPE_NORMAL
- en: 'But `pipe()` has a problem: **error handling**.'
  prefs: []
  type: TYPE_NORMAL
- en: Here's what happens when an error occurs in a piped stream. The stream that
    encounters the error emits an `error` event. But that error **does not propagate**
    to other streams in the pipeline. You already know from the Readable and Writable
    chapters that each stream needs its own error handler, or the error will crash
    your process. But in pipelines, this becomes especially painful.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You need three separate error handlers. Miss one and your process crashes. It's
    tedious, error-prone, and honestly kind of ridiculous for something that should
    be straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: But it gets worse. When an error occurs in the middle of a pipeline, the other
    streams **don't automatically stop**. Suppose the transform throws an error while
    processing a chunk. The transform emits `error` and stops processing. But the
    reader keeps reading and trying to write to the transform, which is now in an
    errored state. The writer is waiting for data that will never come, and it might
    never emit `finish` because the pipeline never completes cleanly.
  prefs: []
  type: TYPE_NORMAL
- en: 'You end up with **dangling resources**. File handles that aren''t closed. Network
    connections that aren''t cleaned up. Memory buffers that aren''t released. The
    pipeline is in a partially-failed state, and cleaning it up requires manually
    calling `destroy()` on each stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is verbose, repetitive, and fragile. If you add a new stream to the pipeline,
    you have to update all the error handlers to destroy the new stream too.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s another limitation of `pipe()`: you can''t easily tell when the entire
    pipeline has completed. The readable emits `end` when it''s done reading. The
    writable emits `finish` when it''s done writing. But which one do you listen to?
    And what if you have multiple transforms in the middle? Each transform emits its
    own `end` event. The final destination emits `finish`. You have to track the **right
    event on the right stream**, which depends on the pipeline''s structure.'
  prefs: []
  type: TYPE_NORMAL
- en: For simple two-stream scenarios, `pipe()` works fine. But for real production
    pipelines with multiple stages and proper error handling requirements, `pipe()`
    is insufficient. This is why `stream.pipeline()` was introduced.
  prefs: []
  type: TYPE_NORMAL
- en: The unpipe() Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Worth covering `unpipe()` before pipeline() - though you''ll rarely use it.
    This method disconnects a piped stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When you call `unpipe()`, the readable stops sending data to the specified
    writable. If you call `unpipe()` without arguments, it disconnects from all destinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Why would you use this? Mainly for dynamic routing scenarios where you want
    to redirect a stream''s output based on runtime conditions. For example, you might
    be reading from a socket and initially piping to a file, but then decide to pipe
    to a different destination based on incoming data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: But in practice? I've almost never needed `unpipe()`. Most pipelines are static
    - you define the flow at setup time and let it run to completion. Dynamic routing
    is better handled with higher-level abstractions, like routing streams or conditional
    transforms.
  prefs: []
  type: TYPE_NORMAL
- en: The main thing to know about `unpipe()` is that it exists, and that when you
    unpipe a stream, the destination does not automatically end. When `unpipe()` is
    called, it removes the destination's listeners from the source stream. The source
    stream's flowing mode state depends on whether any consumers remain attached -
    if all pipe destinations are removed and there are no `data` event listeners,
    the stream switches back to paused mode. If other consumers exist (other piped
    destinations or data listeners), the source continues emitting `data` events.
    If you want the destination to close, you need to call `end()` on it manually.
  prefs: []
  type: TYPE_NORMAL
- en: stream.pipeline()
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`stream.pipeline()` is the modern approach to composing streams. This function
    was added to Node.js specifically to address the error handling and cleanup problems
    with `pipe()`. Here''s the basic usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of chaining `pipe()` calls, you pass all your streams as arguments
    to `pipeline()`, followed by a callback that''s invoked when the pipeline completes
    or errors. That''s the signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`pipeline()` does three things that `pipe()` doesn''t:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automatic error propagation**: If any stream in the pipeline emits an error,
    `pipeline()` stops the pipeline and invokes the callback with that error. You
    don''t need separate error handlers on each stream.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Automatic cleanup**: When an error occurs (or when the pipeline completes),
    `pipeline()` calls `destroy()` on all streams in the pipeline. File handles get
    closed, buffers freed, connections torn down.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Single completion callback**: One callback for everything.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s see what this looks like in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If any of these streams errors - the file read fails, the gzip encounters corrupt
    data, the file write hits a disk-full error - the callback is invoked with the
    error, and all three streams are destroyed. If everything succeeds, the callback
    is invoked with `err` as `undefined`.
  prefs: []
  type: TYPE_NORMAL
- en: This is simpler than the equivalent `pipe()` code with manual error handling.
    No separate error listeners, no manual destroy calls, no tracking which stream
    to listen to for completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s also a promise-based version of `pipeline()` in the `stream/promises`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The promise-based version returns a promise that resolves when the pipeline
    completes or rejects when any stream errors. This fits naturally with async/await
    code. You wrap the `pipeline()` call in a try/catch, and errors are handled like
    any other promise rejection.
  prefs: []
  type: TYPE_NORMAL
- en: This is the **recommended pattern** for modern Node.js code. Use `stream/promises`
    and `async/await` for clean, readable pipeline composition.
  prefs: []
  type: TYPE_NORMAL
- en: How pipeline() Works Internally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Understanding the internals helps you reason about behavior and debug issues.
    When you call `pipeline(s1, s2, s3, callback)`, the function essentially:'
  prefs: []
  type: TYPE_NORMAL
- en: Connects streams using the same `pipe()` mechanics you learned in earlier chapters—the
    same automatic backpressure handling we covered in the Writable Streams chapter
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attaches error listeners to all streams for coordinated error handling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calls `destroy()` on all streams when any error occurs or when completion happens
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invokes the callback once with either an error or undefined
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The key difference from manual `pipe()` is the **error coordination and automatic
    cleanup**. You get the same backpressure handling (which we covered extensively
    in the Writable Streams chapter), but with production-grade error management.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a highly simplified conceptual model showing the basic behavior. **Important**:
    This is a pedagogical simplification to illustrate the concept, not the actual
    implementation. The real Node.js `pipeline()` implementation (based on the `pump`
    library) is significantly more sophisticated and handles many edge cases not shown
    here - including async iterables, generators, complex error scenarios, once-only
    callback guarantees, and proper stream type detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is a conceptual model to help you understand the behavior - the real `pipeline()`
    implementation in Node.js is more sophisticated with its own stream connection
    logic and comprehensive edge case handling (once-only callback invocation, destroyed
    stream handling, complex error scenarios, etc.). Think of this as "what it does"
    rather than "how it does it."
  prefs: []
  type: TYPE_NORMAL
- en: '`pipeline()` handles an edge case you should know about: a stream emitting
    an error after it''s already been destroyed. This can happen with async operations
    in custom streams where an error occurs after the stream has nominally completed.
    The real `pipeline()` implementation ensures the callback is only invoked once,
    even if multiple streams error simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: Using pipeline() with Transform Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You don't even need `Transform` stream instances - you can pass **async generator
    functions**, and `pipeline()` will treat them as transforms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The async generator in the middle is automatically converted to a `Transform`
    stream. For each chunk from the source, the generator transforms it (in this case,
    uppercases it) and yields the result. The yielded values become chunks in the
    output stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'This works well for simple transformations. Instead of creating a custom `Transform`
    class, you write a generator function inline. It reads like a loop: **for each
    input chunk, produce an output chunk.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use regular async functions that return async iterables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This works because `pipeline()` recognizes async iterables and automatically
    wraps them in Transform streams internally.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can even chain multiple generator transforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The first generator converts buffers to strings and splits them into lines,
    handling chunk boundaries with a buffer. The second filters out lines starting
    with "#". Each generator is a pipeline stage, and `pipeline()` handles the plumbing.
  prefs: []
  type: TYPE_NORMAL
- en: This is the recommended way to build stream pipelines in modern Node.js. For
    simple transformations, use inline generators. For complex stateful transforms,
    create a `Transform` class. Mix and match as needed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note about generator functions**: Only values produced with `yield`
    are sent to the output stream. If a generator uses `return` to produce a final
    value, that value is **not** yielded to the pipeline - it''s only accessible to
    code directly consuming the generator. In pipeline transforms, always use `yield`
    for output chunks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Quick aside: if you''re coming from Python, this is like itertools but actually
    built into the language. End digression.'
  prefs: []
  type: TYPE_NORMAL
- en: Error Handling in Stream Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Error handling gets tricky with pipelines because they introduce error scenarios
    that don't exist in single-stream code. You already know from the Readable and
    Writable chapters that streams emit `error` events for various failures (file
    not found, disk full, network dropped, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'In pipelines, these errors can come from **multiple sources simultaneously**:'
  prefs: []
  type: TYPE_NORMAL
- en: The **source stream** might fail to read (file doesn't exist, permission denied,
    network dropped)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **transform stream** might encounter invalid data (parse error, validation
    failure)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **destination stream** might fail to write (disk full, broken pipe, remote
    endpoint closed)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these errors manifests as an `error` event on the stream that encountered
    it. With `pipe()`, you'd have to handle each separately. With `pipeline()`, all
    errors are caught and passed to your callback or promise rejection.
  prefs: []
  type: TYPE_NORMAL
- en: When an error occurs midway through a pipeline, what happens to partial data?
    Suppose you're reading 100MB file, transforming it, and writing the result. At
    50MB, the transform encounters corrupt data and errors. What happened to the first
    50MB?
  prefs: []
  type: TYPE_NORMAL
- en: The answer depends on the destination stream's behavior. If it's writing to
    a file, the file now contains 50MB of **partial output**. The file exists, but
    it's incomplete and possibly invalid. `pipeline()` **doesn't roll back** partial
    writes - it can't. The data is already written to the underlying resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means you need to handle partial data in your application logic. One pattern
    is to write to a temporary file and rename it to the final name only on success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If the pipeline succeeds, the temp file is renamed to the final name. If it
    fails, the temp file is deleted. This ensures that either the complete output
    exists or nothing exists - no partial files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another pattern is to use transactions when writing to a database. Write all
    rows within a transaction, and commit only if the pipeline completes successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`pipeline()` only handles **stream-level cleanup** - calling `destroy()` on
    streams. It doesn''t handle **domain-level cleanup** (deleting partial files,
    rolling back transactions). **That''s your responsibility.**'
  prefs: []
  type: TYPE_NORMAL
- en: Error propagation is where this gets interesting. When a stream in a pipeline
    errors, `pipeline()` immediately calls `destroy()` on all other streams. This
    causes each stream to emit a `close` event, and any pending operations are canceled.
    This is correct - if one stage fails, the entire pipeline should stop.
  prefs: []
  type: TYPE_NORMAL
- en: But what if you want to handle errors from different streams differently? For
    example, if the source fails, you want to log "read error," but if the destination
    fails, you want to log "write error." With `pipeline()`, you only get one error
    in the callback - the first error that occurred.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to distinguish between error sources, you can check the error object''s
    properties or use error wrapping in your custom streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: By tagging errors with a `code` property, you can distinguish them in the error
    handler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another pattern is using `stream.finished()` to detect when a specific stream
    completes or errors, even within a larger pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `finished()` utility attaches listeners to a stream and invokes a callback
    when the stream ends, errors, or is destroyed. This lets you monitor individual
    streams within a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: stream.finished()
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`stream.finished()` deserves a closer look.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `finished()` function takes a stream and a callback, and invokes the callback
    when the stream completes (either successfully or with an error):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: What does "finished" mean? For a `Readable`, it means the stream has ended (pushed
    `null` or destroyed). For a `Writable`, it means the stream has finished writing
    and emitted `finish` (or been destroyed). For a `Duplex` or `Transform`, it means
    both sides have completed.
  prefs: []
  type: TYPE_NORMAL
- en: This is safer than attaching listeners to `end` or `finish` directly, because
    `finished()` also listens for `error`, `close`, and `destroy` events, and handles
    the complex logic of determining whether the stream truly completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s also a promise-based version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The promise resolves when the stream completes successfully or rejects if the
    stream errors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note on event listeners**: `stream.finished()` intentionally leaves dangling
    event listeners (particularly `error`, `end`, `finish`, and `close`) after the
    callback is invoked or the promise settles. This design choice allows `finished()`
    to catch unexpected error events from incorrect stream implementations, preventing
    crashes. For most use cases with short-lived streams, this is not a concern as
    the streams will be garbage collected. However, for memory-sensitive applications
    or long-lived streams, you can use the `cleanup` option to remove these listeners
    automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Why use `finished()` instead of just listening for `end` or `finish`? Because
    streams can end in multiple ways. A stream might emit `end` naturally, or it might
    be destroyed due to an error, or it might be destroyed explicitly via `destroy()`.
    The `finished()` utility handles all these cases and gives you a single callback
    or promise that represents "this stream is done, one way or another."
  prefs: []
  type: TYPE_NORMAL
- en: 'This is useful when you need to know when a specific stream in a complex pipeline
    has completed, even if the overall pipeline is still running. For example, if
    you''re piping a source to multiple destinations (a tee or broadcast pattern),
    you can use `finished()` to know when each destination has consumed all its data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This waits for both destinations to finish writing before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: Error Recovery in Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not all errors are fatal - some deserve a retry. Some errors are **transient**
    and can be retried. Others indicate a fundamental problem and require the pipeline
    to fail.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you're reading from a network source and the connection drops,
    that might be transient. Retrying the connection could succeed. But if you're
    reading a file and get `EACCES` (permission denied), retrying won't help - the
    file's permissions won't magically change.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is **categorizing errors**. Is this an *operational error* (expected
    failure condition) or a *programmer error* (bug in the code)? Is it transient
    or permanent?
  prefs: []
  type: TYPE_NORMAL
- en: 'For transient errors, you can implement retry logic around the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This function attempts the pipeline up to `maxRetries` times. If a transient
    error occurs, it waits (with exponential backoff) and retries. If a non-transient
    error occurs, or if all retries are exhausted, it throws the error.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `source()` is a function that creates a new source stream. You
    can't reuse a stream after it's errored and been destroyed. Each retry needs fresh
    stream instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another recovery pattern is fallback. If one data source fails, try an alternative
    source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This is useful for redundant data sources, like trying a CDN first and falling
    back to an origin server if the CDN is unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: 'For destination failures, you might want to retry writes to a different location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Again, note that `source()` is a function creating a new source. After the first
    pipeline fails, the original source is destroyed, so you need a new one for the
    retry.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **key principle**: decide upfront which errors are recoverable and implement
    your retry or fallback logic **at the pipeline level**, not at the individual
    stream level. Streams don''t know about your application''s retry policy - you
    have to coordinate it externally.'
  prefs: []
  type: TYPE_NORMAL
- en: Partial Data Concerns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Partial data needs attention because it's easy to get wrong. When a pipeline
    fails midway through, any data already written to the destination remains there.
    The pipeline doesn't automatically clean it up.
  prefs: []
  type: TYPE_NORMAL
- en: This is a problem for **data integrity**. If you're writing a database export
    and the pipeline fails at 60%, you have a 60%-complete file. If you later retry
    and succeed, you might end up with duplicate data (the first 60% twice) or you
    might overwrite the partial file with a complete one, depending on how you open
    the output file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are **strategies for handling this**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Write to a temporary location and atomic rename**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This is the **safest pattern** for file outputs. The final file only exists
    if the pipeline completed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Use append mode and idempotent operations**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If your output supports appending (like log files), and your operations are
    idempotent (writing the same data twice is harmless), you can just retry from
    the beginning and append:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: If the pipeline fails and you retry, the second attempt appends more data. If
    you have duplicate detection downstream, this is fine.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Use transactional destinations**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Databases, message queues, and some cloud storage systems support transactions.
    Write within a transaction and commit only on success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The destination doesn't persist any data until the pipeline succeeds.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Write a completion marker**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For scenarios where you can''t use transactions, write a marker file indicating
    successful completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Before processing the output file, check for the marker. If the marker doesn't
    exist, the file is incomplete and should be discarded or retried.
  prefs: []
  type: TYPE_NORMAL
- en: The pattern you choose depends on your destination's capabilities and your consistency
    requirements. The key is to be explicit about what happens on partial failure
    and design your pipeline to handle it.
  prefs: []
  type: TYPE_NORMAL
- en: Destroying Streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You've already learned about `stream.destroy()` in the Readable and Writable
    chapters. As a reminder, calling `destroy()` on any stream transitions it to a
    destroyed state, emits `close`, and optionally emits `error` if you pass one.
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s specific to pipelines is that when you destroy **any** stream in a
    `pipeline()`, the pipeline function automatically destroys all other streams and
    invokes the callback with the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: When you call `source.destroy()`, the source stream stops reading, emits `close`,
    and if you passed an error, emits `error`. The `pipeline()` function sees the
    error and destroys all other streams in the pipeline (in this case, `dest`). The
    callback is invoked with the error.
  prefs: []
  type: TYPE_NORMAL
- en: This automatic cleanup is another advantage of `pipeline()` over manual `pipe()`
    chaining. With `pipe()`, you'd have to track all streams and destroy them manually.
  prefs: []
  type: TYPE_NORMAL
- en: This is useful for implementing cancellation. If a user cancels an operation,
    you destroy the pipeline's source stream. The pipeline stops gracefully, cleans
    up resources, and your callback is invoked to handle the cancellation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also destroy with no error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the stream is destroyed but no `error` event is emitted. The `pipeline()`
    callback is still invoked, but `err` is `null`. This is useful for stopping a
    pipeline without treating it as a failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'One important detail: `destroy()` is idempotent. Calling it multiple times
    on the same stream does nothing after the first call. The stream is destroyed
    once, and subsequent destroy calls are ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: Another thing - when you destroy a stream, buffered data is just... gone. If
    a `Writable` has buffered writes that haven't been flushed yet, they're lost.
    If a `Readable` has buffered data that hasn't been consumed yet, it's lost. Destroy
    means **"stop immediately and throw away any state,"** not "gracefully finish
    pending operations."
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need graceful shutdown (finish writing buffered data before closing),
    use `end()` instead of `destroy()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: But `end()` only works on Writable streams. For Readable streams, there's no
    graceful stop - you either consume all data or you destroy.
  prefs: []
  type: TYPE_NORMAL
- en: Async Iteration Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the Readable Streams chapter, we explored using `for await...of` to consume
    streams with automatic backpressure handling. This pattern is also an alternative
    to `pipe()` and `pipeline()` for building stream processing logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a refresher: when you iterate over a Readable stream with `for await...of`,
    the iterator protocol automatically implements backpressure. The loop doesn''t
    pull the next chunk until the current iteration completes. If your processing
    is async, the stream waits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We won't repeat the full backpressure mechanics here—refer to the "Backpressure
    in Async Iteration" section in the Readable Streams chapter for the complete explanation
    of how the iterator protocol manages flow control.
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s relevant to this chapter is using this pattern specifically for **pipeline
    construction** rather than `pipeline()`. You can build pipelines by reading from
    a source with `for await...of`, transforming each chunk, and writing to a destination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This is a manual pipeline. You're pulling from the source, transforming, and
    writing to the destination, with explicit backpressure handling (wait for `drain`
    if `write()` returns false). This pattern gives you fine-grained control over
    the data flow, but it requires you to manage backpressure manually—forgetting
    the `drain` check leads to unbounded memory growth, as we discussed in the Writable
    Streams chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'A cleaner pattern is to use `stream.Readable.from()` with an async generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The async generator is automatically wrapped in a Readable stream, and `pipeline()`
    handles the plumbing. This combines the clarity of `for await...of` with the robustness
    of `pipeline()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can chain multiple generator transforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Each generator is a pipeline stage. This is readable. Each stage is a simple
    `for await` loop that yields transformed chunks. The composition is handled by
    `pipeline()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This pattern is especially nice for **`objectMode` pipelines** where each chunk
    is a structured object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Each stage is a function from source to async iterable. The `pipeline()` function
    stitches them together. This is functional pipeline composition, and it's often
    clearer than creating Transform classes.
  prefs: []
  type: TYPE_NORMAL
- en: Backpressure with Async Iteration in Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We covered how backpressure works with `for await...of` in the Readable Streams
    chapter. The key points to remember when using this pattern in pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Await async work** - The iterator pulls one chunk at a time; if you await
    async operations, backpressure flows automatically from your processing speed
    back to the source'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Don''t accumulate promises** - Avoid `promises.push(processAsync(chunk))`
    patterns that break backpressure by reading the entire stream into memory before
    processing'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Use controlled concurrency** - For parallel processing with bounded concurrency,
    use libraries like `p-limit` to limit in-flight operations'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the Readable Streams chapter: **you must await async operations** inside
    the loop. Without `await`, you lose backpressure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The advantage of this approach over `pipeline()` is explicit control over data
    flow. The disadvantage is that you must manage error handling yourself—there's
    no automatic cleanup like `pipeline()` provides.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the full mechanics of how the async iterator protocol implements backpressure,
    see the "Backpressure in Async Iteration: How It Works" section in the Readable
    Streams chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Composable Transforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the Transform Streams chapter, we covered how to implement custom transforms.
    Now let's look at building **reusable pipeline components** through factory functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pattern is straightforward—create functions that return configured stream
    instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Each call to `createCSVParser()` returns a fresh Transform instance. You can't
    reuse a stream instance across multiple pipelines (once a stream ends or errors,
    it's done), but you can reuse the factory function.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can make factories configurable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now the transform is parameterized. Different pipelines can extract different
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more complex pipelines, you can compose pipeline segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The `createProcessingPipeline()` function encapsulates the entire transformation
    sequence. You pass in a source and destination, and it wires up all the intermediate
    transforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a higher-order function pattern: functions that create and compose
    streams. It''s useful for building modular, testable streaming code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also compose generator functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Each generator is a function from async iterable to async iterable. You compose
    them by passing one's output as another's input. The `pipeline()` function handles
    the plumbing.
  prefs: []
  type: TYPE_NORMAL
- en: This functional composition style fits naturally with Node.js's streaming model.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline Segments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's formalize the concept of a pipeline segment. A segment is a reusable piece
    of a pipeline - it might be a single transform, or a chain of transforms, or conditional
    logic that routes to different transforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a segment that validates objects and either passes them through or
    routes them to an error destination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This transform validates each object. Valid objects are pushed downstream.
    Invalid objects are written to an error destination (like a log file or error
    stream). This is a branching segment: one input, two outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You use it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Valid objects go to `dest`. Invalid objects go to `errorLog`. The pipeline continues
    even when invalid objects are encountered - they're just routed elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another pattern is conditional segments that choose different transforms based
    on runtime state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Based on a condition function, each object is sent through either `trueTransform`
    or `falseTransform`. This is a routing segment.
  prefs: []
  type: TYPE_NORMAL
- en: These patterns - branching, routing, conditional - are building blocks for complex
    data flows. You compose them into larger pipelines, creating sophisticated processing
    logic while keeping each segment focused and reusable.
  prefs: []
  type: TYPE_NORMAL
- en: Tee and Broadcast Patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes you need to send the same data to multiple destinations. This is called
    a tee (like a T-junction in plumbing) or broadcast pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to tee a stream is to pipe it to multiple destinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Both `dest1` and `dest2` receive the same data from `source`. The source emits
    each chunk once, and both pipes forward it to their respective destinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there''s a catch: backpressure. If `dest1` is slow and signals backpressure,
    the source pauses. But `dest2` might be fast and ready for more data. By pausing
    the source, you''re slowing down `dest2` unnecessarily.'
  prefs: []
  type: TYPE_NORMAL
- en: The source can't pause for one destination and continue for another. It's **all
    or nothing**. So when you tee a stream with `pipe()`, the **slowest destination
    controls the pace** for all destinations.
  prefs: []
  type: TYPE_NORMAL
- en: If this is acceptable (all destinations need the same data, and you're okay
    with the slowest one setting the pace), then simple piping works fine. But if
    you want independent backpressure per destination, you need a more sophisticated
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'One technique is to use `PassThrough` streams as intermediaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Now `dest1` and `dest2` have independent backpressure. If `dest1` is slow, `pass1`
    buffers. If `dest2` is fast, `pass2` doesn't buffer. The source isn't paused by
    either destination.
  prefs: []
  type: TYPE_NORMAL
- en: But this breaks source-level backpressure. If both destinations are slow, `pass1`
    and `pass2` both buffer, and the source isn't aware. You're buffering in memory
    unbounded.
  prefs: []
  type: TYPE_NORMAL
- en: 'For truly independent destinations with bounded memory, you need to use a fan-out
    stream that monitors backpressure from all destinations and pauses the source
    only when all destinations signal backpressure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This writable forwards each chunk to multiple destinations. The key difference
    from simple piping is how it handles backpressure: `write()` returns a boolean
    indicating whether you can continue writing. If it returns `false`, the destination''s
    buffer is full, and you should wait for the `drain` event before writing more.
    This implementation collects promises for any destinations signaling backpressure
    and waits for all of them to drain before invoking the callback, which creates
    backpressure on the source.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You use it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This is a more complex pattern, and it's not common in application code. Most
    of the time, you either accept that the slowest destination controls the pace,
    or you accept unbounded buffering in PassThrough intermediaries. True fan-out
    with independent backpressure per destination is complex and usually only needed
    in specialized scenarios like logging or monitoring systems.
  prefs: []
  type: TYPE_NORMAL
- en: AbortSignal Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Node.js streams support `AbortSignal` for cancellation. You can pass a signal
    to the promise-based `pipeline()`, and if the signal is aborted, the pipeline
    is destroyed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: When you call `controller.abort()`, the pipeline is immediately destroyed. All
    streams are torn down, the promise rejects with an **`AbortError`**, and any pending
    operations are cancelled.
  prefs: []
  type: TYPE_NORMAL
- en: This is useful for user-initiated cancellation, timeouts, or resource cleanup
    in long-running operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also create a timeout signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: If the pipeline doesn't complete within 5 seconds, it's automatically aborted.
  prefs: []
  type: TYPE_NORMAL
- en: 'For complex scenarios with multiple cancellation sources, you can use `AbortSignal.any()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This creates a composite signal that aborts if either the user cancels or the
    timeout expires.
  prefs: []
  type: TYPE_NORMAL
- en: AbortSignal integration makes cancellation explicit and standardized. Instead
    of manually calling `destroy()` on streams, you abort a signal, and the pipeline
    handles the cleanup.
  prefs: []
  type: TYPE_NORMAL
- en: Real-World Pipeline Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's implement a few complete pipelines that demonstrate all the concepts we've
    covered.
  prefs: []
  type: TYPE_NORMAL
- en: '**1) Log File Processing**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a large log file, parse each line as JSON, filter by log level, and write
    to separate output files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The `parseLines` generator handles a common challenge in line-based stream
    processing: chunks don''t align with line boundaries. A chunk might end mid-line,
    splitting `{"level":"ERROR"...` across two chunks. The solution uses buffer accumulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: When you split on `\n`, the last array element is either empty (if the chunk
    ended with a newline) or an incomplete line. Popping that element and saving it
    means the next chunk appends to it, completing the line. After the loop ends,
    any remaining buffered data gets processed—handling files that don't end with
    a newline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `try/catch` around `JSON.parse()` is critical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Without error handling, a single malformed JSON line crashes the entire pipeline,
    losing all progress. With it, the pipeline logs the error and continues. Real-world
    log files contain corrupted entries, so the pipeline needs to handle invalid data
    without stopping.
  prefs: []
  type: TYPE_NORMAL
- en: '`LevelSplitter` both filters data to a side channel and passes all data through:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Every log entry continues down the main pipeline, but ERROR-level logs are
    *also* written to `errors.log`. This creates a branching pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This approach is memory-efficient. Reading the file once and splitting in-stream
    uses constant memory. Two separate pipelines would double I/O and memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `{ objectMode: true }` option is essential because this transform receives
    JavaScript objects from `parseLines`, not buffers. When writing to the side destinations,
    we convert back to JSON strings with `JSON.stringify(log) + "\n"`. Parse once,
    work with objects in the pipeline, serialize only when writing to disk.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The splitters chain in sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Each splitter calls `this.push(log)`, passing objects through. The final destination
    `all.log` receives objects too, but since Writable streams automatically call
    `toString()` on objects, you'd want to add a final transform that serializes to
    JSON for proper formatting (we've simplified this for clarity, but in production
    you'd add `async function* serializeJSON(source) { for await (const obj of source)
    yield JSON.stringify(obj) + "\n"; }` before the final destination).
  prefs: []
  type: TYPE_NORMAL
- en: '**2) CSV Import with Validation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a CSV file, parse rows, validate, and insert into a database with batching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This pipeline uses composable generator functions for data transformation and
    batching for database operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike `parseLines`, `parseCSV` maintains state across chunks—it needs to remember
    the header row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The `headers` variable persists for the generator''s lifetime, capturing the
    first line and using it to structure all subsequent rows. Raw CSV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Becomes structured objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We don't load the entire CSV into memory—each row is processed as data flows
    through.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `validate` generator filters without modifying data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Valid rows continue downstream. Invalid rows are logged but not yielded, preventing
    bad data from reaching the database.
  prefs: []
  type: TYPE_NORMAL
- en: Throwing an error on invalid data would crash the pipeline and lose all progress.
    Real-world data is messy—logging and continuing lets you review errors after the
    import completes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `batch` generator is essential for database operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'This transforms a stream of individual items into a stream of batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Database round-trips are expensive. Batching into groups of 100 can provide
    a 100x speedup over inserting one row at a time. The batch size is a trade-off:
    too small means many round-trips and slow performance; too large means high memory
    usage, timeout risk, and harder error recovery. Most databases work well with
    batch sizes between 100-1000.'
  prefs: []
  type: TYPE_NORMAL
- en: The `if (batch.length > 0)` after the loop handles the final partial batch.
    Without this check, trailing rows get silently dropped.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DatabaseWriter` writable handles async I/O:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: The `_write` method can be async, but you must still call the callback. Call
    `callback()` with no arguments on success, or `callback(err)` to propagate errors.
  prefs: []
  type: TYPE_NORMAL
- en: While `await this.db.insertMany(batch)` runs, the stream is paused. The next
    batch won't be sent until the current insert completes, preventing database overload.
  prefs: []
  type: TYPE_NORMAL
- en: 'The arrow functions `(source) => validate(source, mySchema)` let you pass additional
    arguments to generators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: You're creating specialized versions of generic generators for this specific
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: This entire pipeline uses roughly constant memory regardless of file size. CSV
    parsing keeps only the current line in memory. Validation processes one row at
    a time. Batching holds at most 100 rows. Writing processes only the current batch.
    A 10GB CSV file uses the same memory as a 10MB file.
  prefs: []
  type: TYPE_NORMAL
