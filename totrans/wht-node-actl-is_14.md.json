["```js\nreadable.pipe(transform1).pipe(transform2).pipe(writable);\n```", "```js\nimport { createReadStream, createWriteStream } from \"fs\";\nimport { createGzip } from \"zlib\";\n createReadStream(\"app.log\")\n .pipe(createGzip())\n .pipe(createWriteStream(\"app.log.gz\"));\n```", "```js\nconst reader = createReadStream(\"input.txt\");\nconst transform = createGzip();\nconst writer = createWriteStream(\"output.gz\");\n reader.pipe(transform).pipe(writer);\n reader.on(\"error\", (err) => {\n console.error(\"Reader error:\", err);\n});\n transform.on(\"error\", (err) => {\n console.error(\"Transform error:\", err);\n});\n writer.on(\"error\", (err) => {\n console.error(\"Writer error:\", err);\n});\n```", "```js\nreader.on(\"error\", (err) => {\n reader.destroy();\n transform.destroy();\n writer.destroy();\n console.error(\"Pipeline failed:\", err);\n});\n transform.on(\"error\", (err) => {\n reader.destroy();\n transform.destroy();\n writer.destroy();\n console.error(\"Pipeline failed:\", err);\n});\n writer.on(\"error\", (err) => {\n reader.destroy();\n transform.destroy();\n writer.destroy();\n console.error(\"Pipeline failed:\", err);\n});\n```", "```js\nconst writer = writable();\nreadable.pipe(writer);\n // Later, disconnect\nreadable.unpipe(writer);\n```", "```js\nreadable.unpipe();\n```", "```js\nsocket.on(\"data\", (chunk) => {\n if (shouldRedirect(chunk)) {\n socket.unpipe(fileWriter);\n socket.pipe(differentWriter);\n }\n});\n```", "```js\nimport { pipeline } from \"stream\";\n pipeline(readable, transform, writable, (err) => {\n if (err) {\n console.error(\"Pipeline failed:\", err);\n } else {\n console.log(\"Pipeline succeeded\");\n }\n});\n```", "```js\npipeline(stream1, stream2, ..., streamN, callback)\n```", "```js\nimport { pipeline } from \"stream\";\nimport { createReadStream, createWriteStream } from \"fs\";\nimport { createGzip } from \"zlib\";\n pipeline(\n createReadStream(\"input.txt\"),\n createGzip(),\n createWriteStream(\"output.gz\"),\n (err) => {\n if (err) {\n console.error(\"Compression failed:\", err);\n } else {\n console.log(\"Compression succeeded\");\n }\n }\n);\n```", "```js\nimport { pipeline } from \"stream/promises\";\nimport { createReadStream, createWriteStream } from \"fs\";\nimport { createGzip } from \"zlib\";\n try {\n await pipeline(\n createReadStream(\"input.txt\"),\n createGzip(),\n createWriteStream(\"output.gz\")\n );\n console.log(\"Compression succeeded\");\n} catch (err) {\n console.error(\"Compression failed:\", err);\n}\n```", "```js\nfunction simplifiedPipeline(...args) {\n const callback = args.pop();\n const streams = args;\n // Connect streams with pipe()\n for (let i = 0; i < streams.length - 1; i++) {\n streams[i].pipe(streams[i + 1]);\n }\n // Track completion\n const lastStream = streams[streams.length - 1];\n lastStream.on(\"finish\", () => {\n destroyAll(streams);\n callback();\n });\n // Handle errors\n for (const stream of streams) {\n stream.on(\"error\", (err) => {\n destroyAll(streams);\n callback(err);\n });\n }\n}\n function destroyAll(streams) {\n for (const stream of streams) {\n stream.destroy();\n }\n}\n```", "```js\nimport { pipeline } from \"stream/promises\";\nimport { createReadStream, createWriteStream } from \"fs\";\n await pipeline(\n createReadStream(\"input.txt\"),\n async function* (source) {\n for await (const chunk of source) {\n yield chunk.toString().toUpperCase();\n }\n },\n createWriteStream(\"output.txt\")\n);\n```", "```js\nasync function uppercase(source) {\n for await (const chunk of source) {\n yield chunk.toString().toUpperCase();\n }\n}\n await pipeline(\n createReadStream(\"input.txt\"),\n uppercase,\n createWriteStream(\"output.txt\")\n);\n```", "```js\nawait pipeline(\n createReadStream(\"log.txt\"),\n async function* (source) {\n let buffer = \"\";\n for await (const chunk of source) {\n buffer += chunk.toString();\n const lines = buffer.split(\"\\n\");\n buffer = lines.pop();\n for (const line of lines) {\n yield line + \"\\n\";\n }\n }\n if (buffer) yield buffer;\n },\n async function* (source) {\n for await (const line of source) {\n if (!line.startsWith(\"#\")) {\n yield line;\n }\n }\n },\n createWriteStream(\"filtered.txt\")\n);\n```", "```js\nimport { rename, unlink } from \"fs/promises\";\n const tempFile = \"output.tmp\";\nconst finalFile = \"output.dat\";\n try {\n await pipeline(source, transform, createWriteStream(tempFile));\n await rename(tempFile, finalFile);\n} catch (err) {\n await unlink(tempFile); // Clean up partial file\n throw err;\n}\n```", "```js\nconst tx = await db.beginTransaction();\n try {\n await pipeline(\n source,\n transform,\n new DatabaseWriter(tx)\n );\n await tx.commit();\n} catch (err) {\n await tx.rollback();\n throw err;\n}\n```", "```js\nclass SourceStream extends Readable {\n _read() {\n const err = new Error(\"Read failed\");\n err.code = \"ERR_SOURCE_READ\";\n this.destroy(err);\n }\n}\n pipeline(source, transform, destination, (err) => {\n if (err && err.code === \"ERR_SOURCE_READ\") {\n console.error(\"Source read error:\", err);\n } else if (err) {\n console.error(\"Other error:\", err);\n }\n});\n```", "```js\nimport { pipeline, finished } from \"stream\";\n const transform = createSomeTransform();\n finished(transform, (err) => {\n if (err) {\n console.error(\"Transform specifically failed:\", err);\n }\n});\n pipeline(source, transform, destination, (err) => {\n if (err) {\n console.error(\"Overall pipeline failed:\", err);\n }\n});\n```", "```js\nimport { finished } from \"stream\";\n finished(someStream, (err) => {\n if (err) {\n console.error(\"Stream errored:\", err);\n } else {\n console.log(\"Stream finished successfully\");\n }\n});\n```", "```js\nimport { finished } from \"stream/promises\";\n try {\n await finished(someStream);\n console.log(\"Stream finished\");\n} catch (err) {\n console.error(\"Stream errored:\", err);\n}\n```", "```js\nimport { finished } from \"stream/promises\";\n await finished(someStream, { cleanup: true }); // Removes listeners after completion\n```", "```js\nconst dest1 = createWriteStream(\"output1.txt\");\nconst dest2 = createWriteStream(\"output2.txt\");\n source.pipe(dest1);\nsource.pipe(dest2);\n await Promise.all([\n finished(dest1),\n finished(dest2),\n]);\n console.log(\"Both destinations finished\");\n```", "```js\nasync function pipelineWithRetry(maxRetries) {\n for (let attempt = 0; attempt <= maxRetries; attempt++) {\n try {\n await pipeline(source(), transform, destination);\n return; // Success\n } catch (err) {\n if (isTransientError(err) && attempt < maxRetries) {\n console.log(`Attempt ${attempt + 1} failed, retrying...`);\n await delay(1000 * Math.pow(2, attempt)); // Exponential backoff\n } else {\n throw err; // Give up\n }\n }\n }\n}\n function isTransientError(err) {\n return err.code === \"ECONNRESET\" || err.code === \"ETIMEDOUT\"; // Network errors\n}\n```", "```js\ntry {\n await pipeline(primarySource, transform, destination);\n} catch (err) {\n console.warn(\"Primary source failed, trying fallback\");\n await pipeline(fallbackSource, transform, destination);\n}\n```", "```js\ntry {\n await pipeline(source, transform, primaryDest);\n} catch (err) {\n console.warn(\"Primary destination failed, trying backup\");\n await pipeline(source(), transform, backupDest);\n}\n```", "```js\nconst temp = \"output.tmp\";\nconst final = \"output.dat\";\n try {\n await pipeline(source, transform, createWriteStream(temp));\n await rename(temp, final);\n} catch (err) {\n await unlink(temp).catch(() => {}); // Clean up, ignore errors\n throw err;\n}\n```", "```js\nawait pipeline(source, transform, createWriteStream(\"output.log\", { flags: \"a\" }));\n```", "```js\nconst tx = await db.beginTransaction();\ntry {\n await pipeline(source, transform, new DatabaseWriter(tx));\n await tx.commit();\n} catch (err) {\n await tx.rollback();\n throw err;\n}\n```", "```js\nawait pipeline(source, transform, createWriteStream(\"output.dat\"));\nawait writeFile(\"output.dat.complete\", \"\");\n```", "```js\nconst source = createReadStream(\"input.txt\");\nconst dest = createWriteStream(\"output.txt\");\n pipeline(source, dest, (err) => {\n if (err) {\n console.error(\"Pipeline stopped:\", err.message);\n }\n});\n // Later, cancel the pipeline (e.g., after user action)\nsetTimeout(() => {\n source.destroy(new Error(\"Cancelled by user\"));\n}, 100);\n```", "```js\nsource.destroy();\n```", "```js\nwritable.end(); // Finish writing buffered data, then close\n```", "```js\nfor await (const chunk of readableStream) {\n await processAsync(chunk); // Stream waits for this\n}\n```", "```js\nconst source = createReadStream(\"input.txt\");\nconst dest = createWriteStream(\"output.txt\");\n for await (const chunk of source) {\n const transformed = chunk.toString().toUpperCase();\n const ok = dest.write(transformed);\n if (!ok) {\n await new Promise((resolve) => dest.once(\"drain\", resolve));\n }\n}\n dest.end();\n```", "```js\nasync function* transform(source) {\n for await (const chunk of source) {\n yield chunk.toString().toUpperCase();\n }\n}\n const source = createReadStream(\"input.txt\");\nconst transformed = Readable.from(transform(source));\nconst dest = createWriteStream(\"output.txt\");\n await pipeline(transformed, dest);\n```", "```js\nasync function* toUppercase(source) {\n for await (const chunk of source) {\n yield chunk.toString().toUpperCase();\n }\n}\n async function* filterEmpty(source) {\n for await (const line of source) {\n if (line.trim().length > 0) {\n yield line;\n }\n }\n}\n await pipeline(\n source,\n toUppercase,\n filterEmpty,\n dest\n);\n```", "```js\nasync function* parseJSON(source) {\n for await (const line of source) {\n yield JSON.parse(line);\n }\n}\n async function* extractField(source, field) {\n for await (const obj of source) {\n yield obj[field];\n }\n}\n await pipeline(\n source,\n parseJSON,\n (source) => extractField(source, \"name\"),\n dest\n);\n```", "```js\n// WRONG - loses backpressure\nfor await (const chunk of source) {\n slowOperation(chunk); // No await! Loop continues immediately\n}\n // CORRECT - maintains backpressure\nfor await (const chunk of source) {\n await slowOperation(chunk); // Stream waits until this completes\n}\n```", "```js\nfunction createCSVParser() {\n return new Transform({\n objectMode: true,\n transform(chunk, encoding, callback) {\n const lines = chunk.toString().split(\"\\n\");\n for (const line of lines) {\n if (line.trim()) {\n this.push(line.split(\",\"));\n }\n }\n callback();\n },\n });\n}\n // Use in multiple pipelines\nawait pipeline(source1, createCSVParser(), dest1);\nawait pipeline(source2, createCSVParser(), dest2);\n```", "```js\nfunction createFieldExtractor(fields) {\n return new Transform({\n objectMode: true,\n transform(obj, encoding, callback) {\n const extracted = {};\n for (const field of fields) {\n extracted[field] = obj[field];\n }\n this.push(extracted);\n callback();\n },\n });\n}\n await pipeline(\n source,\n createFieldExtractor([\"name\", \"email\"]),\n dest\n);\n```", "```js\nfunction createProcessingPipeline(source, dest) {\n return pipeline(\n source,\n createCSVParser(),\n createFieldExtractor([\"name\", \"email\"]),\n createValidator(),\n dest\n );\n}\n await createProcessingPipeline(source1, dest1);\nawait createProcessingPipeline(source2, dest2);\n```", "```js\nconst parseCSV = async function* (source) {\n for await (const chunk of source) {\n const lines = chunk.toString().split(\"\\n\");\n for (const line of lines) {\n if (line.trim()) {\n yield line.split(\",\");\n }\n }\n }\n};\n const extractFields = (fields) =>\n async function* (source) {\n for await (const obj of source) {\n const extracted = {};\n for (const field of fields) {\n extracted[field] = obj[field];\n }\n yield extracted;\n }\n };\n await pipeline(\n source,\n parseCSV,\n extractFields([\"name\", \"email\"]),\n dest\n);\n```", "```js\nfunction createValidationSegment(schema, errorDest) {\n const valid = new PassThrough({ objectMode: true });\n const invalid = new PassThrough({ objectMode: true });\n invalid.pipe(errorDest);\n return new Transform({\n objectMode: true,\n transform(obj, encoding, callback) {\n if (schema.validate(obj)) {\n this.push(obj);\n } else {\n invalid.write(obj);\n }\n callback();\n },\n });\n}\n```", "```js\nconst errorLog = createWriteStream(\"errors.log\");\n await pipeline(\n source,\n createValidationSegment(mySchema, errorLog),\n dest\n);\n```", "```js\nfunction createConditionalSegment(condition, trueTransform, falseTransform) {\n return new Transform({\n objectMode: true,\n async transform(obj, encoding, callback) {\n try {\n const result = await condition(obj);\n const transform = result ? trueTransform : falseTransform;\n transform.write(obj);\n callback();\n } catch (err) {\n callback(err);\n }\n },\n });\n}\n```", "```js\nsource.pipe(dest1);\nsource.pipe(dest2);\n```", "```js\nconst pass1 = new PassThrough();\nconst pass2 = new PassThrough();\n source.on(\"data\", (chunk) => {\n pass1.write(chunk);\n pass2.write(chunk);\n});\n source.on(\"end\", () => {\n pass1.end();\n pass2.end();\n});\n pass1.pipe(dest1);\npass2.pipe(dest2);\n```", "```js\nclass FanOut extends Writable {\n constructor(destinations, options) {\n super(options);\n this.destinations = destinations;\n }\n _write(chunk, encoding, callback) {\n const allReady = [];\n // Write to all destinations and check for backpressure\n for (const dest of this.destinations) {\n const canContinue = dest.write(chunk, encoding);\n if (!canContinue) {\n // Destination buffer is full, wait for drain\n allReady.push(\n new Promise((resolve) => {\n dest.once('drain', resolve);\n })\n );\n }\n }\n // If any destination signaled backpressure, wait for all to drain\n if (allReady.length > 0) {\n Promise.all(allReady)\n .then(() => callback())\n .catch((err) => callback(err));\n } else {\n // All writes succeeded without backpressure\n callback();\n }\n }\n _final(callback) {\n // End all destinations when this stream ends\n for (const dest of this.destinations) {\n dest.end();\n }\n callback();\n }\n}\n```", "```js\npipeline(source, new FanOut([dest1, dest2]), (err) => {\n // Pipeline done\n});\n```", "```js\nimport { pipeline } from \"stream/promises\";\n const controller = new AbortController();\n try {\n await pipeline(source, transform, dest, { signal: controller.signal });\n} catch (err) {\n if (err.name === \"AbortError\") {\n console.log(\"Pipeline cancelled\");\n } else {\n throw err;\n }\n}\n // Note: You can also check err.code === 'ABORT_ERR' which is more robust\n// since the code property is harder to accidentally modify\n // To cancel: controller.abort();\n```", "```js\nconst signal = AbortSignal.timeout(5000); // 5 second timeout\n try {\n await pipeline(source, transform, dest, { signal });\n console.log(\"Pipeline completed\");\n} catch (err) {\n if (err.name === \"AbortError\") {\n console.log(\"Pipeline timed out\");\n } else {\n throw err;\n }\n}\n```", "```js\nconst userCancel = new AbortController();\nconst timeout = AbortSignal.timeout(10000);\n const signal = AbortSignal.any([userCancel.signal, timeout]);\n try {\n await pipeline(source, transform, dest, { signal });\n} catch (err) {\n if (err.name === \"AbortError\") {\n console.log(\"Cancelled by either user or timeout\");\n } else {\n throw err;\n }\n}\n```", "```js\nimport { pipeline } from \"stream/promises\";\nimport { createReadStream, createWriteStream } from \"fs\";\nimport { Transform } from \"stream\";\n async function* parseLines(source) {\n let buffer = \"\";\n for await (const chunk of source) {\n buffer += chunk.toString();\n const lines = buffer.split(\"\\n\");\n buffer = lines.pop();\n for (const line of lines) {\n if (line.trim()) {\n try {\n yield JSON.parse(line);\n } catch (err) {\n console.error(\"Parse error:\", err);\n }\n }\n }\n }\n if (buffer.trim()) {\n try {\n yield JSON.parse(buffer);\n } catch (err) {\n console.error(\"Parse error:\", err);\n }\n }\n}\n class LevelSplitter extends Transform {\n constructor(level, dest, options) {\n super({ ...options, objectMode: true });\n this.level = level;\n this.dest = dest;\n }\n _transform(log, encoding, callback) {\n if (log.level === this.level) {\n this.dest.write(JSON.stringify(log) + \"\\n\");\n }\n this.push(log);\n callback();\n }\n}\n const errorDest = createWriteStream(\"errors.log\");\nconst warnDest = createWriteStream(\"warnings.log\");\n await pipeline(\n createReadStream(\"app.log\"),\n parseLines,\n new LevelSplitter(\"ERROR\", errorDest),\n new LevelSplitter(\"WARN\", warnDest),\n createWriteStream(\"all.log\")\n);\n```", "```js\nlet buffer = \"\";\nfor await (const chunk of source) {\n buffer += chunk.toString();\n const lines = buffer.split(\"\\n\");\n buffer = lines.pop(); // Save incomplete line for next chunk\n // Process complete lines...\n}\n```", "```js\ntry {\n yield JSON.parse(line);\n} catch (err) {\n console.error(\"Parse error:\", err);\n}\n```", "```js\n_transform(log, encoding, callback) {\n if (log.level === this.level) {\n this.dest.write(JSON.stringify(log) + \"\\n\"); // Side channel\n }\n this.push(log); // Pass through to next stage\n callback();\n}\n```", "```js\nLogs → parseLines → [All logs continue]\n ↓\n ERROR logs → errors.log\n ↓ [All logs continue]\n WARN logs → warnings.log\n ↓ [All logs continue]\n All logs → all.log\n```", "```js\nparseLines → LevelSplitter(\"ERROR\") → LevelSplitter(\"WARN\") → all.log\n```", "```js\nasync function* parseCSV(source) {\n let buffer = \"\";\n let headers = null;\n for await (const chunk of source) {\n buffer += chunk.toString();\n const lines = buffer.split(\"\\n\");\n buffer = lines.pop();\n for (const line of lines) {\n if (!headers) {\n headers = line.split(\",\");\n } else {\n const values = line.split(\",\");\n const row = {};\n for (let i = 0; i < headers.length; i++) {\n row[headers[i]] = values[i];\n }\n yield row;\n }\n }\n }\n}\n async function* validate(source, schema) {\n for await (const row of source) {\n if (schema.validate(row)) {\n yield row;\n } else {\n console.error(\"Invalid row:\", row);\n }\n }\n}\n async function* batch(source, size) {\n let batch = [];\n for await (const item of source) {\n batch.push(item);\n if (batch.length >= size) {\n yield batch;\n batch = [];\n }\n }\n if (batch.length > 0) {\n yield batch;\n }\n}\n class DatabaseWriter extends Writable {\n constructor(db, options) {\n super({ ...options, objectMode: true });\n this.db = db;\n }\n async _write(batch, encoding, callback) {\n try {\n await this.db.insertMany(batch);\n callback();\n } catch (err) {\n callback(err);\n }\n }\n}\n await pipeline(\n createReadStream(\"data.csv\"),\n parseCSV,\n (source) => validate(source, mySchema),\n (source) => batch(source, 100),\n new DatabaseWriter(db)\n);\n```", "```js\nlet buffer = \"\";\nlet headers = null; // Persists across all chunks\n for await (const chunk of source) {\n // ... process chunks\n if (!headers) {\n headers = line.split(\",\"); // First line becomes headers\n } else {\n // Subsequent lines become data objects\n const values = line.split(\",\");\n const row = {};\n for (let i = 0; i < headers.length; i++) {\n row[headers[i]] = values[i];\n }\n yield row;\n }\n}\n```", "```js\nname,email,age\nAlice,alice@example.com,30\nBob,bob@example.com,25\n```", "```js\n{ name: \"Alice\", email: \"alice@example.com\", age: \"30\" }\n{ name: \"Bob\", email: \"bob@example.com\", age: \"25\" }\n```", "```js\nasync function* validate(source, schema) {\n for await (const row of source) {\n if (schema.validate(row)) {\n yield row; // Valid rows continue\n } else {\n console.error(\"Invalid row:\", row); // Invalid rows logged, not yielded\n }\n }\n}\n```", "```js\nasync function* batch(source, size) {\n let batch = [];\n for await (const item of source) {\n batch.push(item);\n if (batch.length >= size) {\n yield batch; // Emit full batch\n batch = []; // Reset for next batch\n }\n }\n if (batch.length > 0) {\n yield batch; // Don't forget partial final batch\n }\n}\n```", "```js\nInput:  item1, item2, item3, ..., item100, item101, ...\nOutput: [item1...item100], [item101...item200], ...\n```", "```js\nasync _write(batch, encoding, callback) {\n try {\n await this.db.insertMany(batch);\n callback(); // Signal success\n } catch (err) {\n callback(err); // Signal error\n }\n}\n```", "```js\nawait pipeline(\n createReadStream(\"data.csv\"),\n parseCSV,\n (source) => validate(source, mySchema),\n (source) => batch(source, 100),\n new DatabaseWriter(db)\n);\n```"]