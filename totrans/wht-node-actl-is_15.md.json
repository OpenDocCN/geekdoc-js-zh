["```js\nconst readable = createReadStream(\"largefile.mp4\");\nconst socket = getSocketSomehow();\n readable.pipe(socket);\n```", "```js\nimport { copyFile, constants } from \"fs/promises\";\n // Try copy-on-write first, fall back to sendfile-based copy\nawait copyFile(src, dest, constants.COPYFILE_FICLONE);\n```", "```js\nconst chunks = [];\nreadable.on(\"data\", (chunk) => {\n chunks.push(chunk);\n});\n readable.on(\"end\", () => {\n const combined = Buffer.concat(chunks);\n processData(combined);\n});\n```", "```js\nreadable.on(\"data\", (chunk) => {\n processChunk(chunk);\n});\n```", "```js\nconst str = buffer.toString(\"utf8\");\nconst processedStr = processString(str);\nconst newBuffer = Buffer.from(processedStr, \"utf8\");\n```", "```js\nconst slice = buffer.subarray(10, 50);\n```", "```js\nconst slice = buffer.subarray(10, 50);\nprocessTemporarily(slice);\n // If you need to keep it long-term:\nconst copy = Buffer.from(slice);\n// Now the original buffer can be GC'd\n```", "```js\n// Unnecessary copy:\nconst copy = Buffer.from(originalBuffer);\nwriteStream.write(copy);\n // Just use the original:\nwriteStream.write(originalBuffer);\n```", "```js\nfs.writeSync(fd, buffer1);\nfs.writeSync(fd, buffer2);\nfs.writeSync(fd, buffer3);\n```", "```js\nclass BatchWriter extends Writable {\n _writev(chunks, callback) {\n // chunks is an array: [{ chunk, encoding }, ...]\n const buffers = chunks.map(({ chunk }) => chunk);\n // Write all buffers in one syscall\n fs.writev(this.fd, buffers, (err) => {\n callback(err);\n });\n }\n}\n```", "```js\nwritable.cork();\nwritable.write(chunk1);\nwritable.write(chunk2);\nwritable.write(chunk3);\nwritable.uncork(); // Flushes as one _writev() call\n```", "```js\nclass BatchedSocket extends Writable {\n constructor(socket, options) {\n super(options);\n this.socket = socket;\n }\n _write(chunk, encoding, callback) {\n this.socket.write(chunk, callback);\n }\n _writev(chunks, callback) {\n const buffers = chunks.map((c) => c.chunk);\n const combined = Buffer.concat(buffers);\n this.socket.write(combined, callback);\n }\n}\n```", "```js\n_writev(chunks, callback) {\n const buffers = chunks.map((c) => c.chunk);\n fs.writev(this.fd, buffers, (err) => {\n callback(err);\n });\n}\n```", "```js\nlet pendingWrites = 0;\nlet isCorked = false;\n function writeWithBatching(chunk) {\n pendingWrites++;\n if (pendingWrites === 1 && chunk.length < 4096 && !isCorked) {\n writable.cork();\n isCorked = true;\n }\n writable.write(chunk, (err) => {\n pendingWrites = Math.max(0, pendingWrites - 1);\n if (pendingWrites === 0 && isCorked) {\n writable.uncork();\n isCorked = false;\n }\n });\n}\n```", "```js\nconst reusableBuffer = Buffer.allocUnsafe(65536);\n readable.on(\"data\", (chunk) => {\n chunk.copy(reusableBuffer, 0, 0, chunk.length);\n processBuffer(reusableBuffer.subarray(0, chunk.length));\n});\n```", "```js\nconst buf = Buffer.allocUnsafe(1024);\nconst bytesRead = readDataInto(buf);\nconst safeSlice = buf.subarray(0, bytesRead);\n```", "```js\nclass BufferPool {\n constructor(bufferSize, poolSize) {\n this.bufferSize = bufferSize;\n this.pool = [];\n for (let i = 0; i < poolSize; i++) {\n this.pool.push(Buffer.allocUnsafe(bufferSize));\n }\n }\n acquire() {\n return this.pool.pop() || Buffer.allocUnsafe(this.bufferSize);\n }\n release(buffer) {\n if (this.pool.length < 100) {\n this.pool.push(buffer);\n }\n }\n}\n```", "```js\nconst pool = new BufferPool(16384, 10);\n class PooledReadable extends Readable {\n _read(size) {\n const buffer = pool.acquire();\n readDataInto(buffer, (err, bytesRead) => {\n if (err) {\n pool.release(buffer);\n this.destroy(err);\n } else if (bytesRead === 0) {\n pool.release(buffer);\n this.push(null);\n } else {\n const data = Buffer.from(buffer.subarray(0, bytesRead));\n this.push(data);\n pool.release(buffer);\n }\n });\n }\n}\n```", "```js\nwritable.cork();\nfor (const item of items) {\n writable.write(processItem(item));\n}\nwritable.uncork();\n```", "```js\nwritable.cork();\ntry {\n // ... writes ...\n} finally {\n writable.uncork();\n}\n```", "```js\nwritable.cork(); // counter = 1\nwritable.cork(); // counter = 2\nwritable.uncork(); // counter = 1, no flush\nwritable.uncork(); // counter = 0, flush\n```", "```js\nfunction writeHeader(writable) {\n writable.cork();\n writable.write(header);\n writable.uncork();\n}\n function writeBody(writable) {\n writable.cork();\n for (const chunk of chunks) {\n writable.write(chunk);\n }\n writable.uncork();\n}\n writable.cork();\nwriteHeader(writable); // Nested cork/uncork\nwriteBody(writable); // Nested cork/uncork\nwritable.uncork(); // Final flush\n```", "```js\nlet lastWrite = Date.now();\nlet corked = false;\nlet uncorkTimer = null;\n function adaptiveWrite(chunk) {\n const now = Date.now();\n if (now - lastWrite < 10 && !corked) {\n writable.cork();\n corked = true;\n }\n writable.write(chunk);\n lastWrite = now;\n if (uncorkTimer) clearTimeout(uncorkTimer);\n uncorkTimer = setTimeout(() => {\n if (corked) {\n writable.uncork();\n corked = false;\n }\n uncorkTimer = null;\n }, 10);\n}\n```", "```js\nlet text = \"\";\nreadable.on(\"data\", (chunk) => {\n text += chunk.toString();\n});\n```", "```js\nconst chunks = [];\nreadable.on(\"data\", (chunk) => {\n chunks.push(chunk.toString());\n});\n readable.on(\"end\", () => {\n const text = chunks.join(\"\");\n processText(text);\n});\n```", "```js\nconst buffers = [];\nreadable.on(\"data\", (chunk) => {\n buffers.push(chunk);\n});\n readable.on(\"end\", () => {\n const combined = Buffer.concat(buffers);\n processBuffer(combined);\n});\n```", "```js\nreadable.on(\"data\", (chunk) => {\n processChunk(chunk); // No accumulation\n});\n```", "```js\nconst str = buffer.toString();\nif (str.includes(\"keyword\")) {\n // ...\n}\n```", "```js\nif (buffer.indexOf(\"keyword\") !== -1) {\n // ...\n}\n```", "```js\nreadable.pause();\n // Later, trigger buffer fill without consuming data:\nreadable.read(0);\n```", "```js\nreadable.pause();\nsetupResources();\nreadable.read(0); // Initiates async buffer fill\n // Wrong: buffer likely empty here, _read() hasn't completed yet\n// readable.resume();\n // Right: wait for data to be available\nreadable.once('readable', () => {\n readable.resume(); // Now data is actually available\n});\n```", "```js\n// Slower: three transforms\npipeline(\n source,\n toUpperCase,\n removeWhitespace,\n trimLines,\n dest\n);\n // Faster: one combined transform\npipeline(\n source,\n allInOne, // Does all three transformations\n dest\n);\n```", "```js\nconst combined = new Transform({\n transform(chunk, encoding, callback) {\n let result = toUpperCase(chunk);\n result = removeWhitespace(result);\n result = trimLines(result);\n this.push(result);\n callback();\n },\n});\n```", "```js\nif (shouldTransform) {\n pipeline(source, transform, dest);\n} else {\n pipeline(source, dest);\n}\n```", "```js\nreadable.on(\"data\", (chunk) => {\n processChunk(chunk);\n if (shouldPause()) {\n readable.pause();\n }\n});\n // Later, resume based on readableFlowing:\nif (readable.readableFlowing === false) {\n readable.resume();\n}\n```", "```js\nif (readable.readableFlowing === null) {\n // Stream hasn't been started, use flowing mode\n readable.on(\"data\", handler);\n} else if (readable.readableFlowing === false) {\n // Stream is paused, resume it\n readable.resume();\n}\n```", "```js\nconst start = Date.now();\nlet bytes = 0;\n source.on(\"data\", (chunk) => {\n bytes += chunk.length;\n});\n source.on(\"end\", () => {\n const duration = (Date.now() - start) / 1000;\n const throughput = bytes / duration / 1024 / 1024;\n console.log(`Baseline: ${throughput.toFixed(2)} MB/s`);\n});\n```", "```js\nclass OptimizedWriter extends Writable {\n _writev(chunks, callback) {\n // ... batching logic ...\n }\n}\n```", "```js\nsetInterval(() => {\n const mem = process.memoryUsage();\n console.log(`Heap: ${(mem.heapUsed / 1024 / 1024).toFixed(2)} MB`);\n}, 1000);\n```", "```js\nnode --prof script.js\nnode --prof-process isolate-*.log > profile.txt\n```", "```js\nimport { createReadStream, createWriteStream } from \"fs\";\nimport { pipeline } from \"stream/promises\";\nimport { Writable } from \"stream\";\n const pool = new BufferPool(65536, 10);\n class OptimizedWriter extends Writable {\n constructor(dest, options) {\n super(options);\n this.dest = dest;\n }\n _write(chunk, encoding, callback) {\n this.dest.write(chunk, callback);\n }\n _writev(chunks, callback) {\n const buffers = chunks.map((c) => c.chunk);\n const combined = Buffer.concat(buffers);\n this.dest.write(combined, callback);\n }\n}\n async function optimizedCopy(src, dest) {\n const reader = createReadStream(src, {\n highWaterMark: 65536,\n });\n const writer = new OptimizedWriter(\n createWriteStream(dest, { highWaterMark: 65536 })\n );\n await pipeline(reader, writer);\n}\n await optimizedCopy(\"input.dat\", \"output.dat\");\n```", "```js\nimport { copyFile, constants } from \"fs/promises\";\n // Use copy-on-write if filesystem supports it\nawait copyFile(src, dest, constants.COPYFILE_FICLONE);\n```", "```js\nstrace -c -f node your-script.js\n```", "```js\nstrace -e write,writev -f node your-script.js\n```", "```js\nperf record -F 99 -g node your-script.js\nperf report\n```", "```js\nnode --trace-gc your-script.js\n```", "```js\nnode --trace-gc --trace-gc-verbose your-script.js\n```", "```js\nimport { monitorEventLoopDelay } from 'perf_hooks';\n const h = monitorEventLoopDelay({ resolution: 20 });\nh.enable();\n // Run your stream pipeline\n setInterval(() => {\n // Note: all values are in nanoseconds, divide by 1e6 for milliseconds\n console.log(`p50: ${(h.percentile(50) / 1e6).toFixed(2)}ms`);\n console.log(`p99: ${(h.percentile(99) / 1e6).toFixed(2)}ms`);\n console.log(`max: ${(h.max / 1e6).toFixed(2)}ms`);\n}, 1000);\n```", "```js\nNODE_DEBUG=fs,net,stream node your-script.js\n```"]