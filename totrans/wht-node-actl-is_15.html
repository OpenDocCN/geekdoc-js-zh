<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Zero-Copy, Scatter/Gather I/O & Advanced Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Zero-Copy, Scatter/Gather I/O & Advanced Techniques</h1>
<blockquote>åŽŸæ–‡ï¼š<a href="https://www.thenodebook.com/streams/zero-copy-scatter-gather">https://www.thenodebook.com/streams/zero-copy-scatter-gather</a></blockquote><div class="relative my-6 p-4 border-l-4 rounded-r border-red-500 bg-red-50 dark:bg-red-950/30 text-red-900 dark:text-red-100" role="alert"><div class="flex gap-3"><span class="text-2xl flex-shrink-0">ðŸš¨</span><div class="flex-1"><div class="font-bold text-sm mb-1">Caution</div><div class="text-sm leading-relaxed [&amp;&gt;p]:mb-2 [&amp;&gt;p:last-child]:mb-0"><p class="text-base leading-relaxed mb-4 font-normal">This chapter covers advanced performance optimization techniques. If you feel uncomfortable while reading, feel free to skip this and come back later. The techniques here matter most when you're processing large volumes of data and have already identified I/O as your bottleneck through profiling.</p></div></div></div></div>
<p class="text-base leading-relaxed mb-4 font-normal">Every time you copy a file in Node.js, you're likely copying the same data four times. First from disk into kernel memory. Then from kernel memory into your Node.js process memory. Then from your process memory back into kernel memory for the destination. Finally from kernel memory to the destination disk. Four copies for what should conceptually be one operation.</p>
<p class="text-base leading-relaxed mb-4 font-normal">This matters because copying is expensive. Every copy involves CPU time, memory bandwidth, and cache pollution. When you're moving gigabytes of data through a stream pipeline, unnecessary copies become the bottleneck. Your disk might be capable of 2GB/sec, but you're getting 500MB/sec because you're spending CPU cycles shuffling bytes around in memory instead of actually doing useful work.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The performance techniques we're covering in this chapter  -  zero-copy, scatter/gather I/O, buffer pooling  -  aren't academic curiosities. They're the difference between a stream pipeline that maxes out your hardware and one that wastes resources on bookkeeping. We're going to look at how data actually moves through a system, where the copies happen, and how to eliminate them. Then we'll explore techniques for batching I/O operations to reduce syscall overhead, and strategies for managing buffers to minimize garbage collection pressure.</p>
<p class="text-base leading-relaxed mb-4 font-normal">By the end, you'll understand how to make streams faster, why they're slow in the first place, and which optimizations actually matter for your workload.</p>
<h2 id="what-is-zero-copy" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">What is Zero-Copy?</h2>
<p class="text-base leading-relaxed mb-4 font-normal">The term "zero-copy" gets thrown around loosely, and it's not as magical as it sounds. We need to be precise about what it actually means.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Traditional I/O involves copying data between different regions of memory. Your operating system maintains a strict separation between kernel space (where the OS kernel runs) and user space (where your application runs). This separation matters for system stability  -  it prevents applications from corrupting kernel memory or interfering with each other.</p>
<p class="text-base leading-relaxed mb-4 font-normal">When you read a file in Node.js, here's what typically happens. The operating system reads data from disk into a kernel buffer. This is the first copy  -  from disk to kernel memory. Then, because your Node.js process can't directly access kernel memory, the OS copies that data from the kernel buffer into your process's memory space. That's the second copy. When you write that data to another file, the process reverses: your process writes to a buffer in user space, the OS copies it into a kernel buffer (third copy), and then writes it to disk (fourth copy).</p>
<p class="text-base leading-relaxed mb-4 font-normal">Four copies. For a simple file copy operation.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Each copy operation involves several expensive steps. The CPU must execute instructions to read from one memory location and write to another. This consumes CPU cycles that could be spent on actual computation. It also pollutes the CPU cache  -  when you copy megabytes of data through the cache, you evict useful data that other parts of your program need, causing cache misses later.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Memory bandwidth is also finite. Modern systems have very fast memory, but there's still a limit to how many bytes per second can move between CPU and RAM. When you're copying the same data multiple times, you're consuming that bandwidth repeatedly for the same bytes. This becomes the bottleneck when moving large amounts of data.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Consider a web server serving a 1GB video file. With traditional I/O, that 1GB gets copied four times, but each CPU-mediated copy involves both a read and a write on the memory bus. The actual bandwidth breakdown:</p>
<ul class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:disc;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">Disk to kernel buffer: DMA write (1GB)</li>
<li class="ml-2 font-normal" style="display:list-item">Kernel to user space: CPU read + write (2GB)</li>
<li class="ml-2 font-normal" style="display:list-item">User space to kernel: CPU read + write (2GB)</li>
<li class="ml-2 font-normal" style="display:list-item">Kernel to network: DMA read (1GB)</li>
</ul>
<p class="text-base leading-relaxed mb-4 font-normal">That's 6GB of memory bandwidth consumed to serve 1GB of actual data. On a system capable of 50GB/sec memory bandwidth, that single file transfer consumes 120ms just in memory operations, before even accounting for disk or network I/O latency.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Zero-copy is the technique of eliminating some or all of these intermediate copies. The "zero" is aspirational  -  you can't truly have zero copies because data has to move from one place to another  -  but you can avoid copies between kernel and user space, which is where most of the CPU overhead occurs.</p>
<p class="text-base leading-relaxed mb-4 font-normal">If you're just moving data from one file to another without examining or modifying it, why copy it into your process's memory at all? The kernel already has the data. It could just move it directly from the source file's kernel buffer to the destination file's kernel buffer, bypassing your process entirely.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> syscall on Linux does exactly this. You tell the kernel "copy data from file descriptor A to file descriptor B," and it does so entirely within kernel space. No copying to user space. No CPU cycles spent in your process moving bytes around. The data flows from source to destination with the fewest possible copies.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The implementation varies by operating system. Linux has <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> and <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">splice()</code>. FreeBSD and macOS have <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> with slightly different semantics. Windows has <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">TransmitFile()</code>. These are OS-level primitives that applications can use to achieve zero-copy transfers in specific scenarios.</p>
<p class="text-base leading-relaxed mb-4 font-normal">On modern systems with DMA (Direct Memory Access), it's even better. DMA controllers can transfer data between devices and memory without involving the CPU at all. The disk controller reads data from disk and writes it directly to memory. The network card reads data from memory and sends it over the wire. The CPU just sets up the transfer and then does other work while the DMA controller handles the actual data movement.</p>
<p class="text-base leading-relaxed mb-4 font-normal">When zero-copy works perfectly, the CPU's job is reduced to setting up transfers. The data moves via DMA and kernel-to-kernel copies, never touching user space, never consuming CPU cycles for memcpy operations. The CPU initiates the transfer with a syscall, the kernel programs the DMA controllers, and the data flows directly from disk to network (or file to file) without ever entering the CPU cache.</p>
<p class="text-base leading-relaxed mb-4 font-normal">This is the ideal case. In practice, there are still some copies. The disk might read into a kernel buffer first, then the kernel copies (or maps) that buffer to the network card's DMA region. But compared to four copies (disk â†’ kernel â†’ user â†’ kernel â†’ destination), two kernel-level copies is a big improvement.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Zero-copy only works when you don't need to examine or modify the data. The moment you need to transform data  -  parse it, compress it, encrypt it  -  you need access to it in user space. You can't ask the kernel to "compress this data before writing it" (well, you can in some specialized cases with kernel-level compression, but it's not broadly applicable). You need to copy data into your process, transform it, and copy it back out.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Zero-copy gives you maximum throughput when data flows untouched. The second you need to process the data, you're back to traditional I/O with all its copying overhead. This is why zero-copy is most applicable to proxy scenarios: an HTTP proxy forwarding requests, a file server serving static files, a reverse proxy routing traffic. In these cases, you're just moving data from one socket or file to another without modification.</p>
<p class="text-base leading-relaxed mb-4 font-normal">There's also a practical limitation: zero-copy only works for certain source-destination combinations. Linux's <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> requires the source to be a regular file (something that supports <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">mmap()</code>), not a pipe or socket. You can't use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> for socket-to-socket transfers at all on Linux  -  the input must be a file. For socket-to-socket zero-copy, you'd need <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">splice()</code> with an intermediate pipe, which is more complex to implement.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Understanding these constraints helps you reason about when your streams will be fast (zero-copy applies) and when they'll be slower (fallback to traditional I/O). You can't make every stream zero-copy  -  that's impossible. But you can structure your pipelines so the high-volume data paths use zero-copy when available.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Let's see how this applies to Node.js streams.</p>
<h2 id="zero-copy-in-nodejs-streams" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Zero-Copy in Node.js Streams</h2>
<p class="text-base leading-relaxed mb-4 font-normal">A common misconception is that Node.js automatically uses zero-copy techniques like <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> when you pipe a file to a socket. It doesn't.</p>
<p class="text-base leading-relaxed mb-4 font-normal">When you write code like this:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> readable</span><span style="color:#F97583"> =</span><span style="color:#B392F0"> createReadStream</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"largefile.mp4"</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> socket</span><span style="color:#F97583"> =</span><span style="color:#B392F0"> getSocketSomehow</span><span style="color:#E1E4E8">();</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">pipe</span><span style="color:#E1E4E8">(socket);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Node.js does <strong class="font-bold">not</strong> automatically use the <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> system call. The standard stream <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">pipe()</code> method reads data through user space buffers using regular <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read()</code> and <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">write()</code> operations. The data flows from disk into a kernel buffer, then into your Node.js process memory, then back to a kernel buffer, and finally to the socket. That's the traditional four-copy path we discussed earlier.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Despite claims you may have read elsewhere, Node.js streams don't use kernel-level zero-copy. Here's what's actually true:</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">libuv does have <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">uv_fs_sendfile()</code></strong>, but it's used for file-to-file operations like <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.copyFile()</code>, not for stream piping. When you call <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.copyFile()</code>, libuv can use the kernel's efficient file copying mechanisms. On Linux, this might use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> or copy-on-write reflinks (with <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">COPYFILE_FICLONE</code> flag) if the filesystem supports them. But this is file-to-file, not file-to-socket.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Node.js streams use buffered I/O through JavaScript.</strong> When you <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">pipe()</code>, Node sets up event listeners. The readable stream emits <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">data</code> events with Buffer chunks. The writable stream's <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">write()</code> method is called for each chunk. This all happens in JavaScript, with data passing through V8's heap. There's no magic kernel bypass.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Why doesn't Node.js use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> for file-to-socket streaming? Several reasons:</p>
<ol class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:decimal;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">
<p class="text-base leading-relaxed mb-4 font-normal"><code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> behaves differently on Linux, macOS, and FreeBSD. Windows uses <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">TransmitFile()</code> with different semantics. The abstraction cost adds up.</p>
</li>
<li class="ml-2 font-normal" style="display:list-item">
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">HTTPS complicates things.</strong> Zero-copy requires data to flow untouched through the kernel. Traditional TLS requires encrypting every byte in user space. Linux 4.13+ introduced kernel TLS (kTLS), which can enable zero-copy for TLS traffic by handling encryption in the kernel. However, Node.js doesn't currently use kTLS, so HTTPS traffic still requires user-space encryption. Since most production traffic is HTTPS, the practical benefit of kernel-level zero-copy in Node.js is limited.</p>
</li>
<li class="ml-2 font-normal" style="display:list-item">
<p class="text-base leading-relaxed mb-4 font-normal">Node's stream backpressure mechanism relies on JavaScript callbacks and the <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">drain</code> event. Integrating this with kernel-level <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> is complex.</p>
</li>
<li class="ml-2 font-normal" style="display:list-item">
<p class="text-base leading-relaxed mb-4 font-normal">Node.js had <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> support briefly in its early days, but it was removed after the libeio to libuv transition due to bugs and cross-platform issues.</p>
</li>
</ol>
<p class="text-base leading-relaxed mb-4 font-normal">So when would you actually get zero-copy benefits in Node.js? There are a few scenarios:</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold"><code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.copyFile()</code> for file-to-file copies.</strong> This uses libuv's <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">uv_fs_copyfile()</code> which can use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> or copy-on-write reflinks:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> { copyFile, constants } </span><span style="color:#F97583">from</span><span style="color:#9ECBFF"> "fs/promises"</span><span style="color:#E1E4E8">;</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">// Try copy-on-write first, fall back to sendfile-based copy</span></span>
<span class="line"><span style="color:#F97583">await</span><span style="color:#B392F0"> copyFile</span><span style="color:#E1E4E8">(src, dest, constants.</span><span style="color:#79B8FF">COPYFILE_FICLONE</span><span style="color:#E1E4E8">);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">On filesystems that support reflinks (like Btrfs, XFS, APFS), this creates an instant copy that shares physical blocks until one copy is modified. That's true zero-copy.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Native addons.</strong> If you absolutely need <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> for file-to-socket, you can write a native addon that calls it directly. This requires handling partial writes, backpressure, and platform differences yourself. It's rarely worth it for most applications.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">HTTP/2 with <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">respondWithFile()</code>.</strong> The <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">http2</code> module has <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">http2stream.respondWithFile()</code> and <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">respondWithFD()</code> methods that are optimized for serving files. These still go through user space but are more efficient than manual streaming.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The practical implications: don't structure your code expecting automatic zero-copy magic. Instead, focus on the optimizations you can control  -  buffer sizes, avoiding unnecessary copies in your code, and using <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> for batching. These give you real, measurable wins.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The stream abstraction prioritizes correctness, flexibility, and cross-platform compatibility over maximum raw throughput. For most applications, this is the right trade-off. The scenarios where kernel-level zero-copy matters (serving huge files over HTTP to thousands of concurrent clients) are better handled by specialized software like nginx or a CDN, not Node.js.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The concepts of zero-copy are still useful even without kernel bypass. You can minimize copies in your own code, and that's what we'll focus on next.</p>
<h2 id="memory-mapping" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Memory Mapping</h2>
<p class="text-base leading-relaxed mb-4 font-normal">There's another zero-copy approach worth understanding: memory mapping. Instead of reading a file into a buffer, you can map the file directly into your process's address space. The file's contents appear as a region of memory, and you access it by reading from that memory region.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Memory mapping uses the OS's virtual memory system. The kernel maps the file's pages into your process's address space, but it doesn't actually load the file into physical memory until you access those pages. When you read from a mapped page, the OS loads that page from disk. When you write to it, the OS marks the page as dirty and flushes it back to disk later.</p>
<p class="text-base leading-relaxed mb-4 font-normal">This is zero-copy in the sense that you're not explicitly copying the file's data into a separate buffer. You're accessing the file's data directly via the memory map. Changes you make to the mapped memory are reflected in the file.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Node.js core doesn't expose memory mapping. There's no built-in JavaScript API for <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">mmap()</code>. To use memory mapping in Node.js, you need third-party native addons. The original <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">node-mmap</code> and <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">mmap-io</code> packages are largely unmaintained and may not work with modern Node.js versions. If you need mmap functionality, look for actively maintained forks or consider whether the use case truly requires memory mapping  -  often <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.read()</code> with appropriate offsets works well enough.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Memory mapping is useful for random access to large files. If you need to read specific offsets within a multi-gigabyte file, mapping it into memory lets you treat it like a huge byte array. No need to seek and read chunks  -  just index into the mapped region.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But memory mapping has trade-offs. The OS does apply readahead to memory-mapped files, but the optimization works differently than with <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read()</code>. With <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read()</code>, the kernel knows exactly how much data you want upfront. With mmap, the kernel must detect your access pattern through page faults  -  you access a page, the kernel loads it and potentially prefetches nearby pages. This reactive approach adds latency for initial accesses that <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read()</code> avoids. For pure sequential streaming, a read stream is usually faster.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For streaming, memory mapping rarely makes sense. For random access workloads (like a database), it can work well. Just be aware of the trade-offs and measure performance for your specific use case.</p>
<h2 id="avoiding-unnecessary-buffer-copies-in-your-code" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Avoiding Unnecessary Buffer Copies in Your Code</h2>
<p class="text-base leading-relaxed mb-4 font-normal">Even when Node.js can't use OS-level zero-copy, you can avoid unnecessary copies in your own code. Let's talk about common patterns that introduce extra copies and how to eliminate them.</p>
<p class="text-base leading-relaxed mb-4 font-normal">One frequent culprit is <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.concat()</code>:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> chunks</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> [];</span></span>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"data"</span><span style="color:#E1E4E8">, (</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#E1E4E8">  chunks.</span><span style="color:#B392F0">push</span><span style="color:#E1E4E8">(chunk);</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"end"</span><span style="color:#E1E4E8">, () </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">  const</span><span style="color:#79B8FF"> combined</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">concat</span><span style="color:#E1E4E8">(chunks);</span></span>
<span class="line"><span style="color:#B392F0">  processData</span><span style="color:#E1E4E8">(combined);</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This pattern collects chunks into an array, then concatenates them. The <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.concat()</code> call allocates a new buffer and copies all chunks into it. That's an extra copy.</p>
<p class="text-base leading-relaxed mb-4 font-normal">If you're going to process the data as one contiguous buffer, this copy is unavoidable. But if you can process chunks individually, skip the concatenation:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"data"</span><span style="color:#E1E4E8">, (</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#B392F0">  processChunk</span><span style="color:#E1E4E8">(chunk);</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Process each chunk as it arrives. No intermediate buffer, no concatenation copy.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Another anti-pattern is converting buffers to strings and back:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> str</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> buffer.</span><span style="color:#B392F0">toString</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"utf8"</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> processedStr</span><span style="color:#F97583"> =</span><span style="color:#B392F0"> processString</span><span style="color:#E1E4E8">(str);</span></span>
<span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> newBuffer</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">from</span><span style="color:#E1E4E8">(processedStr, </span><span style="color:#9ECBFF">"utf8"</span><span style="color:#E1E4E8">);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Each conversion allocates new memory and copies data. If your processing can work directly on buffers (using <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">buffer.indexOf()</code>, <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">buffer.subarray()</code>, etc.), avoid string conversion.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Buffer slicing is a zero-copy operation when used correctly:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> slice</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> buffer.</span><span style="color:#B392F0">subarray</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">50</span><span style="color:#E1E4E8">);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This doesn't copy data. It creates a view of the original buffer from byte 10 to byte 50. The slice shares the underlying memory with the original buffer. Modifying the slice modifies the original buffer. This is zero-copy slicing.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Note: Use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">subarray()</code> instead of <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">slice()</code>. <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.slice()</code> has the same behavior, but it's deprecated (DEP0158) because it's inconsistent with <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">TypedArray.prototype.slice()</code> which creates a copy. As of Node.js v25, calling <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">slice()</code> emits runtime deprecation warnings. Use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">subarray()</code> instead  -  it's the recommended API and won't trigger warnings.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But if you then modify the original buffer's length or reallocate it, the slice becomes invalid. And if you slice tiny fragments from many large buffers and keep those slices alive, you're preventing the large buffers from being garbage collected. The small slice holds a reference to the large buffer, keeping it in memory.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The safe pattern: slice for temporary processing, then copy if you need to retain the data:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> slice</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> buffer.</span><span style="color:#B392F0">subarray</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">50</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#B392F0">processTemporarily</span><span style="color:#E1E4E8">(slice);</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">// If you need to keep it long-term:</span></span>
<span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> copy</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">from</span><span style="color:#E1E4E8">(slice);</span></span>
<span class="line"><span style="color:#6A737D">// Now the original buffer can be GC'd</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">One more pattern: avoid <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.from(buffer)</code> unless you explicitly need a copy. If you just need to pass a buffer to another function, pass the original:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D">// Unnecessary copy:</span></span>
<span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> copy</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">from</span><span style="color:#E1E4E8">(originalBuffer);</span></span>
<span class="line"><span style="color:#E1E4E8">writeStream.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(copy);</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">// Just use the original:</span></span>
<span class="line"><span style="color:#E1E4E8">writeStream.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(originalBuffer);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">The writable stream doesn't modify the buffer (unless it's a very unusual stream), so there's no reason to copy it.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Every <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.concat()</code>, <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.from()</code>, and <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">buffer.toString()</code> potentially allocates and copies. Only do these operations when the semantics of your code require them, not out of habit. And remember to use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">subarray()</code> instead of <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">slice()</code> for zero-copy views.</p>
<h2 id="scattergather-io" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Scatter/Gather I/O</h2>
<p class="text-base leading-relaxed mb-4 font-normal">Scatter/gather I/O is a technique for reducing the number of syscalls when working with multiple buffers.</p>
<p class="text-base leading-relaxed mb-4 font-normal">In traditional I/O, if you want to write three buffers to a file, you make three write syscalls:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">fs.</span><span style="color:#B392F0">writeSync</span><span style="color:#E1E4E8">(fd, buffer1);</span></span>
<span class="line"><span style="color:#E1E4E8">fs.</span><span style="color:#B392F0">writeSync</span><span style="color:#E1E4E8">(fd, buffer2);</span></span>
<span class="line"><span style="color:#E1E4E8">fs.</span><span style="color:#B392F0">writeSync</span><span style="color:#E1E4E8">(fd, buffer3);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Each syscall has overhead. The CPU switches from user mode to kernel mode, the kernel processes the request, and the CPU switches back to user mode. For small writes, this overhead can exceed the actual I/O cost.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Let's quantify this overhead. A syscall typically takes 50-200 nanoseconds just for the mode switch, depending on CPU and OS. Then there's the kernel's work of processing the request  -  validating parameters, setting up I/O, etc. For a simple write, this might be another 100-500 nanoseconds. So each syscall has a baseline cost of roughly 150-700 nanoseconds before any actual data movement happens.</p>
<p class="text-base leading-relaxed mb-4 font-normal">If you're writing three 1KB buffers, that's three syscalls, so 450-2100 nanoseconds of pure overhead. The actual writing of 3KB might take only a few hundred nanoseconds on a fast SSD. The syscall overhead can exceed the I/O cost.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Now scale this up. If you're writing 1000 small buffers, you're spending 150-700 microseconds just on syscall overhead. That might not sound like much, but in a high-throughput system processing millions of writes per second, this overhead adds up to real CPU time.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Scatter/gather I/O lets you pass multiple buffers to a single syscall. For writes (gather), you're gathering data from multiple buffers and writing it in one operation. For reads (scatter), you're scattering incoming data into multiple buffers.</p>
<p class="text-base leading-relaxed mb-4 font-normal">On Linux, the <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">writev()</code> syscall handles gather writes. You pass an array of buffers (technically, an array of iovec structures pointing to buffers), and the kernel writes all of them in one operation. This is much more efficient than separate write calls.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The scatter counterpart is <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">readv()</code>, which reads data into multiple buffers. You specify an array of buffers, and the kernel fills them in order. If the read provides more data than the first buffer can hold, it continues into the second buffer, and so on. This is useful when you know the structure of incoming data  -  for example, a fixed-size header followed by variable-length payload. You can scatter-read into a header buffer and a payload buffer in one syscall.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Node.js exposes gather writes through the <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> method on writable streams, but scatter reads aren't directly exposed in the stream API because the stream abstraction doesn't know in advance how many buffers to scatter into. However, the lower-level <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.readv()</code> and <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.writev()</code> functions are available if you need them.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Node.js writable streams expose this via the <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> method. If you're implementing a custom writable stream and you override <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code>, Node will batch writes and call your <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> with an array of chunks instead of calling <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_write()</code> multiple times.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Here's how it works. When a writable stream is corked or when multiple chunks are written rapidly, Node buffers them. If the stream implements <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code>, Node calls it with all buffered chunks:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> BatchWriter</span><span style="color:#F97583"> extends</span><span style="color:#B392F0"> Writable</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#B392F0">  _writev</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">chunks</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">callback</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#6A737D">    // chunks is an array: [{ chunk, encoding }, ...]</span></span>
<span class="line"><span style="color:#F97583">    const</span><span style="color:#79B8FF"> buffers</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> chunks.</span><span style="color:#B392F0">map</span><span style="color:#E1E4E8">(({ </span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8"> }) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> chunk);</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">    // Write all buffers in one syscall</span></span>
<span class="line"><span style="color:#E1E4E8">    fs.</span><span style="color:#B392F0">writev</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">this</span><span style="color:#E1E4E8">.fd, buffers, (</span><span style="color:#FFAB70">err</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#B392F0">      callback</span><span style="color:#E1E4E8">(err);</span></span>
<span class="line"><span style="color:#E1E4E8">    });</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">The <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">chunks</code> array contains objects with <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">chunk</code> and <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">encoding</code> properties. You extract the chunks and pass them to a syscall that supports vectored I/O.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The benefit: instead of N syscalls for N chunks, you make one syscall. For a stream writing many small chunks (like a HTTP response with many small writes), this can cut syscall overhead a lot.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But there's a nuance. Node only calls <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> when multiple chunks are buffered. If chunks arrive slowly (one at a time with gaps between), Node calls <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_write()</code> for each. To force batching, you can cork the stream:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">cork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(chunk1);</span></span>
<span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(chunk2);</span></span>
<span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(chunk3);</span></span>
<span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">uncork</span><span style="color:#E1E4E8">(); </span><span style="color:#6A737D">// Flushes as one _writev() call</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Corking suppresses <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_write()</code> calls and buffers everything. Uncorking flushes the buffer, calling <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> with all buffered chunks.</p>
<p class="text-base leading-relaxed mb-4 font-normal">This is useful when you know you're about to write multiple chunks. Cork before the writes, uncork after, and you get batching even if <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_write()</code> would normally be called.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For scatter reads (the opposite direction), Node doesn't expose <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">readv()</code> directly because readable streams pull data on demand, and it's not clear how many buffers to scatter into. But if you're using low-level <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs</code> APIs, you can use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.readv()</code> to read into multiple buffers in one syscall.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Batching multiple I/O operations into one syscall reduces overhead. Scatter/gather I/O is the mechanism for doing this with multiple buffers.</p>
<h2 id="implementing-_writev-for-maximum-throughput" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Implementing _writev() for Maximum Throughput</h2>
<p class="text-base leading-relaxed mb-4 font-normal">Here's an example of implementing <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> to optimize a writable stream.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Suppose you're writing to a network socket and you want to minimize syscalls. The socket's default behavior calls <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">write()</code> for each chunk. If chunks are small, you're making many syscalls.</p>
<p class="text-base leading-relaxed mb-4 font-normal">By implementing <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code>, you batch writes:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> BatchedSocket</span><span style="color:#F97583"> extends</span><span style="color:#B392F0"> Writable</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">  constructor</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">socket</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">options</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#79B8FF">    super</span><span style="color:#E1E4E8">(options);</span></span>
<span class="line"><span style="color:#79B8FF">    this</span><span style="color:#E1E4E8">.socket </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> socket;</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"/>
<span class="line"><span style="color:#B392F0">  _write</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">encoding</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">callback</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#79B8FF">    this</span><span style="color:#E1E4E8">.socket.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(chunk, callback);</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"/>
<span class="line"><span style="color:#B392F0">  _writev</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">chunks</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">callback</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#F97583">    const</span><span style="color:#79B8FF"> buffers</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> chunks.</span><span style="color:#B392F0">map</span><span style="color:#E1E4E8">((</span><span style="color:#FFAB70">c</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> c.chunk);</span></span>
<span class="line"><span style="color:#F97583">    const</span><span style="color:#79B8FF"> combined</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">concat</span><span style="color:#E1E4E8">(buffers);</span></span>
<span class="line"/>
<span class="line"><span style="color:#79B8FF">    this</span><span style="color:#E1E4E8">.socket.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(combined, callback);</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> concatenates buffers, which does introduce a copy. But it's still a net win if the alternative is many small syscalls.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Here's the trade-off. Without <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code>, you make N syscalls for N chunks. Syscalls are expensive. With <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> that concatenates, you make one copy (concatenation) and one syscall. If syscall overhead exceeds copy overhead, batching wins.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But you can do better. If your underlying I/O mechanism supports true vectored writes (like <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">writev()</code> on a file descriptor), you can avoid the concatenation:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#B392F0">_writev</span><span style="color:#E1E4E8">(chunks, callback) {</span></span>
<span class="line"><span style="color:#F97583">  const</span><span style="color:#79B8FF"> buffers</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> chunks.</span><span style="color:#B392F0">map</span><span style="color:#E1E4E8">((</span><span style="color:#FFAB70">c</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> c.chunk);</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">  fs.</span><span style="color:#B392F0">writev</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">this</span><span style="color:#E1E4E8">.fd, buffers, (</span><span style="color:#FFAB70">err</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#B392F0">    callback</span><span style="color:#E1E4E8">(err);</span></span>
<span class="line"><span style="color:#E1E4E8">  });</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Now you're passing multiple buffers to a vectored I/O syscall. The kernel writes them all without you copying them into a single buffer. This is true gather I/O.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For sockets, Node's net.Socket doesn't expose vectored writes at the JavaScript level, but libuv (which underlies Node) uses them internally where supported. By implementing <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> and letting the socket's internal write handle batching, you benefit from libuv's optimizations.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The key is to always implement <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> when your destination supports batched writes. Even if you have to concatenate buffers, it's often faster than multiple syscalls.</p>
<p class="text-base leading-relaxed mb-4 font-normal">One more pattern: adaptive batching. If chunks are large, batching doesn't help much. If chunks are small, batching matters a lot. You can track chunk sizes and cork/uncork dynamically:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">let</span><span style="color:#E1E4E8"> pendingWrites </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#F97583">let</span><span style="color:#E1E4E8"> isCorked </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> false</span><span style="color:#E1E4E8">;</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">function</span><span style="color:#B392F0"> writeWithBatching</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#E1E4E8">  pendingWrites</span><span style="color:#F97583">++</span><span style="color:#E1E4E8">;</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">  if</span><span style="color:#E1E4E8"> (pendingWrites </span><span style="color:#F97583">===</span><span style="color:#79B8FF"> 1</span><span style="color:#F97583"> &amp;&amp;</span><span style="color:#E1E4E8"> chunk.</span><span style="color:#79B8FF">length</span><span style="color:#F97583"> &lt;</span><span style="color:#79B8FF"> 4096</span><span style="color:#F97583"> &amp;&amp;</span><span style="color:#F97583"> !</span><span style="color:#E1E4E8">isCorked) {</span></span>
<span class="line"><span style="color:#E1E4E8">    writable.</span><span style="color:#B392F0">cork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">    isCorked </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> true</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">  writable.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(chunk, (</span><span style="color:#FFAB70">err</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#E1E4E8">    pendingWrites </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> Math.</span><span style="color:#B392F0">max</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, pendingWrites </span><span style="color:#F97583">-</span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> (pendingWrites </span><span style="color:#F97583">===</span><span style="color:#79B8FF"> 0</span><span style="color:#F97583"> &amp;&amp;</span><span style="color:#E1E4E8"> isCorked) {</span></span>
<span class="line"><span style="color:#E1E4E8">      writable.</span><span style="color:#B392F0">uncork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">      isCorked </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> false</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#E1E4E8">    }</span></span>
<span class="line"><span style="color:#E1E4E8">  });</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This corks on the first small write and uncorks once writes settle. Using the write callback ensures we uncork even if some writes fail. The <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Math.max(0, ...)</code> guards against the counter going negative if something unexpected happens.</p>
<p class="text-base leading-relaxed mb-4 font-normal">This is a simplified heuristic. In production, you'd track timing and chunk sizes more carefully, and add cleanup for stream close/error events. But the idea is solid: batch small writes, skip batching for large writes.</p>
<h2 id="buffer-pooling" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Buffer Pooling</h2>
<p class="text-base leading-relaxed mb-4 font-normal">Every time you allocate a buffer, you're asking V8 to allocate memory. Every allocation is tracked by the garbage collector. When buffers are no longer referenced, the GC reclaims them. Frequent allocations create GC pressure  -  the garbage collector has to run more often, pausing your application to reclaim memory.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For high-throughput streams, buffer allocation overhead can become a bottleneck. If you're processing gigabytes of data in small chunks, you might allocate millions of buffers, each creating GC work.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Understanding the mechanics matters here. When you allocate a buffer with <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.alloc(size)</code>, V8 doesn't just hand you memory. It has to find a free memory region (possibly triggering GC if memory is fragmented), initialize metadata to track the allocation, potentially zero the memory for security, and link the buffer into its tracking structures.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For a small buffer (say 1KB), this allocation might take 500-2000 nanoseconds, depending on heap state. If you're allocating one buffer per chunk and processing 100,000 chunks per second, that's 50-200 milliseconds per second spent just on allocation overhead  -  5-20% of CPU time wasted on bookkeeping.</p>
<p class="text-base leading-relaxed mb-4 font-normal">When those buffers become unreachable, the garbage collector has to reclaim them. V8's GC has multiple generations (young generation, old generation), and objects move between generations based on age. Buffers that live for many GC cycles get promoted to the old generation, which is more expensive to collect. Frequent buffer allocations and deallocations thrash the GC, causing longer and more frequent pause times.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Buffer pooling is the technique of reusing buffers to avoid repeated allocations. Instead of allocating a new buffer for each chunk, you allocate a pool of buffers upfront and reuse them.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The core idea is simple: allocate N buffers at startup, keep them in a pool, and hand them out when needed. When a buffer is no longer needed, return it to the pool instead of letting it be garbage collected. This transforms repeated allocations into simple pool management  -  popping and pushing from an array, which is orders of magnitude faster than GC operations.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The difficulty is in managing buffer lifetimes. You can only reuse a buffer when you're certain it's no longer referenced anywhere else. If you return a buffer to the pool while some other code still holds a reference, that code will see its data corrupted when the buffer is reused. This is a use-after-free bug, just in JavaScript instead of C.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The safe approach is to only pool buffers with well-defined, scoped lifetimes. For example, buffers used for a single I/O operation: read into buffer, process immediately, return to pool. As long as processing doesn't retain a reference to the buffer, it's safe to reuse.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Here's the simplest form: a single reusable buffer:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> reusableBuffer</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">allocUnsafe</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">65536</span><span style="color:#E1E4E8">);</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"data"</span><span style="color:#E1E4E8">, (</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#E1E4E8">  chunk.</span><span style="color:#B392F0">copy</span><span style="color:#E1E4E8">(reusableBuffer, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, chunk.</span><span style="color:#79B8FF">length</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#B392F0">  processBuffer</span><span style="color:#E1E4E8">(reusableBuffer.</span><span style="color:#B392F0">subarray</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, chunk.</span><span style="color:#79B8FF">length</span><span style="color:#E1E4E8">));</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">You allocate one 64KB buffer. For each incoming chunk, you copy it into the reusable buffer, process it, and then reuse the buffer for the next chunk.</p>
<p class="text-base leading-relaxed mb-4 font-normal">This eliminates per-chunk allocations. You allocate once, reuse many times, reducing GC pressure.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But there's an important detail: <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.allocUnsafe()</code>. This allocates a buffer without zeroing its memory. Normal <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.alloc()</code> zeroes the buffer, which ensures no leftover data from previous uses. Zeroing costs CPU cycles. If you're about to overwrite the entire buffer anyway, zeroing is wasted work.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.allocUnsafe()</code> skips zeroing. The buffer contains whatever data was in that memory region before. This is faster but dangerous. If you don't overwrite the entire buffer, you might leak sensitive data from previous operations.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The safe pattern: use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.allocUnsafe()</code> when you're immediately overwriting the buffer, and slice the buffer to the actual data length to avoid exposing uninitialized memory:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> buf</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">allocUnsafe</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">1024</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> bytesRead</span><span style="color:#F97583"> =</span><span style="color:#B392F0"> readDataInto</span><span style="color:#E1E4E8">(buf);</span></span>
<span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> safeSlice</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> buf.</span><span style="color:#B392F0">subarray</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, bytesRead);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">The slice contains only the bytes you wrote. The rest of the buffer (which might have garbage data) isn't exposed.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For more flexible pooling, maintain a pool of buffers and check them out/in:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> BufferPool</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">  constructor</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">bufferSize</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">poolSize</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#79B8FF">    this</span><span style="color:#E1E4E8">.bufferSize </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> bufferSize;</span></span>
<span class="line"><span style="color:#79B8FF">    this</span><span style="color:#E1E4E8">.pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [];</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">let</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">; i </span><span style="color:#F97583">&lt;</span><span style="color:#E1E4E8"> poolSize; i</span><span style="color:#F97583">++</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#79B8FF">      this</span><span style="color:#E1E4E8">.pool.</span><span style="color:#B392F0">push</span><span style="color:#E1E4E8">(Buffer.</span><span style="color:#B392F0">allocUnsafe</span><span style="color:#E1E4E8">(bufferSize));</span></span>
<span class="line"><span style="color:#E1E4E8">    }</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"/>
<span class="line"><span style="color:#B392F0">  acquire</span><span style="color:#E1E4E8">() {</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> this</span><span style="color:#E1E4E8">.pool.</span><span style="color:#B392F0">pop</span><span style="color:#E1E4E8">() </span><span style="color:#F97583">||</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">allocUnsafe</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">this</span><span style="color:#E1E4E8">.bufferSize);</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"/>
<span class="line"><span style="color:#B392F0">  release</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">buffer</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">this</span><span style="color:#E1E4E8">.pool.</span><span style="color:#79B8FF">length</span><span style="color:#F97583"> &lt;</span><span style="color:#79B8FF"> 100</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#79B8FF">      this</span><span style="color:#E1E4E8">.pool.</span><span style="color:#B392F0">push</span><span style="color:#E1E4E8">(buffer);</span></span>
<span class="line"><span style="color:#E1E4E8">    }</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">You allocate a pool of buffers upfront. When you need a buffer, you pop one from the pool. When you're done, you return it to the pool. If the pool is empty, you allocate a new buffer (falling back to normal allocation). If the pool is too large (to avoid hoarding memory), you discard returned buffers instead of pooling them.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Security warning:</strong> This basic pool doesn't zero buffers on release. If your application handles sensitive data (passwords, tokens, PII), previous contents remain in memory and could leak to subsequent users of the same buffer. For security-sensitive applications, either zero the buffer before releasing (<code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">buffer.fill(0)</code>) or don't pool buffers that held sensitive data.</p>
<p class="text-base leading-relaxed mb-4 font-normal">This cuts allocations way down. Instead of allocating per chunk, you allocate a small pool upfront and reuse those buffers thousands of times.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Use it in a readable stream:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> pool</span><span style="color:#F97583"> =</span><span style="color:#F97583"> new</span><span style="color:#B392F0"> BufferPool</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">16384</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">);</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> PooledReadable</span><span style="color:#F97583"> extends</span><span style="color:#B392F0"> Readable</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#B392F0">  _read</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">size</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#F97583">    const</span><span style="color:#79B8FF"> buffer</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> pool.</span><span style="color:#B392F0">acquire</span><span style="color:#E1E4E8">();</span></span>
<span class="line"/>
<span class="line"><span style="color:#B392F0">    readDataInto</span><span style="color:#E1E4E8">(buffer, (</span><span style="color:#FFAB70">err</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">bytesRead</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">      if</span><span style="color:#E1E4E8"> (err) {</span></span>
<span class="line"><span style="color:#E1E4E8">        pool.</span><span style="color:#B392F0">release</span><span style="color:#E1E4E8">(buffer);</span></span>
<span class="line"><span style="color:#79B8FF">        this</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">destroy</span><span style="color:#E1E4E8">(err);</span></span>
<span class="line"><span style="color:#E1E4E8">      } </span><span style="color:#F97583">else</span><span style="color:#F97583"> if</span><span style="color:#E1E4E8"> (bytesRead </span><span style="color:#F97583">===</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#E1E4E8">        pool.</span><span style="color:#B392F0">release</span><span style="color:#E1E4E8">(buffer);</span></span>
<span class="line"><span style="color:#79B8FF">        this</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">push</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">null</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#E1E4E8">      } </span><span style="color:#F97583">else</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">        const</span><span style="color:#79B8FF"> data</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">from</span><span style="color:#E1E4E8">(buffer.</span><span style="color:#B392F0">subarray</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, bytesRead));</span></span>
<span class="line"><span style="color:#79B8FF">        this</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">push</span><span style="color:#E1E4E8">(data);</span></span>
<span class="line"><span style="color:#E1E4E8">        pool.</span><span style="color:#B392F0">release</span><span style="color:#E1E4E8">(buffer);</span></span>
<span class="line"><span style="color:#E1E4E8">      }</span></span>
<span class="line"><span style="color:#E1E4E8">    });</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">You acquire a buffer, use it for reading, copy the relevant portion with <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.from()</code>, push that copy, and release the original buffer back to the pool. The buffer is reused for the next read.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The explicit copy with <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.from()</code> is to be noted here. <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">subarray()</code> creates a view that shares memory with the original buffer  -  it does NOT copy data. If you pushed the subarray directly and released the buffer, downstream consumers might see corrupted data when the pool reuses that buffer for another read. Always copy before releasing a pooled buffer.</p>
<p class="text-base leading-relaxed mb-4 font-normal">You're still controlling where allocations happen. Instead of allocating a buffer in <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_read()</code> each time (which V8 has to manage), you're allocating smaller copies of just the bytes you read. The pool buffers are large and reused, reducing pressure on the allocator.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For truly zero-copy pooling, you'd need to pass pool buffers directly downstream and ensure they're released after consumption. This means coordinating with the consumer, which gets messy. In practice, pooling is most useful when you control both ends of the data flow (like a custom protocol implementation).</p>
<p class="text-base leading-relaxed mb-4 font-normal">Buffer pooling reduces allocations, which reduces GC pressure. Use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.allocUnsafe()</code> for buffers you're about to overwrite, and be careful with slicing to avoid leaking uninitialized data.</p>
<h2 id="batching-writes-for-efficiency" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Batching Writes for Efficiency</h2>
<p class="text-base leading-relaxed mb-4 font-normal">We touched on cork/uncork earlier for batching writes. Now we'll examine when and how to use it strategically.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Corking a writable stream tells it to buffer writes instead of flushing them immediately. Uncorking flushes the buffered writes, ideally in a single batched operation (via <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> if implemented).</p>
<p class="text-base leading-relaxed mb-4 font-normal">The benefit: reducing the number of write operations. The cost: increased latency (data sits in the buffer until uncorked).</p>
<p class="text-base leading-relaxed mb-4 font-normal">The key is knowing when to cork. If you're about to write a burst of small chunks, cork before the burst and uncork after:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">cork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">const</span><span style="color:#79B8FF"> item</span><span style="color:#F97583"> of</span><span style="color:#E1E4E8"> items) {</span></span>
<span class="line"><span style="color:#E1E4E8">  writable.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">processItem</span><span style="color:#E1E4E8">(item));</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span>
<span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">uncork</span><span style="color:#E1E4E8">();</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">All writes buffer, then flush together. If <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> is implemented, they're written in one syscall.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But there's a problem with this pattern: if <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">processItem()</code> throws an exception, <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">uncork()</code> never gets called and the stream stays corked. Always pair cork with uncork in a try/finally:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">cork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#F97583">try</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#6A737D">  // ... writes ...</span></span>
<span class="line"><span style="color:#E1E4E8">} </span><span style="color:#F97583">finally</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#E1E4E8">  writable.</span><span style="color:#B392F0">uncork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">The <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">finally</code> block ensures uncork even if an error occurs.</p>
<p class="text-base leading-relaxed mb-4 font-normal">One subtlety: Node allows multiple cork calls. Each cork increments an internal counter. Each uncork decrements it. The stream only flushes when the counter reaches zero:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">cork</span><span style="color:#E1E4E8">(); </span><span style="color:#6A737D">// counter = 1</span></span>
<span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">cork</span><span style="color:#E1E4E8">(); </span><span style="color:#6A737D">// counter = 2</span></span>
<span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">uncork</span><span style="color:#E1E4E8">(); </span><span style="color:#6A737D">// counter = 1, no flush</span></span>
<span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">uncork</span><span style="color:#E1E4E8">(); </span><span style="color:#6A737D">// counter = 0, flush</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This lets you nest cork regions. The innermost cork/uncork doesn't trigger a flush; only the outermost uncork does.</p>
<p class="text-base leading-relaxed mb-4 font-normal">This is useful for complex control flow where multiple functions might cork/uncork:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">function</span><span style="color:#B392F0"> writeHeader</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">writable</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#E1E4E8">  writable.</span><span style="color:#B392F0">cork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">  writable.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(header);</span></span>
<span class="line"><span style="color:#E1E4E8">  writable.</span><span style="color:#B392F0">uncork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">function</span><span style="color:#B392F0"> writeBody</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">writable</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#E1E4E8">  writable.</span><span style="color:#B392F0">cork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#F97583">  for</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">const</span><span style="color:#79B8FF"> chunk</span><span style="color:#F97583"> of</span><span style="color:#E1E4E8"> chunks) {</span></span>
<span class="line"><span style="color:#E1E4E8">    writable.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(chunk);</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"><span style="color:#E1E4E8">  writable.</span><span style="color:#B392F0">uncork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">cork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#B392F0">writeHeader</span><span style="color:#E1E4E8">(writable); </span><span style="color:#6A737D">// Nested cork/uncork</span></span>
<span class="line"><span style="color:#B392F0">writeBody</span><span style="color:#E1E4E8">(writable); </span><span style="color:#6A737D">// Nested cork/uncork</span></span>
<span class="line"><span style="color:#E1E4E8">writable.</span><span style="color:#B392F0">uncork</span><span style="color:#E1E4E8">(); </span><span style="color:#6A737D">// Final flush</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Both <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">writeHeader</code> and <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">writeBody</code> cork/uncork, but because they're nested within an outer cork, their uncorks don't flush. The final uncork flushes everything.</p>
<p class="text-base leading-relaxed mb-4 font-normal">When not to cork: if writes are already large (megabytes), corking doesn't help. The overhead of buffering might exceed any batching benefit. Cork is most useful for many small writes.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Also, don't cork indefinitely. If you're processing a long-running stream and you cork at the start, data buffers until the end. This consumes memory and increases latency. Cork only around bursts of writes, not for the entire stream lifetime.</p>
<p class="text-base leading-relaxed mb-4 font-normal">A pattern for adaptive corking: cork when writes are frequent, uncork when they slow down:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">let</span><span style="color:#E1E4E8"> lastWrite </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> Date.</span><span style="color:#B392F0">now</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#F97583">let</span><span style="color:#E1E4E8"> corked </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> false</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#F97583">let</span><span style="color:#E1E4E8"> uncorkTimer </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> null</span><span style="color:#E1E4E8">;</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">function</span><span style="color:#B392F0"> adaptiveWrite</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#F97583">  const</span><span style="color:#79B8FF"> now</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Date.</span><span style="color:#B392F0">now</span><span style="color:#E1E4E8">();</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">  if</span><span style="color:#E1E4E8"> (now </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> lastWrite </span><span style="color:#F97583">&lt;</span><span style="color:#79B8FF"> 10</span><span style="color:#F97583"> &amp;&amp;</span><span style="color:#F97583"> !</span><span style="color:#E1E4E8">corked) {</span></span>
<span class="line"><span style="color:#E1E4E8">    writable.</span><span style="color:#B392F0">cork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">    corked </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> true</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">  writable.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(chunk);</span></span>
<span class="line"><span style="color:#E1E4E8">  lastWrite </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> now;</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">  if</span><span style="color:#E1E4E8"> (uncorkTimer) </span><span style="color:#B392F0">clearTimeout</span><span style="color:#E1E4E8">(uncorkTimer);</span></span>
<span class="line"><span style="color:#E1E4E8">  uncorkTimer </span><span style="color:#F97583">=</span><span style="color:#B392F0"> setTimeout</span><span style="color:#E1E4E8">(() </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> (corked) {</span></span>
<span class="line"><span style="color:#E1E4E8">      writable.</span><span style="color:#B392F0">uncork</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">      corked </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> false</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#E1E4E8">    }</span></span>
<span class="line"><span style="color:#E1E4E8">    uncorkTimer </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> null</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#E1E4E8">  }, </span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">If writes happen within 10ms of each other, cork. If 10ms pass without writes, uncork. This batches rapid writes while flushing promptly when writes slow down. Note that we clear and reset the timer on each write  -  without this, rapid writes would accumulate many pending timers.</p>
<p class="text-base leading-relaxed mb-4 font-normal">This is a heuristic. The right threshold depends on your workload. Measure and tune.</p>
<h2 id="avoiding-string-concatenation-overhead-in-streams" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Avoiding String Concatenation Overhead in Streams</h2>
<p class="text-base leading-relaxed mb-4 font-normal">String concatenation in JavaScript can be memory-inefficient when accumulating large amounts of text. Modern engines like V8 optimize concatenation using "cons strings" (also called ropes)  -  instead of immediately copying, they create a tree structure pointing to the original strings. The actual copying happens later when the string is read or flattened.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But this optimization has limits. Cons strings add memory overhead for the tree structure, and flattening happens unpredictably  -  when you access a character, search the string, or pass it to native code. In stream scenarios where you're accumulating many chunks, these deferred copies still happen eventually, and the memory overhead of maintaining deep cons string trees can cause issues.</p>
<p class="text-base leading-relaxed mb-4 font-normal">In streams, the problematic pattern looks like this:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">let</span><span style="color:#E1E4E8"> text </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> ""</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"data"</span><span style="color:#E1E4E8">, (</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#E1E4E8">  text </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> chunk.</span><span style="color:#B392F0">toString</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Each <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">+=</code> either creates a cons string (deferred cost) or triggers flattening of previous cons strings (immediate cost). For a large file with many chunks, you end up with either a deep tree that eventually flattens expensively, or repeated flattening operations that approach O(N^2) behavior.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The fix: use an array to collect chunks, then join once:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> chunks</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> [];</span></span>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"data"</span><span style="color:#E1E4E8">, (</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#E1E4E8">  chunks.</span><span style="color:#B392F0">push</span><span style="color:#E1E4E8">(chunk.</span><span style="color:#B392F0">toString</span><span style="color:#E1E4E8">());</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"end"</span><span style="color:#E1E4E8">, () </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">  const</span><span style="color:#79B8FF"> text</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> chunks.</span><span style="color:#B392F0">join</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">""</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#B392F0">  processText</span><span style="color:#E1E4E8">(text);</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal"><code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Array.push()</code> is cheap. <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Array.join()</code> allocates once and copies all strings in one pass. Linear time instead of quadratic.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Even better, if you're working with buffers, collect buffers and concatenate at the end:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> buffers</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> [];</span></span>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"data"</span><span style="color:#E1E4E8">, (</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#E1E4E8">  buffers.</span><span style="color:#B392F0">push</span><span style="color:#E1E4E8">(chunk);</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"end"</span><span style="color:#E1E4E8">, () </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">  const</span><span style="color:#79B8FF"> combined</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">concat</span><span style="color:#E1E4E8">(buffers);</span></span>
<span class="line"><span style="color:#B392F0">  processBuffer</span><span style="color:#E1E4E8">(combined);</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal"><code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.concat()</code> allocates once and copies all buffers. Avoid <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">toString()</code> until you actually need a string.</p>
<p class="text-base leading-relaxed mb-4 font-normal">If you need to process data incrementally (not accumulate the entire stream), process chunks directly:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"data"</span><span style="color:#E1E4E8">, (</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#B392F0">  processChunk</span><span style="color:#E1E4E8">(chunk); </span><span style="color:#6A737D">// No accumulation</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">No concatenation, no accumulation, just per-chunk processing.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Another anti-pattern: converting buffers to strings for simple operations:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> str</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> buffer.</span><span style="color:#B392F0">toString</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> (str.</span><span style="color:#B392F0">includes</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"keyword"</span><span style="color:#E1E4E8">)) {</span></span>
<span class="line"><span style="color:#6A737D">  // ...</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Buffers have methods for searching:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> (buffer.</span><span style="color:#B392F0">indexOf</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"keyword"</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">!==</span><span style="color:#F97583"> -</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#6A737D">  // ...</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">No conversion, no string allocation, just a buffer scan.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Strings are immutable, and concatenation creates new strings. For streams, minimize conversions and concatenations. Work with buffers when possible, and collect chunks in arrays when accumulation is necessary.</p>
<h2 id="the-streamread0-hack" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">The stream.read(0) Hack</h2>
<p class="text-base leading-relaxed mb-4 font-normal">An obscure but occasionally useful trick: calling <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read(0)</code> on a readable stream.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Normally, <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read(size)</code> pulls <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">size</code> bytes from the stream's internal buffer. But <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read(0)</code> doesn't pull any data. Instead, it triggers the stream's internal buffer check, potentially calling <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_read()</code> if certain conditions are met.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_read()</code> will only be called when <strong class="font-bold">both</strong> conditions are true:</p>
<ol class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:decimal;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">The internal buffer is below <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">highWaterMark</code></li>
<li class="ml-2 font-normal" style="display:list-item">The stream is not currently in the middle of a <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_read()</code> call</li>
</ol>
<p class="text-base leading-relaxed mb-4 font-normal">This is useful when you're using a stream in paused mode and you want to initiate buffer filling:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">pause</span><span style="color:#E1E4E8">();</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">// Later, trigger buffer fill without consuming data:</span></span>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">read</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Calling <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read(0)</code> tells the stream "check if you need to read more data," without actually consuming anything. If the buffer is low and no read is in progress, <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_read()</code> is called to start filling the buffer.</p>
<p class="text-base leading-relaxed mb-4 font-normal">One important caveat: <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_read()</code> implementations are almost always asynchronous. They call <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">this.push()</code> later, after I/O completes. So <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read(0)</code> merely <em class="italic">initiates</em> a read request; it doesn't synchronously fill the buffer. If you need data to be available, you must wait for the async operation to complete:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">pause</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#B392F0">setupResources</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">read</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">); </span><span style="color:#6A737D">// Initiates async buffer fill</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">// Wrong: buffer likely empty here, _read() hasn't completed yet</span></span>
<span class="line"><span style="color:#6A737D">// readable.resume();</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">// Right: wait for data to be available</span></span>
<span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">once</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">'readable'</span><span style="color:#E1E4E8">, () </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#E1E4E8">  readable.</span><span style="color:#B392F0">resume</span><span style="color:#E1E4E8">(); </span><span style="color:#6A737D">// Now data is actually available</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This is a niche optimization. Most code doesn't need it. But if you're writing low-level stream plumbing and you need fine-grained control over when <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_read()</code> is called, <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read(0)</code> is the tool.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For debugging: if <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read(0)</code> doesn't trigger <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_read()</code>, it doesn't necessarily indicate a bug. It might mean the buffer is already at or above <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">highWaterMark</code>, or that a previous <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_read()</code> is still in progress. Check <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">readable.readableLength</code> against the stream's <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">highWaterMark</code> to understand the current state.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Treat this as an advanced technique. For normal stream usage, you won't touch <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">read(0)</code>. But it's part of the stream API, and knowing it exists can help when you're deep in the internals.</p>
<h2 id="avoiding-intermediate-transforms" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Avoiding Intermediate Transforms</h2>
<p class="text-base leading-relaxed mb-4 font-normal">Each transform in a pipeline adds overhead. The data passes through the transform's buffering, the <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_transform()</code> method is called, and the output is buffered again. For complex pipelines with many stages, this overhead compounds.</p>
<p class="text-base leading-relaxed mb-4 font-normal">If you can combine transformations, you reduce stages and improve performance:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D">// Slower: three transforms</span></span>
<span class="line"><span style="color:#B392F0">pipeline</span><span style="color:#E1E4E8">(</span></span>
<span class="line"><span style="color:#E1E4E8">  source,</span></span>
<span class="line"><span style="color:#E1E4E8">  toUpperCase,</span></span>
<span class="line"><span style="color:#E1E4E8">  removeWhitespace,</span></span>
<span class="line"><span style="color:#E1E4E8">  trimLines,</span></span>
<span class="line"><span style="color:#E1E4E8">  dest</span></span>
<span class="line"><span style="color:#E1E4E8">);</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">// Faster: one combined transform</span></span>
<span class="line"><span style="color:#B392F0">pipeline</span><span style="color:#E1E4E8">(</span></span>
<span class="line"><span style="color:#E1E4E8">  source,</span></span>
<span class="line"><span style="color:#E1E4E8">  allInOne, </span><span style="color:#6A737D">// Does all three transformations</span></span>
<span class="line"><span style="color:#E1E4E8">  dest</span></span>
<span class="line"><span style="color:#E1E4E8">);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">The combined transform does the same work in one pass. Less buffering, fewer method calls, less overhead.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The trade-off is modularity. Separate transforms are easier to test and reuse. A combined transform is a monolith. Choose based on your priorities: performance vs. maintainability.</p>
<p class="text-base leading-relaxed mb-4 font-normal">If performance matters and you have a pipeline with many simple transforms, consider combining them. If maintainability matters and transforms are reused across pipelines, keep them separate.</p>
<p class="text-base leading-relaxed mb-4 font-normal">One pattern: lazy combination. Start with separate transforms for clarity. If profiling shows pipeline overhead is high, combine the hot path:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> combined</span><span style="color:#F97583"> =</span><span style="color:#F97583"> new</span><span style="color:#B392F0"> Transform</span><span style="color:#E1E4E8">({</span></span>
<span class="line"><span style="color:#B392F0">  transform</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">encoding</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">callback</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#F97583">    let</span><span style="color:#E1E4E8"> result </span><span style="color:#F97583">=</span><span style="color:#B392F0"> toUpperCase</span><span style="color:#E1E4E8">(chunk);</span></span>
<span class="line"><span style="color:#E1E4E8">    result </span><span style="color:#F97583">=</span><span style="color:#B392F0"> removeWhitespace</span><span style="color:#E1E4E8">(result);</span></span>
<span class="line"><span style="color:#E1E4E8">    result </span><span style="color:#F97583">=</span><span style="color:#B392F0"> trimLines</span><span style="color:#E1E4E8">(result);</span></span>
<span class="line"><span style="color:#79B8FF">    this</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">push</span><span style="color:#E1E4E8">(result);</span></span>
<span class="line"><span style="color:#B392F0">    callback</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">  },</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">You've encapsulated the combined logic, and the pipeline has one fewer stage.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Another optimization: eliminate no-op transforms. Sometimes transforms pass data unchanged under certain conditions:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> (shouldTransform) {</span></span>
<span class="line"><span style="color:#B392F0">  pipeline</span><span style="color:#E1E4E8">(source, transform, dest);</span></span>
<span class="line"><span style="color:#E1E4E8">} </span><span style="color:#F97583">else</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#B392F0">  pipeline</span><span style="color:#E1E4E8">(source, dest);</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">If the transform isn't needed, skip it entirely. Don't include a passthrough transform "just in case."</p>
<p class="text-base leading-relaxed mb-4 font-normal">Every pipeline stage has a cost. Minimize stages when performance matters. Combine transforms judiciously to balance speed and code clarity.</p>
<h2 id="optimizing-readablereadableflowing-for-manual-control" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Optimizing readable.readableFlowing for Manual Control</h2>
<p class="text-base leading-relaxed mb-4 font-normal">The <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">readable.readableFlowing</code> property tells you whether the stream is in flowing mode (true), paused mode (false), or hasn't been set yet (null).</p>
<p class="text-base leading-relaxed mb-4 font-normal">You can use this for manual flow control. Suppose you're consuming a stream and you want to pause based on external conditions:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"data"</span><span style="color:#E1E4E8">, (</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#B392F0">  processChunk</span><span style="color:#E1E4E8">(chunk);</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">  if</span><span style="color:#E1E4E8"> (</span><span style="color:#B392F0">shouldPause</span><span style="color:#E1E4E8">()) {</span></span>
<span class="line"><span style="color:#E1E4E8">    readable.</span><span style="color:#B392F0">pause</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">// Later, resume based on readableFlowing:</span></span>
<span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> (readable.readableFlowing </span><span style="color:#F97583">===</span><span style="color:#79B8FF"> false</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#E1E4E8">  readable.</span><span style="color:#B392F0">resume</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Checking <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">readableFlowing</code> before calling <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">resume()</code> avoids redundant resume calls. If the stream is already flowing, <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">resume()</code> does nothing, but checking first saves a method call.</p>
<p class="text-base leading-relaxed mb-4 font-normal">This is micro-optimization territory, but in tight loops processing millions of chunks, small savings add up.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Another pattern: conditionally switching modes based on flow state:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> (readable.readableFlowing </span><span style="color:#F97583">===</span><span style="color:#79B8FF"> null</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#6A737D">  // Stream hasn't been started, use flowing mode</span></span>
<span class="line"><span style="color:#E1E4E8">  readable.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"data"</span><span style="color:#E1E4E8">, handler);</span></span>
<span class="line"><span style="color:#E1E4E8">} </span><span style="color:#F97583">else</span><span style="color:#F97583"> if</span><span style="color:#E1E4E8"> (readable.readableFlowing </span><span style="color:#F97583">===</span><span style="color:#79B8FF"> false</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#6A737D">  // Stream is paused, resume it</span></span>
<span class="line"><span style="color:#E1E4E8">  readable.</span><span style="color:#B392F0">resume</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This lets you adapt to the stream's current state without assumptions.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">readableFlowing</code> property is also useful for debugging. If a stream isn't emitting data, check <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">readableFlowing</code>. If it's <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">false</code>, the stream is paused. If it's <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">null</code>, no listener has triggered flowing mode yet. If it's <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">true</code>, the stream is flowing, and the issue is elsewhere.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For most code, you won't manipulate <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">readableFlowing</code> directly. But it's a handy introspection tool when building or debugging streaming systems.</p>
<h2 id="performance-profiling" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Performance Profiling</h2>
<p class="text-base leading-relaxed mb-4 font-normal">All these optimizations  -  zero-copy, buffer pooling, cork/uncork  -  only matter if they improve performance for your workload. The only way to know is to measure.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Start with a baseline. Run your stream pipeline without optimizations and measure throughput:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> start</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Date.</span><span style="color:#B392F0">now</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#F97583">let</span><span style="color:#E1E4E8"> bytes </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">;</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">source.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"data"</span><span style="color:#E1E4E8">, (</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#E1E4E8">  bytes </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> chunk.</span><span style="color:#79B8FF">length</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span>
<span class="line"/>
<span class="line"><span style="color:#E1E4E8">source.</span><span style="color:#B392F0">on</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"end"</span><span style="color:#E1E4E8">, () </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">  const</span><span style="color:#79B8FF"> duration</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> (Date.</span><span style="color:#B392F0">now</span><span style="color:#E1E4E8">() </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> start) </span><span style="color:#F97583">/</span><span style="color:#79B8FF"> 1000</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#F97583">  const</span><span style="color:#79B8FF"> throughput</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> bytes </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> duration </span><span style="color:#F97583">/</span><span style="color:#79B8FF"> 1024</span><span style="color:#F97583"> /</span><span style="color:#79B8FF"> 1024</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#E1E4E8">  console.</span><span style="color:#B392F0">log</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">`Baseline: ${</span><span style="color:#E1E4E8">throughput</span><span style="color:#9ECBFF">.</span><span style="color:#B392F0">toFixed</span><span style="color:#9ECBFF">(</span><span style="color:#79B8FF">2</span><span style="color:#9ECBFF">)</span><span style="color:#9ECBFF">} MB/s`</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#E1E4E8">});</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Record the baseline throughput. Then apply one optimization at a time and remeasure.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For example, implement <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code>:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> OptimizedWriter</span><span style="color:#F97583"> extends</span><span style="color:#B392F0"> Writable</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#B392F0">  _writev</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">chunks</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">callback</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#6A737D">    // ... batching logic ...</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Run the pipeline with the optimized writer and measure throughput. If it's higher, the optimization helps. If it's the same or lower, it doesn't (or it's not the bottleneck).</p>
<p class="text-base leading-relaxed mb-4 font-normal">Measure memory usage too:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#B392F0">setInterval</span><span style="color:#E1E4E8">(() </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">  const</span><span style="color:#79B8FF"> mem</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> process.</span><span style="color:#B392F0">memoryUsage</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">  console.</span><span style="color:#B392F0">log</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">`Heap: ${</span><span style="color:#9ECBFF">(</span><span style="color:#E1E4E8">mem</span><span style="color:#9ECBFF">.</span><span style="color:#E1E4E8">heapUsed</span><span style="color:#F97583"> /</span><span style="color:#79B8FF"> 1024</span><span style="color:#F97583"> /</span><span style="color:#79B8FF"> 1024</span><span style="color:#9ECBFF">).</span><span style="color:#B392F0">toFixed</span><span style="color:#9ECBFF">(</span><span style="color:#79B8FF">2</span><span style="color:#9ECBFF">)</span><span style="color:#9ECBFF">} MB`</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#E1E4E8">}, </span><span style="color:#79B8FF">1000</span><span style="color:#E1E4E8">);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">If buffer pooling reduces heap usage without hurting throughput, it's a win. If it reduces throughput more than it saves memory, it's not worth it.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Use Node's built-in profiler to find hotspots:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#B392F0">node</span><span style="color:#79B8FF"> --prof</span><span style="color:#9ECBFF"> script.js</span></span>
<span class="line"><span style="color:#B392F0">node</span><span style="color:#79B8FF"> --prof-process</span><span style="color:#9ECBFF"> isolate-</span><span style="color:#79B8FF">*</span><span style="color:#9ECBFF">.log</span><span style="color:#F97583"> &gt;</span><span style="color:#9ECBFF"> profile.txt</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Look for time spent in buffer operations, syscalls, or transform methods. If 50% of time is in <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.concat()</code>, that's your bottleneck. Optimize that.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The key is to profile your actual workload, not synthetic benchmarks. If you're processing JSON, test with JSON. If you're serving files, test with real file sizes. Optimizations that help with small chunks might hurt with large chunks, or vice versa.</p>
<p class="text-base leading-relaxed mb-4 font-normal">And remember: premature optimization is the root of all evil. Optimize after measuring, not before. Many "obvious" optimizations don't help in practice.</p>
<h2 id="real-world-performance-patterns" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Real-World Performance Patterns</h2>
<p class="text-base leading-relaxed mb-4 font-normal">Before complete examples, we need to establish when each optimization technique provides real value in production systems.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Zero-copy concepts matter in high-bandwidth, low-transform scenarios.</strong> If you're building a static file server, video streaming service, or HTTP proxy, minimizing copies is crucial. However, remember that Node.js streams don't automatically use kernel-level zero-copy like <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code>. The optimizations we've discussed  -  avoiding <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.concat()</code>, using <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">subarray()</code> for views, implementing <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code>  -  help reduce copies in user space. For true kernel-level zero-copy at CDN scale (10,000+ requests/second for large files), specialized servers like nginx are typically more appropriate than Node.js.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But if you're building an API that returns JSON responses (typically under 100KB), zero-copy won't help. The responses are small, they're generated dynamically (requiring transform), and they don't flow through file-to-socket pipes. Traditional I/O is fine here, and optimization effort should focus elsewhere  -  like JSON serialization or database queries.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Scatter/gather I/O (writev) matters when you have many small writes.</strong> HTTP response headers are a classic example. You write status line, multiple header lines, and then body  -  potentially dozens of small chunks. Without writev(), that's dozens of syscalls. With writev(), it's one or a few. For a high-volume HTTP server, this can improve response latency by 10-30% just by reducing syscall overhead.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But if your writes are already large (like streaming 64KB chunks from a file), writev() won't help. Each chunk is written in one syscall anyway. The overhead you're saving is negligible compared to the actual I/O cost.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Buffer pooling pays off when allocation rate is extreme.</strong> If you're processing binary protocols with many small messages (think network packet parsing, IoT sensor data, financial tick data), you might allocate millions of small buffers per second. Pooling can reduce GC pause time by 50-80%, transforming a system that struggles to maintain 10,000 msg/sec into one that easily handles 100,000 msg/sec.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But if your stream processing allocates buffers at a modest rate (say, 1,000 per second for a typical web application), pooling won't provide measurable benefit. The GC can easily handle that rate, and the complexity of pool management isn't justified.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Cork/uncork optimization helps with bursty write patterns.</strong> If your application processes batch jobs  -  reading 1000 records from a database, transforming them, and writing results  -  corking around the batch gives a clear win. You might reduce write operations from 1000 to 10-50, boosting throughput a lot.</p>
<p class="text-base leading-relaxed mb-4 font-normal">But for steady-state streaming (like tailing a log file and forwarding lines), cork/uncork doesn't help. Data flows continuously, and batching doesn't reduce total I/O operations, it just delays them. In some cases, it might even increase latency for no throughput benefit.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The pattern here is clear: measure your workload characteristics first, then apply optimizations that match those characteristics. Don't optimize for scenarios you don't have.</p>
<p class="text-base leading-relaxed mb-4 font-normal">A decision framework:</p>
<ol class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:decimal;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Profile your application under realistic load.</strong> Use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">perf</code> on Linux, Instruments on macOS, or Node's built-in profiler. Identify where CPU time is actually going. If you see high CPU usage but low disk/network utilization, CPU overhead (copying, syscalls) is your bottleneck. If disk/network is saturated and CPU is low, I/O throughput is your limit, not CPU overhead.</p>
</li>
<li class="ml-2 font-normal" style="display:list-item">
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Measure allocation rates and GC impact.</strong> Use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">process.memoryUsage()</code> and track heap growth. Use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">--trace-gc</code> to see GC pause times. If allocations are extreme (multi-MB/sec) and GC pauses are long (<code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">&gt;10ms</code>), buffer pooling might help. If allocations are modest and GC pauses are tiny (<code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">&lt;1ms</code>), pooling won't provide benefit.</p>
</li>
<li class="ml-2 font-normal" style="display:list-item">
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Count your syscalls.</strong> Use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">strace</code> on Linux or <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">dtruss</code> on macOS to see how many syscalls your application makes. If you see thousands of tiny write() calls when you expected a few large ones, writev() or cork/uncork can help. If you see mostly large I/O operations, batching won't matter.</p>
</li>
<li class="ml-2 font-normal" style="display:list-item">
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Benchmark with and without optimizations.</strong> The only way to know if an optimization helps is to measure. Implement the optimization, benchmark throughput and latency, and compare to baseline. If throughput improves by 20% and latency doesn't increase, it's a win. If there's no change or performance degrades, remove the optimization.</p>
</li>
</ol>
<p class="text-base leading-relaxed mb-4 font-normal">This methodical approach prevents premature optimization. You might think buffer pooling will help, but profiling shows allocation isn't a bottleneck. You might assume zero-copy is necessary, but you're transforming every byte and can't use it anyway. Measure, then optimize.</p>
<h2 id="practical-example-optimized-file-copy-pipeline" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Practical Example: Optimized File Copy Pipeline</h2>
<p class="text-base leading-relaxed mb-4 font-normal">Here's an optimized file copy pipeline that combines these techniques.</p>
<p class="text-base leading-relaxed mb-4 font-normal">We'll copy a large file using larger buffers for throughput, <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> for batching, and efficient buffer handling:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> { createReadStream, createWriteStream } </span><span style="color:#F97583">from</span><span style="color:#9ECBFF"> "fs"</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> { pipeline } </span><span style="color:#F97583">from</span><span style="color:#9ECBFF"> "stream/promises"</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> { Writable } </span><span style="color:#F97583">from</span><span style="color:#9ECBFF"> "stream"</span><span style="color:#E1E4E8">;</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> pool</span><span style="color:#F97583"> =</span><span style="color:#F97583"> new</span><span style="color:#B392F0"> BufferPool</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">65536</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">);</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> OptimizedWriter</span><span style="color:#F97583"> extends</span><span style="color:#B392F0"> Writable</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#F97583">  constructor</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">dest</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">options</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#79B8FF">    super</span><span style="color:#E1E4E8">(options);</span></span>
<span class="line"><span style="color:#79B8FF">    this</span><span style="color:#E1E4E8">.dest </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> dest;</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"/>
<span class="line"><span style="color:#B392F0">  _write</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">chunk</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">encoding</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">callback</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#79B8FF">    this</span><span style="color:#E1E4E8">.dest.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(chunk, callback);</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"/>
<span class="line"><span style="color:#B392F0">  _writev</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">chunks</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">callback</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#F97583">    const</span><span style="color:#79B8FF"> buffers</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> chunks.</span><span style="color:#B392F0">map</span><span style="color:#E1E4E8">((</span><span style="color:#FFAB70">c</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> c.chunk);</span></span>
<span class="line"><span style="color:#F97583">    const</span><span style="color:#79B8FF"> combined</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Buffer.</span><span style="color:#B392F0">concat</span><span style="color:#E1E4E8">(buffers);</span></span>
<span class="line"><span style="color:#79B8FF">    this</span><span style="color:#E1E4E8">.dest.</span><span style="color:#B392F0">write</span><span style="color:#E1E4E8">(combined, callback);</span></span>
<span class="line"><span style="color:#E1E4E8">  }</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">async</span><span style="color:#F97583"> function</span><span style="color:#B392F0"> optimizedCopy</span><span style="color:#E1E4E8">(</span><span style="color:#FFAB70">src</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">dest</span><span style="color:#E1E4E8">) {</span></span>
<span class="line"><span style="color:#F97583">  const</span><span style="color:#79B8FF"> reader</span><span style="color:#F97583"> =</span><span style="color:#B392F0"> createReadStream</span><span style="color:#E1E4E8">(src, {</span></span>
<span class="line"><span style="color:#E1E4E8">    highWaterMark: </span><span style="color:#79B8FF">65536</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E1E4E8">  });</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">  const</span><span style="color:#79B8FF"> writer</span><span style="color:#F97583"> =</span><span style="color:#F97583"> new</span><span style="color:#B392F0"> OptimizedWriter</span><span style="color:#E1E4E8">(</span></span>
<span class="line"><span style="color:#B392F0">    createWriteStream</span><span style="color:#E1E4E8">(dest, { highWaterMark: </span><span style="color:#79B8FF">65536</span><span style="color:#E1E4E8"> })</span></span>
<span class="line"><span style="color:#E1E4E8">  );</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">  await</span><span style="color:#B392F0"> pipeline</span><span style="color:#E1E4E8">(reader, writer);</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">await</span><span style="color:#B392F0"> optimizedCopy</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"input.dat"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"output.dat"</span><span style="color:#E1E4E8">);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This pipeline uses 64KB buffers and implements <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> for batching multiple chunks into single write operations. Note that 64KB is actually the default for <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.createReadStream()</code> and <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.createWriteStream()</code> as of Node.js 22  -  we're being explicit here for clarity, but you could omit <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">highWaterMark</code> entirely for file streams. The base <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">stream.Readable</code> class still defaults to 16KB.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For file-to-file copies where you don't need to process the data, consider using <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.copyFile()</code> directly, which can use OS-level optimizations:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> { copyFile, constants } </span><span style="color:#F97583">from</span><span style="color:#9ECBFF"> "fs/promises"</span><span style="color:#E1E4E8">;</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">// Use copy-on-write if filesystem supports it</span></span>
<span class="line"><span style="color:#F97583">await</span><span style="color:#B392F0"> copyFile</span><span style="color:#E1E4E8">(src, dest, constants.</span><span style="color:#79B8FF">COPYFILE_FICLONE</span><span style="color:#E1E4E8">);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">The key takeaway: combine techniques. Use larger buffers for high throughput, implement <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> for batching, and use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.copyFile()</code> when you don't need stream processing.</p>
<h2 id="measurement-and-debugging-techniques" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">Measurement and Debugging Techniques</h2>
<p class="text-base leading-relaxed mb-4 font-normal">Figuring out whether your optimizations are working takes more than eyeballing throughput numbers. Here are some ways to dig deeper.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Syscall tracing</strong> shows exactly how your optimizations affect system calls. On Linux, use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">strace</code> with timestamping:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#B392F0">strace</span><span style="color:#79B8FF"> -c</span><span style="color:#79B8FF"> -f</span><span style="color:#9ECBFF"> node</span><span style="color:#9ECBFF"> your-script.js</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">The <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">-c</code> flag shows a summary of syscall counts. If you implemented <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code> but still see hundreds of individual <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">write()</code> calls, something's wrong  -  maybe the stream isn't corking, or chunks aren't being buffered. If you see dozens of <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">writev()</code> calls with the same number of operations as your data chunks, the optimization is working.</p>
<p class="text-base leading-relaxed mb-4 font-normal">For more detail, trace specific syscalls:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#B392F0">strace</span><span style="color:#79B8FF"> -e</span><span style="color:#9ECBFF"> write,writev</span><span style="color:#79B8FF"> -f</span><span style="color:#9ECBFF"> node</span><span style="color:#9ECBFF"> your-script.js</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This shows every write and writev call with parameters and return values. You can verify that writev() is receiving multiple buffers, not one.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">CPU profiling with perf</strong> shows where CPU time is spent. On Linux:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#B392F0">perf</span><span style="color:#9ECBFF"> record</span><span style="color:#79B8FF"> -F</span><span style="color:#79B8FF"> 99</span><span style="color:#79B8FF"> -g</span><span style="color:#9ECBFF"> node</span><span style="color:#9ECBFF"> your-script.js</span></span>
<span class="line"><span style="color:#B392F0">perf</span><span style="color:#9ECBFF"> report</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This samples the call stack 99 times per second and shows which functions consume the most CPU. If you see high CPU in <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">memcpy</code> or <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">Buffer.concat</code>, you have unnecessary copying. If you see high CPU in syscall entry/exit (<code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">__kernel_vsyscall</code> on x86), you're syscall-bound  -  batching might help.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Look for the <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">sendfile()</code> function in the profile. If it's there and consuming noticeable CPU (which is actually kernel time, but attributed to sendfile), zero-copy is active. If it's absent and you expected it, zero-copy isn't being used.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Memory profiling with Chrome DevTools or V8's heap profiler</strong> shows allocation patterns. Connect to Node with <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">--inspect</code> and open Chrome DevTools. Take heap snapshots before and after processing a stream. If you see millions of small Buffer objects in the heap, allocations are excessive  -  pooling might help. You can also use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">node --heap-prof your-script.js</code> to generate a heap profile file, then load it in DevTools for analysis.</p>
<p class="text-base leading-relaxed mb-4 font-normal">The "Allocation Timeline" view shows allocation rate over time. Spiky allocation patterns indicate burst allocations  -  maybe during certain processing phases. This helps identify where to apply pooling.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">GC tracing with --trace-gc</strong> shows garbage collection activity:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#B392F0">node</span><span style="color:#79B8FF"> --trace-gc</span><span style="color:#9ECBFF"> your-script.js</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">Each GC event prints timing and heap sizes. Look for frequent minor GCs (young generation)  -  a sign of high allocation rate. If buffer pooling reduces minor GC frequency, it's working. Look for major GCs (old generation)  -  these are more expensive. If pooling reduces major GC pause times, that's a big win.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Detailed GC stats with --trace-gc-verbose</strong> provides even more info:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#B392F0">node</span><span style="color:#79B8FF"> --trace-gc</span><span style="color:#79B8FF"> --trace-gc-verbose</span><span style="color:#9ECBFF"> your-script.js</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This shows GC timing, how much memory was reclaimed, how much survived, and promotion rates. High promotion rates (objects moving from young to old generation) indicate long-lived objects  -  not ideal for buffers that should be short-lived or pooled.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Event loop delay measurement</strong> tells you if I/O operations block the event loop. Use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">perf_hooks</code>:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> { monitorEventLoopDelay } </span><span style="color:#F97583">from</span><span style="color:#9ECBFF"> 'perf_hooks'</span><span style="color:#E1E4E8">;</span></span>
<span class="line"/>
<span class="line"><span style="color:#F97583">const</span><span style="color:#79B8FF"> h</span><span style="color:#F97583"> =</span><span style="color:#B392F0"> monitorEventLoopDelay</span><span style="color:#E1E4E8">({ resolution: </span><span style="color:#79B8FF">20</span><span style="color:#E1E4E8"> });</span></span>
<span class="line"><span style="color:#E1E4E8">h.</span><span style="color:#B392F0">enable</span><span style="color:#E1E4E8">();</span></span>
<span class="line"/>
<span class="line"><span style="color:#6A737D">// Run your stream pipeline</span></span>
<span class="line"/>
<span class="line"><span style="color:#B392F0">setInterval</span><span style="color:#E1E4E8">(() </span><span style="color:#F97583">=&gt;</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#6A737D">  // Note: all values are in nanoseconds, divide by 1e6 for milliseconds</span></span>
<span class="line"><span style="color:#E1E4E8">  console.</span><span style="color:#B392F0">log</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">`p50: ${</span><span style="color:#9ECBFF">(</span><span style="color:#E1E4E8">h</span><span style="color:#9ECBFF">.</span><span style="color:#B392F0">percentile</span><span style="color:#9ECBFF">(</span><span style="color:#79B8FF">50</span><span style="color:#9ECBFF">) </span><span style="color:#F97583">/</span><span style="color:#79B8FF"> 1e6</span><span style="color:#9ECBFF">).</span><span style="color:#B392F0">toFixed</span><span style="color:#9ECBFF">(</span><span style="color:#79B8FF">2</span><span style="color:#9ECBFF">)</span><span style="color:#9ECBFF">}ms`</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#E1E4E8">  console.</span><span style="color:#B392F0">log</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">`p99: ${</span><span style="color:#9ECBFF">(</span><span style="color:#E1E4E8">h</span><span style="color:#9ECBFF">.</span><span style="color:#B392F0">percentile</span><span style="color:#9ECBFF">(</span><span style="color:#79B8FF">99</span><span style="color:#9ECBFF">) </span><span style="color:#F97583">/</span><span style="color:#79B8FF"> 1e6</span><span style="color:#9ECBFF">).</span><span style="color:#B392F0">toFixed</span><span style="color:#9ECBFF">(</span><span style="color:#79B8FF">2</span><span style="color:#9ECBFF">)</span><span style="color:#9ECBFF">}ms`</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#E1E4E8">  console.</span><span style="color:#B392F0">log</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">`max: ${</span><span style="color:#9ECBFF">(</span><span style="color:#E1E4E8">h</span><span style="color:#9ECBFF">.</span><span style="color:#E1E4E8">max</span><span style="color:#F97583"> /</span><span style="color:#79B8FF"> 1e6</span><span style="color:#9ECBFF">).</span><span style="color:#B392F0">toFixed</span><span style="color:#9ECBFF">(</span><span style="color:#79B8FF">2</span><span style="color:#9ECBFF">)</span><span style="color:#9ECBFF">}ms`</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#E1E4E8">}, </span><span style="color:#79B8FF">1000</span><span style="color:#E1E4E8">);</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">If p99 delay spikes when processing streams, your code is blocking the event loop. This could be synchronous buffer operations (Buffer.concat() on huge buffers) or large chunks overwhelming the processing loop. Break up the work or use smaller chunks.</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Node.js internal module debugging</strong> can be enabled with the <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">NODE_DEBUG</code> environment variable:</p>
<div class="my-6 overflow-x-auto rounded-lg border border-gray-200 dark:border-gray-700"><pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">NODE_DEBUG</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">fs,net,stream</span><span style="color:#B392F0"> node</span><span style="color:#9ECBFF"> your-script.js</span></span></code></pre></div>
<p class="text-base leading-relaxed mb-4 font-normal">This enables debug output for the specified built-in modules (fs, net, stream in this example). It shows internal operations like file opens, socket connections, and stream state changes.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Note: The <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">DEBUG=*</code> environment variable is for the popular npm <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">debug</code> package, not Node.js internals. For actual syscall-level visibility, use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">strace</code> on Linux or <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">dtruss</code> on macOS as shown earlier.</p>
<p class="text-base leading-relaxed mb-4 font-normal">Combining these techniques gives you a complete picture. Syscall tracing shows I/O behavior, CPU profiling shows computational cost, memory profiling shows allocation impact, GC tracing shows memory management overhead, and event loop monitoring shows responsiveness. Together, they tell you whether your optimizations are actually working and where to focus next.</p>
<h2 id="when-to-use-these-techniques" class="text-2xl md:text-3xl font-bold mb-4 mt-8 scroll-mt-20">When to Use These Techniques</h2>
<p class="text-base leading-relaxed mb-4 font-normal">Not every stream needs these optimizations. Here's when they're worth it:</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Minimize buffer copies when:</strong></p>
<ul class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:disc;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">Processing large files through stream pipelines</li>
<li class="ml-2 font-normal" style="display:list-item">Building high-throughput data processing systems</li>
<li class="ml-2 font-normal" style="display:list-item">Serving static content (though kernel-level zero-copy isn't automatic in Node.js)</li>
</ul>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Use <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">fs.copyFile()</code> with <code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">COPYFILE_FICLONE</code> for true zero-copy file duplication</strong> on filesystems that support reflinks (Btrfs, XFS, APFS).</p>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Don't over-optimize copying when:</strong></p>
<ul class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:disc;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">You need to transform data (copies are unavoidable)</li>
<li class="ml-2 font-normal" style="display:list-item">Data is small (optimization overhead exceeds benefit)</li>
<li class="ml-2 font-normal" style="display:list-item">You're not CPU-bound on buffer operations (profile first!)</li>
</ul>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Use scatter/gather I/O (<code class="px-1.5 py-0.5 bg-gray-100 dark:bg-gray-800 text-sm font-mono rounded border border-gray-200 dark:border-gray-700">_writev()</code>) when:</strong></p>
<ul class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:disc;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">Writing many small chunks</li>
<li class="ml-2 font-normal" style="display:list-item">Syscall overhead is high (profiling confirms)</li>
<li class="ml-2 font-normal" style="display:list-item">Your destination supports vectored writes</li>
</ul>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Skip it when:</strong></p>
<ul class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:disc;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">Chunks are already large (batching doesn't help)</li>
<li class="ml-2 font-normal" style="display:list-item">You're writing to a stream that doesn't benefit (in-memory buffers)</li>
</ul>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Use buffer pooling when:</strong></p>
<ul class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:disc;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">Allocating millions of buffers (GC pressure is high)</li>
<li class="ml-2 font-normal" style="display:list-item">Buffers are uniform size (easy to pool)</li>
<li class="ml-2 font-normal" style="display:list-item">You control buffer lifecycle (can safely reuse)</li>
</ul>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Skip it when:</strong></p>
<ul class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:disc;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">Buffers are variable size (hard to pool efficiently)</li>
<li class="ml-2 font-normal" style="display:list-item">Allocations aren't a bottleneck (profiling shows low GC impact)</li>
</ul>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Use cork/uncork when:</strong></p>
<ul class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:disc;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">Writing bursts of small chunks</li>
<li class="ml-2 font-normal" style="display:list-item">You control the burst boundaries (start and end)</li>
<li class="ml-2 font-normal" style="display:list-item">Latency during the burst is acceptable</li>
</ul>
<p class="text-base leading-relaxed mb-4 font-normal"><strong class="font-bold">Skip it when:</strong></p>
<ul class="mb-4 space-y-2 ml-6 font-normal" style="list-style-type:disc;list-style-position:outside">
<li class="ml-2 font-normal" style="display:list-item">Writes are already batched naturally</li>
<li class="ml-2 font-normal" style="display:list-item">Latency matters (corking delays flushing)</li>
</ul>
<p class="text-base leading-relaxed mb-4 font-normal">Measure first, optimize second. These techniques help, but they add complexity. Apply them where they demonstrably improve performance, not everywhere by default.</p>    
</body>
</html>