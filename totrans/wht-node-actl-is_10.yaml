- en: Foundation of Streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.thenodebook.com/streams/foundation-of-streams](https://www.thenodebook.com/streams/foundation-of-streams)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Before we write a single line of code that uses Node.js streams, we need to
    understand the fundamental problem they solve. This is not a problem unique to
    Node.js, or even to JavaScript. It is a problem as old as computing itself: **how
    do we process data that is larger than the memory available to hold it?**'
  prefs: []
  type: TYPE_NORMAL
- en: This question may seem simple, but its answer has shaped the architecture of
    operating systems, databases, network protocols, and nearly every system that
    handles real-world data at scale. Node.js streams are not an arbitrary API design
    choice. They are a direct, inevitable response to the constraints of physical
    memory and the realities of I/O operations.
  prefs: []
  type: TYPE_NORMAL
- en: The Problem with Large Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us start with a realistic scenario. You are building a web service that
    needs to process uploaded files. Users can upload images, videos, documents -
    any file type. The service must read these files, perhaps transform them in some
    way (compress an image, extract metadata from a video, scan for viruses), and
    then store them or send them elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most straightforward approach - the one that immediately comes to mind
    - is this: read the entire file into memory as a single Buffer, perform your operations
    on that Buffer, and then write the result. In code, this looks simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Three lines. Clean. Easy to reason about. And for small files, this works perfectly.
    But what happens when a user uploads a 2GB video file? Or a 10GB database dump?
    Suddenly, your simple program must allocate 2GB of memory just to hold that one
    file. If ten users upload files simultaneously, you need 20GB of memory. This
    approach does not scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the problem runs deeper than just memory capacity. Even if your server
    has 128GB of RAM, loading an entire 2GB file into memory means you must wait for
    the entire file to be read from disk (or received over the network) before you
    can begin processing it. If reading that file takes 5 seconds, your program sits
    idle for 5 seconds before the first byte is processed. Then, after processing
    is complete, you must write the entire 2GB back to disk or over the network, waiting
    again for the entire write operation to complete. The program is fundamentally
    synchronous in its data flow: read everything, then process everything, then write
    everything.'
  prefs: []
  type: TYPE_NORMAL
- en: This is inefficient. While you are waiting for the disk to deliver the last
    megabyte of the file, you could already be processing the first megabyte. While
    you are processing the middle of the file, you could already be writing the processed
    beginning to the output. The operations - reading, processing, writing - could
    happen concurrently, overlapping in time. But the "read everything into memory"
    approach makes that concurrency impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Operations happen one after another - each must complete before the next begins
  prefs: []
  type: TYPE_NORMAL
- en: READ5 secondsPROCESS3 secondsWRITE4 secondsTOTAL TIME12 seconds
  prefs: []
  type: TYPE_NORMAL
- en: Why chunking?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The fundamental insight is this: **we do not need to hold the entire dataset
    in memory at once to process it**. We only need to hold the portion we are currently
    working on.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a different approach. Instead of reading the entire file, what if we
    read just a small portion of it - say, 64 kilobytes - into memory? We process
    those 64 kilobytes. We write the result. Then we read the next 64 kilobytes, process
    them, write the result, and so on, until the entire file has been processed.
  prefs: []
  type: TYPE_NORMAL
- en: This chunked processing solves both problems. First, our memory usage is now
    bounded by the chunk size, not the file size. Processing a 2GB file requires only
    64KB of memory at any given moment. Second, the operations can now overlap. While
    we are processing chunk N, the operating system can be reading chunk N+1 from
    disk in the background. While we are writing the processed chunk N to the output,
    we can simultaneously be processing chunk N+1.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Usage Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Processing a 2GB file: entire vs. chunked approach'
  prefs: []
  type: TYPE_NORMAL
- en: Entire File in Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Load everything at once
  prefs: []
  type: TYPE_NORMAL
- en: MEMORY USAGE1200MBPeak Memory:2000 MBStatus:DANGEROUS
  prefs: []
  type: TYPE_NORMAL
- en: Chunked Processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Process in small pieces
  prefs: []
  type: TYPE_NORMAL
- en: MEMORY USAGE64MBPeak Memory:64 MBStatus:SAFEEFFICIENCY GAIN31xLess memory required
    with chunked processing
  prefs: []
  type: TYPE_NORMAL
- en: But this chunked approach introduces new complexity. We must manage the flow
    of chunks. We must decide when to read the next chunk, when to process it, and
    when to write it. We must handle the case where the producer of chunks (the file
    system, the network) is faster than the consumer (our processing logic), or vice
    versa. We must ensure that if an error occurs in the middle of processing, we
    clean up resources properly. We must signal when the data stream has ended.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the **stream paradigm** enters. A stream is an abstraction for
    managing the flow of chunked data. It handles the mechanics of reading chunks,
    buffering them when necessary, and delivering them to your processing logic. It
    provides a structured way to think about and implement chunked, asynchronous data
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: The Two Fundamental Models of Streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two fundamentally different ways to organize the flow of data in a
    streaming system. These models are not specific to Node.js. They represent two
    opposing philosophies about who controls the flow of data, and they appear in
    many different programming environments and paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: The first model is **push-based streaming**. In this model, the producer of
    data actively pushes chunks to the consumer. The producer decides when to send
    data. The consumer receives data whenever the producer chooses to send it. This
    is the model of event-driven systems. The producer emits events, and the consumer
    reacts to those events.
  prefs: []
  type: TYPE_NORMAL
- en: The second model is **pull-based streaming**. In this model, the consumer actively
    requests chunks from the producer. The consumer decides when it is ready for more
    data and explicitly asks for it. The producer responds to these requests. This
    is the model of iterator-based systems. The consumer iterates over the data source,
    pulling values one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'PUSH: Producer Controls'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Producer decides WHEN to send data
  prefs: []
  type: TYPE_NORMAL
- en: IN CONTROLPRODUCERsetInterval(() => emit())Waiting...REACTSCONSUMERon('data',
    handle)KEY TRAITS✓Multiple consumers can listen✓Event-driven architecture✗Consumer
    can be overwhelmed
  prefs: []
  type: TYPE_NORMAL
- en: 'PULL: Consumer Controls'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consumer decides WHEN to request data
  prefs: []
  type: TYPE_NORMAL
- en: WAITSPRODUCERfunction* generate()Idle...IN CONTROLCONSUMERiterator.next()KEY
    TRAITS✓Natural backpressure control✓Lazy evaluation possible✗Typically single
    consumer
  prefs: []
  type: TYPE_NORMAL
- en: 'These two models have different characteristics, different trade-offs, and
    different use cases. Node.js streams, as we will see, attempt to combine both
    models into a single, flexible abstraction. But before we can understand that
    hybrid model, we must first understand the pure forms: push and pull.'
  prefs: []
  type: TYPE_NORMAL
- en: Push Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The push model has deep roots in software design. It is formalized in the **Observer
    pattern**, one of the classic design patterns documented in the 1994 "Gang of
    Four" book. The Observer pattern describes a one-to-many dependency between objects:
    when the subject (the observable) changes state, all of its observers (the subscribers)
    are notified automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of streaming, the subject is the data source, and the observers
    are the consumers of that data. When the data source has new data available, it
    notifies all registered consumers by pushing that data to them.
  prefs: []
  type: TYPE_NORMAL
- en: In Node.js, the fundamental building block for push-based systems is the `EventEmitter`
    class. This class, which you have already encountered in your study of the event
    loop and asynchronous primitives, provides a simple but powerful mechanism for
    implementing the Observer pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Observer Pattern in Push Streams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One EventEmitter broadcasts to multiple listeners simultaneously
  prefs: []
  type: TYPE_NORMAL
- en: AUTO-PLAYING - Next emit in 2sEVENT EMITTER
  prefs: []
  type: TYPE_NORMAL
- en: Subject
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: stream.emit('data', chunk)IDLE-waiting...LISTENER 1Observer⏳ Waiting...LISTENER
    2Observer⏳ Waiting...LISTENER 3Observer⏳ Waiting...CODE EXAMPLE
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let us build a simple push-based stream from scratch using `EventEmitter`. This
    will not be a production-ready stream implementation - Node.js already provides
    that - but building it ourselves will clarify the mechanics of the push model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This simple class extends `EventEmitter` and implements a push stream. It takes
    an array of data chunks in its constructor. When `start()` is called, it begins
    pushing chunks to any listeners by emitting `data` events. When all chunks have
    been pushed, it emits an `end` event to signal completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'The consumer uses this stream by registering event listeners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is the essence of the push model. The stream actively pushes data to the
    consumer. The consumer does not request data; it simply reacts to data when it
    arrives.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you might be wondering about the use of `setImmediate()` in the `_pushNext()`
    method. This is not strictly necessary for the logic to work, but it is important
    for the behavior. Without `setImmediate()`, all the data would be pushed synchronously
    in a tight loop during the call to `start()`. By using `setImmediate()`, we ensure
    that each chunk is pushed in a separate event loop tick. This gives the event
    loop a chance to process other events and prevents our stream from monopolizing
    the CPU. This is a simple form of yielding, a pattern you will see repeatedly
    in Node.js's asynchronous architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Push Model's Advantages and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The push model has several advantages. First, it is conceptually simple. The
    producer decides when to produce data, and the consumer simply reacts. This maps
    naturally to event-driven architectures, which are pervasive in Node.js.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the push model can be very efficient when the producer and consumer
    operate at similar speeds. If the producer can generate data as fast as the consumer
    can process it, the data flows smoothly with minimal buffering.
  prefs: []
  type: TYPE_NORMAL
- en: Third, the push model allows for multiple consumers. Because the producer emits
    events, any number of listeners can subscribe to those events and receive the
    same data stream. This fan-out pattern is natural in the Observer pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the push model has a fundamental problem: **backpressure**. What happens
    if the producer is faster than the consumer? In our simple implementation above,
    the producer pushes data as fast as it can, regardless of whether the consumer
    is ready for it. If the consumer takes time to process each chunk - perhaps it
    is writing to a slow disk or making a network request - the producer will keep
    pushing more data. These chunks must be buffered somewhere, waiting for the consumer
    to process them. The buffer grows unbounded, consuming memory, until eventually
    the program runs out of memory and crashes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a production push-based system, we need a mechanism for the consumer to
    signal to the producer: "I am not ready for more data yet. Slow down." This is
    backpressure. The consumer pushes back against the producer to regulate the flow.
    Implementing backpressure in a push-based system is non-trivial. The consumer
    must have a way to tell the producer to pause, and the producer must respect that
    signal. This requires a more sophisticated contract between producer and consumer
    than simply emitting events.'
  prefs: []
  type: TYPE_NORMAL
- en: Node.js streams implement backpressure, as we will see in later chapters. But
    the point here is that backpressure does not naturally fall out of the pure push
    model. It must be added as an additional layer of complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Backpressure Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fast producer overwhelms slow consumer - buffer grows unbounded
  prefs: []
  type: TYPE_NORMAL
- en: FAST PRODUCEREmitting chunks rapidlySLOW CONSUMERProcessing slowlyBUFFER STATE0
    / 8 chunksBuffer OKBuffer within safe limits
  prefs: []
  type: TYPE_NORMAL
- en: Pull Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pull model inverts the control flow. Instead of the producer pushing data
    to the consumer, the consumer pulls data from the producer. The consumer decides
    when it is ready for the next chunk and explicitly requests it.
  prefs: []
  type: TYPE_NORMAL
- en: In JavaScript, the pull model is formalized in the **Iterator** and **Iterable**
    protocols. These protocols define a standard way for objects to produce a sequence
    of values on demand. You have likely used iterators without thinking deeply about
    them. When you write a `for...of` loop over an array, you are using the array's
    built-in iterator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us examine the Iterator protocol. An iterator is an object with a `next()`
    method. Each call to `next()` returns an object with two properties: `value` (the
    next item in the sequence) and `done` (a boolean indicating whether the sequence
    is complete).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple pull-based stream implemented as an iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The consumer uses this stream by explicitly calling `next()` to pull each chunk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is the essence of the pull model. The consumer is in control. It pulls
    data when it is ready. The producer simply responds to those pull requests.
  prefs: []
  type: TYPE_NORMAL
- en: Generators and Iterable Protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JavaScript provides syntactic sugar for implementing iterators: **generator
    functions**. A generator function is a special kind of function that can pause
    its execution and resume later, yielding values one at a time. Generator functions
    are marked with an asterisk (`function*`) and use the `yield` keyword to produce
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our pull stream reimplemented as a generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This generator produces the same sequence of values as our manual iterator,
    but the syntax is much more concise. Under the hood, the generator function automatically
    implements the Iterator protocol. When you call a generator function, it returns
    an iterator object. Each call to the iterator's `next()` method resumes the generator
    function's execution until the next `yield` statement.
  prefs: []
  type: TYPE_NORMAL
- en: Generators also implement the **Iterable** protocol. An iterable is an object
    that has a method with the key `Symbol.iterator`, which returns an iterator. Arrays
    are iterable. Strings are iterable. Generator functions return iterables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because generators are iterable, we can use them with `for...of` loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `for...of` loop automatically calls the iterator's `next()` method behind
    the scenes, pulling values until `done` is `true`.
  prefs: []
  type: TYPE_NORMAL
- en: Generator Call-Yield-Resume Cycle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interactive visualization of how generators pause and resume
  prefs: []
  type: TYPE_NORMAL
- en: STEP 1 OF 8INITIAL CALLGenerator created`const gen = myGenerator()`GENERATOR
    FUNCTION
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'CURRENT STATEPausedValues yielded: 0'
  prefs: []
  type: TYPE_NORMAL
- en: Async Iterators and `for await...of`
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generators solve the problem of synchronous sequences, but real-world data streams
    are asynchronous. Reading from a file, fetching from a network, querying a database
    - all of these operations are inherently asynchronous in Node.js. We need a way
    to pull data asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: JavaScript provides **async iterators** for this purpose. An async iterator
    is like a regular iterator, but its `next()` method returns a Promise that resolves
    to the next value. Async generator functions are marked with `async function*`
    and can use `await` inside them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an async pull stream that simulates asynchronous data production:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The consumer uses `for await...of` to pull from an async iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `for await...of` loop automatically handles the Promises returned by the
    async iterator's `next()` method. Each iteration waits for the next Promise to
    resolve before proceeding. This makes asynchronous pull-based streaming feel as
    natural as synchronous iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Async iterators are a relatively recent addition to JavaScript (standardized
    in ES2018), but they are extremely powerful. They provide a clean, composable
    way to work with asynchronous sequences of data. Node.js streams support async
    iteration, as we will see.
  prefs: []
  type: TYPE_NORMAL
- en: Pull Model's Advantages and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pull model has its own set of advantages. First and foremost, **backpressure
    is implicit**. Because the consumer explicitly pulls each chunk, the producer
    cannot overwhelm the consumer. The producer only produces data when requested.
    If the consumer is slow, it simply pulls less frequently, and the producer idles,
    waiting for the next pull. There is no need for complex signaling between producer
    and consumer to regulate flow. The pull mechanism itself provides the regulation.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the pull model maps naturally to lazy evaluation. The producer can avoid
    doing work until the consumer actually requests data. If the consumer only pulls
    the first few items from a potentially infinite sequence, the producer never generates
    the rest. This can be a significant efficiency win.
  prefs: []
  type: TYPE_NORMAL
- en: Third, the pull model composes elegantly. You can chain multiple pull-based
    transformations together, and each stage will only pull from the previous stage
    when it needs data. The entire pipeline is driven by the final consumer's pull
    requests, propagating backward through the chain.
  prefs: []
  type: TYPE_NORMAL
- en: However, the pull model has limitations. It is less natural for event-driven
    systems. If data arrives unpredictably (for example, messages over a WebSocket
    connection), it does not fit cleanly into the pull model. You cannot pull data
    that has not yet arrived. The pull model works best when the producer can generate
    data on demand, not when the producer is itself reacting to external events.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the pull model does not naturally support fan-out. An iterator
    produces a sequence of values, and that sequence is consumed by pulling. Once
    a value is pulled, it is consumed. If you want multiple consumers to receive the
    same data, you need to implement a separate mechanism to broadcast or tee the
    stream.
  prefs: []
  type: TYPE_NORMAL
- en: Node.js Streams's Hybrid Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Node.js streams are neither purely push nor purely pull. They are a hybrid model
    that combines the advantages of both approaches while mitigating their limitations.
  prefs: []
  type: TYPE_NORMAL
- en: At their core, Node.js streams are push-based. They extend `EventEmitter`, and
    data flows through them via `data` events. This makes them a natural fit for Node.js's
    event-driven architecture. However, Node.js streams implement backpressure explicitly.
    Consumers can signal to producers that they are not ready for more data, and producers
    must respect this signal. This backpressure mechanism adds pull-like control to
    the push-based architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, Node.js streams support async iteration. You can consume a Readable
    stream using `for await...of`, treating it as a pull-based async iterator. Under
    the hood, the async iterator pulls from the stream''s internal buffer, and the
    stream manages the flow from the underlying data source. This allows you to use
    whichever consumption model best fits your use case: event-based (push) or iterator-based
    (pull).'
  prefs: []
  type: TYPE_NORMAL
- en: This hybrid approach is not without complexity. Node.js streams have gone through
    several iterations in their design, and the API has evolved over time to add new
    features and address discovered issues. We will explore this history briefly,
    because understanding how streams evolved helps us understand why they work the
    way they do today.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go back... in time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Streams have been part of Node.js since the very beginning. The initial implementation
    was simple: streams emitted `data` events, and consumers listened for those events.
    There was no concept of pausing or backpressure. If the consumer could not keep
    up, data would accumulate in memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Node.js version **0.10** introduced Streams2, a major redesign that added explicit
    support for backpressure. Readable streams gained two modes of operation: "paused"
    and "flowing." In paused mode, the consumer explicitly calls `read()` to pull
    data from the stream''s internal buffer. In flowing mode, the stream pushes data
    to the consumer via `data` events, but the consumer can pause the stream to signal
    backpressure. Writable streams gained a mechanism where the `write()` method returns
    a boolean indicating whether the internal buffer is full, signaling to the producer
    to stop writing until a `drain` event is emitted.'
  prefs: []
  type: TYPE_NORMAL
- en: Node.js **v10.0** and beyond refined this model further, adding features like
    `stream.pipeline()` for robust error handling, `stream.finished()` for detecting
    stream completion, and async iterator support. These additions made streams more
    ergonomic and reliable for production use.
  prefs: []
  type: TYPE_NORMAL
- en: Today, Node.js streams are a mature, battle-tested abstraction. They are used
    throughout the Node.js ecosystem - by the `fs` module for file I/O, by the `http`
    module for request and response bodies, by `zlib` for compression, by `crypto`
    for encryption, and by countless third-party libraries.
  prefs: []
  type: TYPE_NORMAL
- en: The Four Stream Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Node.js defines four fundamental types of streams. Each type represents a different
    role in the data flow.
  prefs: []
  type: TYPE_NORMAL
- en: '**Readable streams** are sources of data. They produce data that can be consumed.
    Examples include `fs.createReadStream()` for reading files, `http.IncomingMessage`
    for HTTP request bodies on the server side or response bodies on the client side,
    and `process.stdin` for reading from standard input.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Writable streams** are sinks for data. They consume data that can be written
    to them. Examples include `fs.createWriteStream()` for writing files, `http.ServerResponse`
    for HTTP response bodies, and `process.stdout` for writing to standard output.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transform streams** are both readable and writable. They consume data, transform
    it in some way, and produce new data. They sit in the middle of a pipeline, accepting
    input on their writable side and emitting output on their readable side. Examples
    include `zlib.createGzip()` for compression and `crypto.createCipheriv()` for
    encryption. Transform streams are subclasses of Duplex streams with a simplified
    interface for the common case where the readable output is directly derived from
    the writable input.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Duplex streams** are also both readable and writable, but unlike Transform
    streams, their readable and writable sides are independent. They represent two-way
    communication channels. The most common example is `net.Socket`, which represents
    a TCP connection. Data written to a socket is sent over the network, and data
    received from the network can be read from the socket. The two directions of data
    flow are separate; writing to the socket does not directly affect what can be
    read from it.'
  prefs: []
  type: TYPE_NORMAL
- en: These four types form the vocabulary of streaming in Node.js. By combining them,
    you can construct complex data processing pipelines. A Readable stream can be
    piped to a Transform stream, which can be piped to another Transform stream, which
    can be piped to a Writable stream. Each stage processes data incrementally, in
    chunks, with backpressure propagating backward through the pipeline to ensure
    memory usage remains bounded.
  prefs: []
  type: TYPE_NORMAL
- en: Conceptualizing Data Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us visualize how data flows through a stream pipeline. Imagine a simple
    pipeline with three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: A Readable stream (the source) reads chunks from a file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Transform stream (the processor) converts each chunk to uppercase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Writable stream (the sink) writes each chunk to another file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data flows forward through the pipeline: from the Readable stream to the Transform
    stream to the Writable stream. Each chunk moves through the stages in sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But control signals flow backward. If the Writable stream''s internal buffer
    fills up (perhaps because disk writes are slow), it signals backpressure. The
    Transform stream, seeing that the Writable stream cannot accept more data, pauses
    its own consumption from the Readable stream. The Readable stream, seeing that
    no one is pulling data from it, stops reading from the file. The entire pipeline
    pauses until the Writable stream''s buffer drains and it emits a signal that it
    is ready for more data. At that point, the pipeline resumes: the Readable stream
    reads more data, the Transform stream processes it, and the Writable stream writes
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: This bidirectional flow - data forward, backpressure backward - is the key to
    bounded memory usage in stream pipelines. Without backpressure, the fast stages
    would produce data faster than the slow stages could consume it, and buffers would
    grow without limit. With backpressure, the pipeline self-regulates, ensuring that
    every stage operates at the speed of the slowest stage.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional Pipeline Flow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data flows forward ➜ Backpressure signals flow backward ⬅
  prefs: []
  type: TYPE_NORMAL
- en: STAGE 1
  prefs: []
  type: TYPE_NORMAL
- en: Readable Stream
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: fs.createReadStream()ACTIVE - Reading dataSTAGE 2
  prefs: []
  type: TYPE_NORMAL
- en: Transform Stream
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: zlib.createGzip()STAGE 3
  prefs: []
  type: TYPE_NORMAL
- en: Writable Stream
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: fs.createWriteStream()FORWARD DATA FLOW
  prefs: []
  type: TYPE_NORMAL
- en: Chunks move from source → transform → destination
  prefs: []
  type: TYPE_NORMAL
- en: BACKPRESSURE SIGNAL
  prefs: []
  type: TYPE_NORMAL
- en: Pause signal travels backward when buffer is full
  prefs: []
  type: TYPE_NORMAL
- en: When to Use Streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Streams are not always the right tool. For small amounts of data that easily
    fit in memory, reading the entire dataset into a Buffer or string is simpler and
    often faster. Streams add overhead - the event loop must schedule callbacks, data
    must be chunked, and backpressure must be managed. If you are processing a 10KB
    JSON file, streams are overkill. Just use `fs.readFile()` or `fs.readFileSync()`,
    parse the JSON, and be done.
  prefs: []
  type: TYPE_NORMAL
- en: Streams shine when working with large datasets or unbounded data. If you are
    processing a multi-gigabyte log file, streams are essential. If you are handling
    an incoming HTTP request body of unknown size, streams are the correct abstraction.
    If you are implementing a network protocol where messages arrive continuously,
    streams provide the structure you need.
  prefs: []
  type: TYPE_NORMAL
- en: Streams are also valuable when you want to start processing data before all
    of it is available. Consider an HTTP server responding to a file download request.
    Without streams, the server would have to read the entire file into memory before
    starting to send the response. With streams, the server can start sending the
    first chunks of the file as soon as they are read from disk, significantly reducing
    the time to first byte for the client.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, streams are useful for composing pipelines of transformations. If you
    need to read a file, decompress it, parse it, transform the parsed data, and write
    the result to another file, streams allow you to express this as a clean, linear
    pipeline where each stage is a separate, focused transformation. This composability
    is a major advantage of the stream abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: Common Use Cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several patterns appear repeatedly when working with streams in Node.js.
  prefs: []
  type: TYPE_NORMAL
- en: '**File I/O** is the most common use case. Reading and writing large files should
    almost always be done with streams. This avoids loading the entire file into memory
    and allows processing to begin immediately.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network communication** is inherently streaming. HTTP request and response
    bodies are streams. TCP sockets are duplex streams. When you send data over a
    network, you do not have all the data up front; it is generated or received incrementally.
    Streams are the natural abstraction for network protocols.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data transformation pipelines** are a perfect fit for streams. Any time you
    have a series of transformations to apply to data - parsing, filtering, mapping,
    aggregating - streams allow you to express each transformation as a separate,
    composable stage. This is common in ETL (Extract, Transform, Load) workflows,
    log processing, and data analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time data processing** often uses streams. If you are processing events
    from a message queue, sensor data from IoT devices, or user interactions in a
    web application, streams provide a way to handle each event as it arrives without
    accumulating events in memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Proxying and multiplexing** leverage the duplex nature of sockets. When building
    a proxy server or a load balancer, you pipe data between sockets, forwarding requests
    and responses without buffering the entire message. This allows the proxy to handle
    very large requests and responses efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting the Stage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have now established the conceptual foundation for streams. We understand
    the problem they solve: processing large or unbounded data without exhausting
    memory. We understand the two fundamental models: push and pull, and we have seen
    how Node.js streams combine both. We understand the four stream types and the
    roles they play in data flow.'
  prefs: []
  type: TYPE_NORMAL
- en: What we have not yet done is implement or use real Node.js streams. We have
    built simple examples to illustrate concepts, but we have not explored the actual
    `stream.Readable`, `stream.Writable`, `stream.Transform`, and `stream.Duplex`
    classes. We have not examined how to implement custom streams, how to configure
    their behavior, or how to construct robust pipelines with error handling.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is the work of the next chapters, where we will dive deep into Readable
    streams: how they are implemented, how they manage internal buffers, how their
    two modes of operation work, and how to create custom Readable streams from various
    data sources.'
  prefs: []
  type: TYPE_NORMAL
- en: But all of that rests on the foundation we have built here. Streams are not
    magic. They are a systematic response to the constraints of memory and the realities
    of asynchronous I/O. They implement well-established patterns - Observer, Iterator
    - adapted to the specific needs of Node.js's event-driven architecture. By understanding
    streams from first principles, you will be able to reason about their behavior,
    debug problems when they arise, and design your own streaming systems with confidence.
  prefs: []
  type: TYPE_NORMAL
- en: It is going to get interesting from now onwards!
  prefs: []
  type: TYPE_NORMAL
