- en: The Event Loop Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://www.thenodebook.com/node-arch/event-loop-intro](https://www.thenodebook.com/node-arch/event-loop-intro)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Single-Threaded Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you're here, you've probably written some Node.js code before. You live and
    breathe asynchronicity. You probably write `Promise.then()` chains in your sleep
    and use `async/await` with the kind of fluency that makes others jealous. You
    know - deep in your bones - that you must *never, ever* block the main thread.
    You get the *what* of writing non-blocking code.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is about the why and the how. We're going to open up that "black
    box" you've gotten used to and take a good, hard look at the cool, complex machinery
    that make it happen.
  prefs: []
  type: TYPE_NORMAL
- en: This isn't a beginner's guide. Far from it. This is a deep dive for the practicing
    engineer who's ready to graduate from a "it just works" intuition to a precise,
    mechanical model of the Node.js runtime. We‚Äôre going to break down how Node handles
    so many things at once, getting past the simple explanations to see how it really
    works underneath.
  prefs: []
  type: TYPE_NORMAL
- en: 'The central paradox of Node.js, the question that keeps people up at night,
    is this: how can it handle *tens of thousands* of simultaneous connections on
    a single thread? A single worker, with no parallel execution for your JavaScript,
    achieving that kind of scale? It sounds impossible. The answer is the ***event
    loop***. By the time we‚Äôre done here, you won''t just know its name; you‚Äôll understand
    its phases, its priorities, and how it talks to the other core parts of the Node
    runtime. You‚Äôll be able to predict the execution order of any async code with
    confidence and hunt down performance bugs that used to feel like ghosts in the
    machine.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's get our hands dirty.
  prefs: []
  type: TYPE_NORMAL
- en: The Call Stack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can even whisper the word "asynchronous," we have to get comfortable
    with its opposite. The bedrock of all JavaScript execution is the synchronous
    call stack. Think of it as a stack of plates. Every time you call a function,
    you're placing a new plate - a "frame" - on top of the stack. This frame holds
    all the function's arguments and local variables.
  prefs: []
  type: TYPE_NORMAL
- en: The call stack is a classic Last-In, First-Out (LIFO) structure. The last plate
    you put on is the first one you take off. When a function finishes its job and
    returns, its frame is popped off the top of the stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s trace this ridiculously simple synchronous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here‚Äôs how this works -
  prefs: []
  type: TYPE_NORMAL
- en: '`first()` is called. Its frame is pushed onto the stack. `[first]`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`first()` logs "One". Easy enough.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`first()` calls `second()`. `second()`''s frame is plopped right on top. `[first,
    second]`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`second()` logs "Two".'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`second()` calls `third()`. `third()`''s frame gets added to the growing tower.
    `[first, second, third]`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`third()` logs "Three". It''s all out of work, so it returns. Its frame is
    popped. `[first, second]`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Control returns to `second()`. It has nothing left to do, so it returns. Its
    frame is popped. `[first]`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We're back in `first()`. It logs "Done with first", runs out of lines, and returns.
    Its frame is popped. `[]`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The stack is now empty. The script is done. This is the one and only workspace
    for *all* of your JavaScript. There‚Äôs just one call stack, and it can only do
    one thing at a time: whatever‚Äôs on the very top of that stack.'
  prefs: []
  type: TYPE_NORMAL
- en: What "Blocking" Truly Means
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '"Blocking" isn''t some fuzzy, abstract concept. It''s a direct, brutal consequence
    of having a single call stack. To "block the event loop" is just a fancy way of
    saying you‚Äôve put a function on the call stack that refuses to leave. It just
    sits there, taking forever to finish its work.'
  prefs: []
  type: TYPE_NORMAL
- en: And while that function's frame is hogging the top of the stack, nothing else
    - *nothing* - can run. The entire process is held hostage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs look at a more realistic example: a CPU-heavy crypto operation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Run this, and you'll see something interesting. The `setTimeout` callback, which
    should have fired after a second, only shows up *after* "Blocking operation finished."
    What gives?
  prefs: []
  type: TYPE_NORMAL
- en: We call `setTimeout`. Node‚Äôs APIs happily schedule a timer to go off in 1000ms.
    This is a non-blocking, fire-and-forget operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`hashContinuously()` gets pushed onto the call stack.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `while` loop from hell begins. For five painful seconds, the V8 engine is
    completely consumed with hashing a value over and over. The `hashContinuously`
    frame just sits at the top of the stack.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the 1-second mark, the timer patiently "fires." All this means is its callback
    function is placed into a queue, ready and waiting to be executed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**But here''s the catch:** the event loop can''t touch that queue. Why? Because
    the call stack isn''t empty! It‚Äôs still stuck with `hashContinuously()`. The event
    loop is effectively frozen, tapping on the glass, waiting for the stack to clear.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, after five seconds, the `while` loop ends. `hashContinuously()` logs
    its last message and returns. Its frame is mercifully popped from the stack.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At last, the call stack is empty. The event loop springs to life, grabs the
    waiting timer callback, shoves it onto the stack, and executes it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is the dictatorship of the stack. One slow function can bring your entire
    app to a screeching halt. This is the very problem Node's entire asynchronous
    architecture was built to solve. So, if the call stack is the bottleneck, where
    does the solution come from? To answer that, we have to look under the hood, beyond
    JavaScript itself.
  prefs: []
  type: TYPE_NORMAL
- en: V8, Libuv, and the Node.js Bindings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like you read in the previous chapter [Inside the v8 Engine](https://thenodebook.com/node-arch/v8-engine-intro),
    the thing we call the "Node.js runtime" isn't one single program. It's more like
    a supergroup - a few powerful technologies playing together in perfection. Understanding
    their distinct roles is really important.
  prefs: []
  type: TYPE_NORMAL
- en: '**V8: The JavaScript Execution Engine**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ‚ÑπÔ∏èNote
  prefs: []
  type: TYPE_NORMAL
- en: We talked about [v8 in a lot of depth in the previous chapter](https://thenodebook.com/node-arch/v8-engine-intro)
    and there‚Äôs also going to be a dedicated lesson with six chapters (21.1‚Äì21.6).
    Still, I assume some readers will jump straight to this chapter, so I‚Äôll provide
    a brief explanation for the sake of clarity.
  prefs: []
  type: TYPE_NORMAL
- en: V8 is Google's legendary open-source JavaScript engine, written in C++. If Node
    were a car, V8 would be the engine (hence the name). It‚Äôs the part that actually
    executes your JavaScript code. Its main jobs are as follwos -
  prefs: []
  type: TYPE_NORMAL
- en: V8 grabs your raw JavaScript and, through a ridiculously smart Just-In-Time
    (JIT) compilation process, transforms it into highly optimized native machine
    code. It‚Äôs why modern JavaScript is so fast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like we saw, V8 is the strict manager of the single call stack, pushing
    and popping frames.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: V8 handles all the memory allocation for your objects and variables in a place
    called the heap. It's also the garbage collector, cleaning up messes you're done
    with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, here''s what''s absolutely critical to get: what V8 *doesn''t* do. By
    itself, V8 is clueless about the outside world. It has no concept of a file system,
    it doesn''t know how to open a network socket, and it has no idea how to set a
    timer. Functions like `setTimeout`, `fs.readFile`, and `http.createServer`? They
    aren''t part of JavaScript or V8\. They are APIs provided by browsers or, in our
    case, Node.js.'
  prefs: []
  type: TYPE_NORMAL
- en: Think of V8 as a brilliant linguistics professor who only speaks JavaScript.
    To do anything in the real world, it needs an interpreter and a helper.
  prefs: []
  type: TYPE_NORMAL
- en: Libuv
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Libuv is a C library built from the ground up for one purpose: asynchronous
    I/O. It was originally made for Node.js, and it''s the secret sauce that gives
    Node its event-driven, non-blocking superpowers. Its responsibilities are huge,
    but we can lump them into three big buckets -'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Event Loop Itself.** That''s right. The event loop we keep talking about?
    It''s implemented and run by Libuv. The six phases we''ll get into shortly are
    all orchestrated by Libuv''s C code. When you start a Node process, it''s Libuv
    that kicks off the loop.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Abstracting the Operating System.** This is where the real magic happens.
    Different operating systems have their own super-efficient ways of handling async
    operations. Linux has `epoll`, macOS has `kqueue`, and Windows has its I/O Completion
    Ports (IOCP). These are kernel-level tools that let a program say, "Hey, watch
    these files and network sockets for me, and just wake me up when something interesting
    happens." Libuv provides a single, beautiful C API that works on top of all these
    different systems. This is why your `http.createServer` code runs performantly
    everywhere without you having to change a single line. When you tell Node to listen
    on a port, it''s Libuv making the right non-blocking call to the OS.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Thread Pool.** Okay, this is a common point of confusion, so lean in.
    We say Node is single-threaded, but that''s only half the story. Your JavaScript
    runs on a single thread, yes. But Libuv maintains its own small, internal pool
    of threads. Why? Because as great as modern OSes are, some operations are just
    unavoidably, stubbornly blocking. This includes most file system stuff, some DNS
    lookups, and a few CPU-intensive crypto functions. If Node ran these on the main
    thread, they''d block the loop - game over. Instead, Libuv cleverly delegates
    these specific jobs to its thread pool. A worker thread from the pool makes the
    slow, blocking system call. When it''s done, it signals the main event loop, which
    can then finally execute your JavaScript callback. The default size of this pool
    is four, but you can change it with the `UV_THREADPOOL_SIZE` environment variable
    (a very handy trick to know!).'
  prefs: []
  type: TYPE_NORMAL
- en: Node.js C++ Bindings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So now we have two separate worlds. We have the V8 world, which speaks JavaScript,
    and the Libuv world, a C library that speaks in file descriptors and system calls.
    How on earth do they talk to each other?
  prefs: []
  type: TYPE_NORMAL
- en: They communicate through a set of C++ programs called the **Node.js bindings**.
    These bindings are the crucial translation layer, the bridge that connects the
    world of V8 to the world of Libuv.
  prefs: []
  type: TYPE_NORMAL
- en: When you make a seemingly simple call like `fs.readFile('/path/to/file', callback)`,
    a whole dance happens behind the scenes -
  prefs: []
  type: TYPE_NORMAL
- en: '**V8** sees the `fs.readFile` function call and starts executing it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: But wait! This function isn't pure JavaScript; it's a binding. The call is immediately
    routed to a specific C++ function inside Node's source code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This C++ binding function acts as a translator. It takes your JavaScript arguments
    (the file path and your callback function) and packages them up into a "request"
    object that Libuv can understand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The binding then hands this request over to **Libuv**, telling it, "Go read
    this file for me, and please use the thread pool since this might take a while."
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Libuv does its thing. Once the file is read, it puts a "completion event" on
    a queue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Later, the **event loop** (which is being run by Libuv) sees this event waiting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The event signals back to the C++ bindings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bindings take the result (either the file data or an error), translate it
    back into something JavaScript-friendly, and then - finally! - it invokes your
    original JavaScript **callback function** with those results, pushing that callback
    onto the V8 **call stack** to be executed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Phew. That round trip - from JavaScript to C++ to Libuv to the OS and all the
    way back again - is the life story of every single async operation in Node.js.
  prefs: []
  type: TYPE_NORMAL
- en: The Six Phases in Detail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: üí°Tip
  prefs: []
  type: TYPE_NORMAL
- en: There's an awesome tool created by [@vagostep](https://github.com/vagostep)
    that allows you to visualize how the Event Loop works. You might want to play
    around with it. Here's the link - [NodeLoops](https://nodeloops.com/)
  prefs: []
  type: TYPE_NORMAL
- en: The event loop isn't just one big queue. That's a common misconception. It's
    a highly structured, multi-phase cycle. Each full lap through this cycle is called
    a "tick." Getting your head around these phases is the absolute key to understanding
    why async operations execute in the order they do. Libuv's loop is just a repeating
    journey through these six core phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The "Tick": An Overview of a Single Iteration'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First things first: a "tick" is not a unit of time. It''s just a single, complete
    progression through all the phases of the event loop. A tick doesn‚Äôt necessarily
    equal a specific number of milliseconds; how long it takes depends on the work
    done during that iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÑπÔ∏èNote
  prefs: []
  type: TYPE_NORMAL
- en: 'If you‚Äôve played (or developed) games, think of a tick as a frame: the event
    loop, like a game loop, repeatedly performs work once per tick.'
  prefs: []
  type: TYPE_NORMAL
- en: During a tick, the loop checks the queue for each phase. If a phase's queue
    has callbacks waiting, it will execute them one by one, in First-In, First-Out
    (FIFO) order, until the queue is empty or some system-dependent limit is hit.
    Then, it shuffles along to the next phase. Simple as that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Phase 1: Timers'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the first stop on our tour. The loop's only job here is to run callbacks
    scheduled by `setTimeout()` and `setInterval()`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, technically, a timer callback isn't guaranteed to run at the *exact* millisecond
    you specified. The delay you provide is the *minimum* time until the callback
    is eligible to run. When the loop enters this phase, it checks the clock. Has
    the time for any of our scheduled timers passed? If so, their callbacks are run.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering how Node can handle thousands of timers without constantly
    checking a giant list. It‚Äôs cleverer than that. Libuv uses a special data structure
    called a **min-heap**. A min-heap is a tree-like structure where the smallest
    element is always at the root. In this context, "smallest" means the timer that's
    set to expire next. This lets Libuv know how long it can "sleep" until the next
    timer is due in O(1) time - incredibly fast. This is a huge reason why Node's
    timers are so cheap.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÑπÔ∏èNote
  prefs: []
  type: TYPE_NORMAL
- en: Libuv uses a min-heap so the next-expiring timer can be discovered in O(1) time,
    but inserting or removing timers is O(log n). That makes timers efficient for
    large sets, but creating or canceling many timers still has non-zero cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Phase 2 & 3: Pending Callbacks and Internal Operations'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After timers, the loop zips through a couple of internal phases you rarely interact
    with directly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 2: Pending Callbacks -** This phase runs I/O callbacks that were deferred
    to the next loop iteration. A weird edge case, really. For instance, if a TCP
    socket throws an `EAGAIN` error while writing data, Node will queue the callback
    to be retried here. For 99% of developers, this phase is just background noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Phase 3: Idle, Prepare -** These are used internally by Libuv for housekeeping
    before it gets to the really important stuff. Not exposed to us in JavaScript
    land at all.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Phase 4: The Poll Phase'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ‚ÑπÔ∏èNote
  prefs: []
  type: TYPE_NORMAL
- en: '**Poll**(ing) generally means asking ‚Äúis there anything ready?‚Äù repeatedly.
    For example, asking the kernel which I/O handles (file descriptors, sockets, etc.)
    are ready to perform I/O.'
  prefs: []
  type: TYPE_NORMAL
- en: Alright, pay attention, because this is the big one. The poll phase is arguably
    the most important and complex part of the whole loop. It does two main things
    -
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating Wait Time and Polling for I/O.** The loop figures out how long
    it can afford to wait for new I/O events. It looks at when the next timer is due
    and other factors, and then it makes a call to the OS''s notification system (like
    `epoll_wait` on Linux). This is the only "blocking" part of the event loop, but
    it''s a good kind of blocking. The process uses zero CPU, just patiently waiting
    for the kernel to tap it on the shoulder and say, "Hey, that file you were reading
    is done," or "You''ve got a new network connection."'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Processing the Poll Queue.** When the wait is over (either because time''s
    up or an I/O event happened), the loop processes the poll queue. This queue holds
    the callbacks for almost all of your I/O operations: a network connection being
    established, data read from a socket, or a file read (from the thread pool) finishing
    up.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The behavior here is smart. If the poll queue is **not empty**, the loop will
    churn through its callbacks until the queue is drained. But if the poll queue
    **is empty**, the loop's behavior changes -
  prefs: []
  type: TYPE_NORMAL
- en: If any scripts have been scheduled with `setImmediate()`, the loop will immediately
    end the poll phase and move on to the [check phase](#phase-5-the-check-phase)
    to run them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are no `setImmediate()`s waiting, the loop will just hang out here,
    waiting for new I/O events to arrive. When they do, their callbacks will be executed
    right away.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This phase is also where a Node process can decide it's time to die. If the
    event loop enters the poll phase and sees no pending I/O, no active timers, no
    immediates, and no other handles keeping it alive, it concludes there‚Äôs no more
    work to do, and the process gracefully exits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Phase 5: The Check Phase'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This phase is wonderfully simple. It has one job and one job only: execute
    callbacks scheduled by `setImmediate()`. If you need to run some code immediately
    after the poll phase is done with its events, this is your tool.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A use case: A Food Delivery App''s Order Confirmation'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ‚ö†Ô∏èWarning
  prefs: []
  type: TYPE_NORMAL
- en: '`setImmediate()` decouples work for latency reasons but is not durable - it
    executes only while the process is alive. For critical or guaranteed background
    tasks use a persistent job queue (RabbitMQ, Redis queues, Kafka, or a database
    job table) or an external worker to ensure retries and durability. I''m using
    it here as an example to illustrate the event loop.'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are building the backend for a food delivery service like Uber Eats
    or Zomato. When a user places an order, two main things need to happen -
  prefs: []
  type: TYPE_NORMAL
- en: '**Confirm the order -** Write the order details to your main database. This
    is a critical I/O operation and must be completed successfully.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Notify the restaurant -** Send a notification to the restaurant''s tablet
    or system. This is a separate action and should not delay the user''s confirmation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You want to tell the user their order is confirmed as soon as the database write
    is complete. The restaurant notification can happen a split second later; it doesn't
    need to be part of the same database transaction.
  prefs: []
  type: TYPE_NORMAL
- en: How `setImmediate()` Solves This
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You would use `setImmediate()` to decouple the restaurant notification from
    the database confirmation.
  prefs: []
  type: TYPE_NORMAL
- en: The system receives an order and calls a function to save it to the database
    (an I/O operation). This callback will run in the **Poll phase** of the event
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the callback for the database operation, once you know the order is saved
    successfully, you immediately schedule the restaurant notification using `setImmediate()`.
  prefs: []
  type: TYPE_NORMAL
- en: The event loop finishes the Poll phase and immediately moves to the **Check
    phase**, where it executes the `setImmediate()` callback, sending the notification
    to the restaurant.
  prefs: []
  type: TYPE_NORMAL
- en: This ensures that the core, user-facing task (confirming the order in the database)
    is completed as fast as possible. The secondary, internal task (notifying the
    restaurant) is reliably scheduled to happen right after, without slowing down
    the primary one.
  prefs: []
  type: TYPE_NORMAL
- en: Here is what the simplified code logic would look like -
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Phase 6: Close Callbacks**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final phase of a tick is for cleanup. It handles "close" events. For example,
    if you abruptly destroy a socket with `socket.destroy()`, the `'close'` event's
    callback will be fired off in this phase.
  prefs: []
  type: TYPE_NORMAL
- en: After this, the loop checks if there's anything left keeping it alive. If there
    is, the whole cycle starts over again, returning to the timers phase for the next
    tick. And on and on it goes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Express Lane: Microtasks vs. Macrotasks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, we've just laid out the six-lane highway of the event loop. But it turns
    out there's another, higher-priority express lane that operates outside of this
    whole system. Understanding it is really crucial for predicting execution order.
    I welcome to the world of microtasks.
  prefs: []
  type: TYPE_NORMAL
- en: What even are these?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get this right, we need to be a little more formal with our terms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Macrotask (or Task) -** This is any callback that gets placed into one of
    the queues for the six event loop phases. A timer callback? That''s a macrotask.
    An I/O callback? Macrotask. An immediate callback? You guessed it, macrotask.
    The event loop processes macrotasks from *one* phase''s queue per tick.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microtask -** This is a callback that gets placed in a special, high-priority
    queue that lives outside the main loop phases. In Node, there are two of these:
    the `nextTick` queue and the Promise Jobs queue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the Golden Rule of execution order, the one you should tattoo on your
    brain: After **any single macrotask** from any phase is executed, the runtime
    will immediately execute **every single task** currently in the microtask queues
    before it even thinks about moving on to the next macrotask.'
  prefs: []
  type: TYPE_NORMAL
- en: This is huge. It means microtasks can cut in line and execute in between macrotasks
    from the very same phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `process.nextTick()` Queue: The Highest Priority'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The callbacks you schedule with `process.nextTick()` live in the VIP lounge
    of microtask queues. The name is a bit of a lie; it doesn't run on the "next tick."
    It runs *immediately* after the current operations on the call stack finishes,
    before the event loop is even allowed to proceed to the next phase or the next
    macrotask. It's the most aggressive "cut in line" you can do.
  prefs: []
  type: TYPE_NORMAL
- en: This gives it incredible power, but also makes it incredibly dangerous. Because
    the `nextTick` queue is processed in its entirety before the loop can move on,
    a recursive `process.nextTick()` call can starve the event loop, preventing any
    I/O or timers from ever running.
  prefs: []
  type: TYPE_NORMAL
- en: I once spent half a day debugging a server that was completely unresponsive
    to network requests but wasn't crashing. The culprit? A library was accidentally
    calling `process.nextTick` recursively under a specific error condition. The loop
    was spinning forever, just processing microtasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Run that code. It will just print "Starvation call..." forever. The `setTimeout`
    callback will never get a chance because the event loop is perpetually stuck,
    unable to get to the timers phase.
  prefs: []
  type: TYPE_NORMAL
- en: So, Why Does This Happen?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let's walk through the execution of that code to see how it traps the event
    loop in a never-ending cycle.
  prefs: []
  type: TYPE_NORMAL
- en: '`let count = 0;` We start by setting up a simple counter. This is just to prove
    that our function is, in fact, running over and over again.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`function starveTheLoop() { ... }` This is the code in question. We define
    the function, but nothing happens yet. It''s just sitting in memory, waiting to
    be called.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`setTimeout(() => { ... }, 1000);` We schedule a timer. Node.js sees this and
    says, "Okay, cool. In about one second, I''ll put this callback into the **timers
    queue** (a macrotask queue) to be executed." It then moves on immediately. It
    doesn''t wait for the second to pass.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`console.log("Starting the starvation...");` This is the first piece of code
    that actually runs. It''s a synchronous operation. It gets pushed onto the call
    stack, prints its message to the console, and pops off. Easy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`starveTheLoop();` This is where the trap is sprung. We make the first call
    to our function. The following happens -'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`starveTheLoop` is pushed onto the call stack.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It prints `Starvation call: ${++count}`. The console now shows "Starvation
    call: 1".'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now for the critical part: it calls **`process.nextTick(starveTheLoop)`**.
    This doesn''t call the function right away. Instead, it places the `starveTheLoop`
    function into the high-priority `nextTick` queue.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The first `starveTheLoop` call finishes and is popped off the call stack.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main script has now finished executing, and the call stack is empty. The
    event loop is ready to take over. Its job is to check the queues for pending tasks.
    It''s supposed to work through its phases: check timers, check I/O, etc. **BUT**,
    before it can move to *any* phase, it has a strict rule: **"I must process the
    entire `process.nextTick()` queue until it is empty."**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Unwinnable Cycle Begins
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The event loop looks at the `nextTick` queue and sees our `starveTheLoop` function
    waiting there.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It pulls it out and executes it (this is call #2).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function prints "Starvation call: 2".'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: And... it schedules *another* `starveTheLoop` callback in the `nextTick` queue.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This second call finishes. The event loop checks the `nextTick` queue again.
    Is it empty? **Nope!** There's a new task waiting.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, it runs `starveTheLoop` a third time, which prints "Starvation call: 3"
    and puts a *fourth* one right back in the queue.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The event loop is completely stuck. It can never finish processing the `nextTick`
    queue because every time it processes one item, that item puts another one right
    back in. The poor `setTimeout` callback is sitting in the timers queue, waiting
    patiently for its turn, but the event loop never gets past the `nextTick` phase
    to even look at the timers. It has been effectively **starved**.
  prefs: []
  type: TYPE_NORMAL
- en: The Promise Jobs Queue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second microtask queue is for Promises. Whenever a Promise resolves or rejects,
    any callbacks attached via `.then()`, `.catch()`, or `.finally()` are scheduled
    as microtasks in this queue. And yes, our beloved `async/await` is just syntactic
    sugar that uses this very same mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'This queue has a slightly lower priority than `process.nextTick()`. The order
    of operations is always:'
  prefs: []
  type: TYPE_NORMAL
- en: Execute the current macrotask.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drain the *entire* `nextTick` queue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drain the *entire* Promise Jobs queue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Okay, *now* we can proceed to the next macrotask.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you `await` something, you're effectively splitting your `async` function
    in two. Everything *before* the `await` runs synchronously. The rest of the function
    gets wrapped in a `.then()` and scheduled as a microtask on the Promise Jobs queue,
    to be executed after the awaited promise settles.
  prefs: []
  type: TYPE_NORMAL
- en: A Complex Execution Order Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's put this all together with a scary-looking code snippet that will test
    our new mental model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output here is always, deterministically: `1, 9, 4, 3, 2, 5, 7, 8, 6`.
    Let''s walk through why, step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''1\. Start''` and `''9\. End''` are logged synchronously. All the async stuff
    is scheduled.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The main script ends. The call stack is empty. **Golden Rule time: drain the
    microtask queues!**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `nextTick` queue always goes first. We log `'4\. nextTick'`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Promise Jobs queue is next. We log `'3\. Promise'`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Microtask queues are now empty. The event loop can finally begin its first tick.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Phase 1: Timers.** Our `setTimeout(..., 0)` is ready. The macrotask runs,
    and we log `''2\. Timeout''`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loop zips through the next few phases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Phase 4: Poll.** The loop waits for I/O. Eventually, the `fs.readFile` finishes.
    Its callback is now a macrotask in the poll queue, ready to go.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The macrotask is executed. We log `'5\. I/O Callback'`. Inside this function,
    a new immediate, nextTick, and promise are scheduled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The I/O macrotask finishes. What happens now? **Golden Rule time again! We must
    drain microtasks before moving on.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the `nextTick` queue. We find one and log `'7\. nextTick from I/O'`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the Promise Jobs queue. We find one and log `'8\. Promise from I/O'`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Microtask queues are empty again. The loop can now proceed from where it left
    off in the poll phase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Phase 5: Check.** The loop sees the `setImmediate` we scheduled. The macrotask
    runs, and we log `''6\. Immediate from I/O''`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loop finishes its tick, finds nothing else to do, and the process exits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See? Not magic. Just rules.
  prefs: []
  type: TYPE_NORMAL
- en: '3Ps: Performance, Patterns, and Pitfalls'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This isn't just an academic exercise. Really understanding the event loop's
    guts directly affects how you write good code and, more importantly, how you debug
    the bad code.
  prefs: []
  type: TYPE_NORMAL
- en: Obvious and Subtle Blockers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ve already beaten the obvious blockers to death: synchronous APIs like
    `fs.readFileSync` and long, CPU-bound loops. But I''ve seen even senior developers
    get tripped up by more subtle blockers that can poison an application''s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Large JSON Operations**. Here''s a sneaky one. `JSON.parse()` and `JSON.stringify()`
    are 100% synchronous, blocking operations. If you''re handling an API request
    with a massive JSON payload (think tens or hundreds of megabytes), the time it
    takes to parse that can be huge - easily tens or hundreds of milliseconds where
    your loop is completely frozen. If you find yourself in this situation, look into
    streaming JSON parsers like `stream-json`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Complex Regular Expressions**. A poorly written regex is another ticking
    time bomb. There''s a nasty phenomenon called "Catastrophic Backtracking" that
    can cause a regex engine to take an exponentially long time to process certain
    strings. A single malicious user input can trigger this, causing a regex match
    to block the CPU for seconds or even minutes. This is a classic Denial of Service
    (DoS) vector. Always, *always* test your regex against "evil" strings and consider
    using libraries that offer protection.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Libuv Thread Pool Revisited
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that Libuv thread pool? Don't worry if you don't, our next chapter
    is going to be a deep dive into Libuv itself! It's crucial to remember it's a
    global, shared resource, and by default, it only has four threads. While functions
    like `fs.readFile` and `crypto.pbkdf2` *feel* asynchronous from your JavaScript's
    perspective, they're all waiting in line for a very small number of actual threads.
  prefs: []
  type: TYPE_NORMAL
- en: This can create some surprising bottlenecks. Imagine a server that gets a request
    and needs to both read a file from a slow network drive (`fs.readFile`) and verify
    a password (`crypto.pbkdf2`). Now, imagine five of these requests hit at the exact
    same time.
  prefs: []
  type: TYPE_NORMAL
- en: The first four requests will each dispatch a task to the thread pool (let's
    say the file reads get there first). All four threads are now busy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fifth request's `fs.readFile` call is made. Libuv tries to hand it off,
    but the pool is full. This fifth task now has to wait in a queue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What about the password hashing for the first four requests? They *also* have
    to wait in that same queue until one of the file reads finishes and frees up a
    thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suddenly, your slow file system is making your authentication latency skyrocket.
    Everything that uses the thread pool is connected. If you have an app that's heavy
    on file I/O, DNS, and crypto, you might seriously need to consider increasing
    the thread pool size with the `UV_THREADPOOL_SIZE` environment variable to avoid
    this kind of logjam.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling and Debugging the Event Loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So how do you know if your loop is struggling? You have to measure it. Don't
    guess, measure.
  prefs: []
  type: TYPE_NORMAL
- en: '**Method 1: The Poor Man''s Latency Checker.** This is low-tech and feels ''hack''ish
    but surprisingly effective for a quick gut check.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you start seeing warnings, it means a simple `setInterval` macrotask was
    delayed, which is a screaming sign that something else was hogging the CPU.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Method 2: `perf_hooks.monitorEventLoopDelay`.** For a more professional approach,
    Node has a built-in, high-resolution tool for this exact purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is far more accurate and has lower overhead than the `setInterval` hack.
    Use this one in production.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Method 3: `async_hooks`.** This is the big gun. For super advanced debugging,
    the `async_hooks` module lets you trace the entire lifecycle of every async resource
    in your app. It''s incredibly powerful but also complex. You''d typically only
    reach for this if you were building developer tools or an APM (Application Performance
    Management) solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategies for CPU-Bound and Parallel Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes you just have a task that is genuinely CPU-intensive. No amount of
    clever async tricks will fix it. The solution isn't to block the loop; it's to
    move the work off the loop entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Offloading to the Loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For tasks that are long but can be chopped into smaller pieces, you can use
    a clever trick to avoid blocking. The idea is to do one chunk of work, then schedule
    the next chunk using `setImmediate()`. This effectively yields control back to
    the event loop between chunks, allowing it to handle I/O and stay responsive.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This pattern is great for keeping your app from freezing, but notice that it
    doesn't actually speed up the total computation time. For that, we need real parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'True Parallelism: `worker_threads`'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ‚ÑπÔ∏èNote
  prefs: []
  type: TYPE_NORMAL
- en: We have an entire lesson dedicated to `worker_threads` - including 7 chapters.
    This section is just a quick overview. So, don't worry if you don't get all the
    details here. Just understand the big picture.
  prefs: []
  type: TYPE_NORMAL
- en: The `worker_threads` module (stable since node v12) is the modern, definitive
    answer for CPU-bound work. A worker thread is not a thread from the Libuv pool.
    It's a completely separate V8 instance, running on its own thread, with its own
    event loop and its own isolated memory.
  prefs: []
  type: TYPE_NORMAL
- en: That isolation is the killer feature. Because memory isn't shared, you sidestep
    all the classic headaches of multi-threaded programming like race conditions and
    deadlocks. You communicate between the main thread and worker threads safely through
    a message-passing channel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, that awful `for` loop runs on a completely separate CPU core, leaving
    our main thread's event loop free and clear to keep handling web requests or whatever
    else it needs to do.
  prefs: []
  type: TYPE_NORMAL
- en: The `cluster` Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's super important not to confuse `worker_threads` with the older `cluster`
    module. They solve different problems. `cluster` isn't for offloading one heavy
    task. It's a tool for scaling an entire I/O-bound application - like an HTTP server
    - across all of your machine's CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: It works by forking your main Node process into multiple child processes. The
    master process grabs a port (say, 8000) and then acts as a load balancer, handing
    out incoming TCP connections to the worker processes. Each worker is a full copy
    of your Node app with its own independent event loop. This lets an 8-core machine
    run 8 instances of your server, effectively multiplying its capacity to handle
    concurrent connections.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÑπÔ∏èNote
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of their primary scaling strategies this way: `worker_threads` are for
    offloading a specific, long-running CPU-bound task from a single event loop to
    prevent that loop from blocking. The `cluster` module, on the other hand, is for
    scaling your entire application across multiple CPU cores by running multiple,
    independent process instances of it. This is highly effective for your servers,
    as it allows you to handle a much larger number of concurrent connections by distributing
    them across multiple event loops. These tools are not mutually exclusive and can
    be powerfully combined.'
  prefs: []
  type: TYPE_NORMAL
- en: Stuff that people often get wrong
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's wrap up with a couple of classic brain-teasers that really test whether
    you've internalized how the loop works.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 `setTimeout(..., 0)` vs. `setImmediate()`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is a famous interview question: which of these runs first? The answer,
    maddeningly, is **it depends**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1: Called from the Main Module**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: When you run this directly in a script, the execution order is **non-deterministic**.
    You might get Timeout then Immediate, or the other way around. The reason is subtle.
    When the script is processed, the timer and immediate are scheduled. The event
    loop then starts up. The `setTimeout(..., 0)` doesn't really have a 0ms delay;
    it's constrained by a system minimum, often around 1ms. When the loop enters the
    timers phase, it checks if that 1ms has elapsed. If the initial startup of the
    loop took more than 1ms (which is totally possible on a busy system), the timer
    will fire first. If startup was super fast, the loop will fly past the (still
    empty) timers phase, hit the poll phase, and then the check phase, running the
    `setImmediate` first. It's a race condition.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 2: Called from within an I/O Callback**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, the order is **always, 100% deterministic. `setImmediate` will execute
    first.** Why the certainty? The I/O callback itself runs in the poll phase. When
    it schedules the timer and the immediate, the loop is *currently in the poll phase*.
    What's the very next phase? The check phase. So the `setImmediate` callback is
    guaranteed to run. The timer callback has to wait until the loop completes its
    full cycle and comes back around to the timers phase on the *next tick*.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage Collection and its Impact on the Loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There''s one last, invisible source of blocking we need to talk about: V8''s
    garbage collector (GC). To clean up memory from objects you''re no longer using,
    the GC has to periodically pause the execution of your JavaScript. This is often
    called a "stop-the-world" event, and it''s as dramatic as it sounds.'
  prefs: []
  type: TYPE_NORMAL
- en: While V8's GC is a marvel of engineering, a major GC cycle in an app with high
    memory pressure can still freeze your event loop for tens or even hundreds of
    milliseconds. During that pause, nothing happens. No JavaScript runs. Your server
    is just as unresponsive as if it were blocked by synchronous code. This is why
    good memory management - like using streams instead of buffering huge files -
    is so critical in Node. It keeps those GC pauses short and sweet.
  prefs: []
  type: TYPE_NORMAL
- en: Final words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whew. We've gone from the simple call stack to the dance between V8 and Libuv,
    through the loop's six phases, and into the VIP status of microtasks.
  prefs: []
  type: TYPE_NORMAL
- en: Mastering the event loop isn't about memorizing the names of the six phases
    for a trivia night. It's about building a solid, reliable mental model that lets
    you reason about how your code will actually behave. This model is a superpower.
    It lets you write screamingly fast, non-blocking apps. It helps you diagnose the
    trickiest performance bugs. And it gives you a true appreciation for what makes
    Node.js such a powerful and unique environment.
  prefs: []
  type: TYPE_NORMAL
- en: With this model in your head, you're not just *using* Node.js anymore. You're
    *thinking* in it. Now go write some code, `console.log` everything, and see if
    you can predict the outcome. That‚Äôs how this really sinks in. Happy coding.
  prefs: []
  type: TYPE_NORMAL
