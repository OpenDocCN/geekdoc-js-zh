- en: Modern Async Pipelines & Error Handling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代异步管道与错误处理
- en: 原文：[https://www.thenodebook.com/streams/modern-pipelines-error-handling](https://www.thenodebook.com/streams/modern-pipelines-error-handling)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[原文链接](https://www.thenodebook.com/streams/modern-pipelines-error-handling)'
- en: You know how individual streams work now - `Readable` producing data, `Writable`
    consuming it, `Transform` processing it in between. Each stream type has its own
    buffering, its own backpressure mechanism, its own event lifecycle. In real applications,
    you rarely work with streams in isolation. You're connecting them together, creating
    **pipelines** where data flows from source through multiple transformation stages
    to a final destination.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经了解了单个流的工作方式——`Readable`产生数据，`Writable`消费它，`Transform`在中间处理它。每种流类型都有自己的缓冲、自己的背压机制、自己的事件生命周期。在实际应用中，你很少单独处理流。你将它们连接起来，创建**管道**，其中数据从源通过多个转换阶段流向最终目的地。
- en: And this is where things get tricky. The concept's fine - piping data from one
    stream to another is straightforward enough - but actually doing it correctly?
    That's where the headaches start. Because when you connect streams, you're dealing
    with **multiple error sources**, **multiple backpressure signals**, and **multiple
    resource cleanup scenarios**. If any stream in your pipeline fails, what happens
    to the others? If backpressure occurs midway through, does it propagate correctly?
    When the pipeline completes, are all resources cleaned up properly?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是事情变得复杂的地方。概念本身是好的——从一条流到另一条流的管道操作足够直接——但实际正确地执行它？这就是头疼开始的地方。因为当你连接流时，你正在处理**多个错误源**、**多个背压信号**和**多个资源清理场景**。如果你的管道中的任何流失败，其他流会发生什么？如果中途发生背压，它是否正确传播？当管道完成时，所有资源是否都得到了适当的清理？
- en: This chapter is about answering those questions. We're going to examine the
    original `pipe()` method and understand both why it exists and why it's insufficient
    for production code. Then we'll dive deep into `stream.pipeline()`, which is the
    modern, recommended approach for composing stream pipelines with proper error
    handling and cleanup. After that, we'll explore error handling patterns specific
    to streaming - because errors in pipelines behave differently from errors in synchronous
    code. We'll look at using async iteration as an alternative pipeline approach,
    and finally we'll cover advanced composition patterns for building reusable, flexible
    pipeline segments.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在回答这些问题。我们将检查原始的`pipe()`方法，了解它存在的原因以及为什么它对于生产代码来说不足够。然后我们将深入研究`stream.pipeline()`，这是现代、推荐的用于组合流管道并具有适当错误处理和清理的方法。之后，我们将探讨特定于流式的错误处理模式——因为管道中的错误与同步代码中的错误行为不同。我们将探讨使用异步迭代作为替代管道方法，最后我们将介绍用于构建可重用、灵活管道段的先进组合模式。
- en: This chapter covers how to connect streams correctly - with proper error propagation,
    resource cleanup, and backpressure handling across all stages.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了如何正确连接流——具有适当的错误传播、资源清理和跨所有阶段的背压处理。
- en: The pipe() Method
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: pipe() 方法
- en: 'You''ve already learned how `pipe()` works from the Readable and Writable chapters.
    As a quick recap: `pipe()` connects a Readable stream to a Writable, automatically
    handling backpressure by calling `pause()` when `write()` returns `false` and
    `resume()` when the Writable emits `drain`. We covered this pattern extensively
    in the Writable Streams chapter when discussing backpressure.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经从可读和可写章节中学习了`pipe()`的工作方式。作为一个快速回顾：`pipe()`将可读流连接到可写流，通过在`write()`返回`false`时调用`pause()`以及在可写流发出`drain`时调用`resume()`来自动处理背压。我们在可写流章节讨论背压时广泛介绍了这种模式。
- en: 'The method returns the destination stream, allowing you to chain pipes:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法返回目标流，允许你链式调用管道：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This creates a four-stage pipeline: readable -> transform1 -> transform2 ->
    writable. Data flows through each stage sequentially, with **backpressure propagating
    backward** from the writable destination all the way to the readable source—a
    pattern we explored in detail in the Writable Streams chapter. If `writable` signals
    backpressure, the entire chain pauses; when `writable` emits `drain`, the resume
    signal propagates forward.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个四阶段的管道：可读 -> 转换1 -> 转换2 -> 可写。数据按顺序通过每个阶段流动，**背压从可写目标反向传播到可读源**——这是我们详细探讨的可写流章节中的模式。如果`writable`发出背压信号，整个链路将暂停；当`writable`发出`drain`时，恢复信号将向前传播。
- en: 'A concrete example—compressing a log file:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具体的例子——压缩日志文件：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Three streams, two `pipe()` calls. The file reader produces chunks, the gzip
    transform compresses them, and the file writer saves the compressed data. Memory
    usage stays bounded because each stage respects backpressure signals.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 三个流，两个 `pipe()` 调用。文件读取器产生数据块，gzip 转换器压缩它们，文件写入器保存压缩数据。内存使用保持有界，因为每个阶段都尊重背压信号。
- en: 'But `pipe()` has a problem: **error handling**.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但 `pipe()` 有一个问题：**错误处理**。
- en: Here's what happens when an error occurs in a piped stream. The stream that
    encounters the error emits an `error` event. But that error **does not propagate**
    to other streams in the pipeline. You already know from the Readable and Writable
    chapters that each stream needs its own error handler, or the error will crash
    your process. But in pipelines, this becomes especially painful.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是管道中发生错误时的情况。遇到错误的流会发出一个 `error` 事件。但这个错误**不会传播**到管道中的其他流。您已经从可读和可写章节中了解到，每个流都需要自己的错误处理器，否则错误会崩溃您的进程。但在管道中，这变得尤为痛苦。
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You need three separate error handlers. Miss one and your process crashes. It's
    tedious, error-prone, and honestly kind of ridiculous for something that should
    be straightforward.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要三个单独的错误处理器。错过任何一个，您的进程就会崩溃。这很繁琐，容易出错，坦白说，对于应该简单直接的事情来说，这有点荒谬。
- en: But it gets worse. When an error occurs in the middle of a pipeline, the other
    streams **don't automatically stop**. Suppose the transform throws an error while
    processing a chunk. The transform emits `error` and stops processing. But the
    reader keeps reading and trying to write to the transform, which is now in an
    errored state. The writer is waiting for data that will never come, and it might
    never emit `finish` because the pipeline never completes cleanly.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但问题更加严重。当管道中间发生错误时，其他流**不会自动停止**。假设在处理数据块时转换器抛出错误。转换器发出 `error` 事件并停止处理。但读取器继续读取并尝试写入转换器，此时转换器处于错误状态。写入器正在等待永远不会到来的数据，并且可能永远不会发出
    `finish` 事件，因为管道从未干净地完成。
- en: 'You end up with **dangling resources**. File handles that aren''t closed. Network
    connections that aren''t cleaned up. Memory buffers that aren''t released. The
    pipeline is in a partially-failed state, and cleaning it up requires manually
    calling `destroy()` on each stream:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您最终会得到**悬空资源**。未关闭的文件句柄。未清理的网络连接。未释放的内存缓冲区。管道处于部分失败状态，清理它需要手动对每个流调用 `destroy()`：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is verbose, repetitive, and fragile. If you add a new stream to the pipeline,
    you have to update all the error handlers to destroy the new stream too.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这很冗长，重复，且脆弱。如果您将新的流添加到管道中，您必须更新所有错误处理器以销毁新的流。
- en: 'There''s another limitation of `pipe()`: you can''t easily tell when the entire
    pipeline has completed. The readable emits `end` when it''s done reading. The
    writable emits `finish` when it''s done writing. But which one do you listen to?
    And what if you have multiple transforms in the middle? Each transform emits its
    own `end` event. The final destination emits `finish`. You have to track the **right
    event on the right stream**, which depends on the pipeline''s structure.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipe()` 的另一个限制是：您无法轻松地知道整个管道何时完成。可读流在完成读取时发出 `end` 事件。可写流在完成写入时发出 `finish`
    事件。但您应该听哪一个？如果您中间有多个转换器呢？每个转换器都会发出自己的 `end` 事件。最终目标发出 `finish` 事件。您必须跟踪**正确的流上的正确事件**，这取决于管道的结构。'
- en: For simple two-stream scenarios, `pipe()` works fine. But for real production
    pipelines with multiple stages and proper error handling requirements, `pipe()`
    is insufficient. This is why `stream.pipeline()` was introduced.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的双流场景，`pipe()` 方法可以很好地工作。但对于具有多个阶段和适当错误处理要求的实际生产级管道，`pipe()` 方法就不够用了。这就是为什么引入了
    `stream.pipeline()` 的原因。
- en: The unpipe() Method
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`unpipe()` 方法'
- en: 'Worth covering `unpipe()` before pipeline() - though you''ll rarely use it.
    This method disconnects a piped stream:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍 `pipeline()` 之前，值得介绍一下 `unpipe()` - 虽然您很少会用到它。此方法断开一个管道流：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When you call `unpipe()`, the readable stops sending data to the specified
    writable. If you call `unpipe()` without arguments, it disconnects from all destinations:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当您调用 `unpipe()` 时，可读流停止向指定的可写流发送数据。如果您不带参数调用 `unpipe()`，它将断开与所有目标流的关系：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Why would you use this? Mainly for dynamic routing scenarios where you want
    to redirect a stream''s output based on runtime conditions. For example, you might
    be reading from a socket and initially piping to a file, but then decide to pipe
    to a different destination based on incoming data:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你为什么要使用这个？主要是在需要根据运行时条件重定向流输出的动态路由场景中。例如，你可能正在从套接字读取，最初将数据管道传输到文件，但后来决定根据传入的数据将数据管道传输到不同的目的地：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: But in practice? I've almost never needed `unpipe()`. Most pipelines are static
    - you define the flow at setup time and let it run to completion. Dynamic routing
    is better handled with higher-level abstractions, like routing streams or conditional
    transforms.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但在实践中？我几乎从未需要`unpipe()`。大多数管道都是静态的 - 你在设置时定义流程，然后让它运行到完成。动态路由更适合用高级抽象来处理，比如路由流或条件转换。
- en: The main thing to know about `unpipe()` is that it exists, and that when you
    unpipe a stream, the destination does not automatically end. When `unpipe()` is
    called, it removes the destination's listeners from the source stream. The source
    stream's flowing mode state depends on whether any consumers remain attached -
    if all pipe destinations are removed and there are no `data` event listeners,
    the stream switches back to paused mode. If other consumers exist (other piped
    destinations or data listeners), the source continues emitting `data` events.
    If you want the destination to close, you need to call `end()` on it manually.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`unpipe()`的主要事情是要知道它存在，并且当你取消管道一个流时，目标不会自动结束。当`unpipe()`被调用时，它会从源流中移除目标流的监听器。源流的状态取决于是否有任何消费者保持连接
    - 如果所有管道目标都被移除且没有`data`事件监听器，流会切换回暂停模式。如果有其他消费者存在（其他管道目标或数据监听器），源流会继续发出`data`事件。如果你想关闭目标，你需要手动调用它的`end()`。
- en: stream.pipeline()
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: stream.pipeline()
- en: '`stream.pipeline()` is the modern approach to composing streams. This function
    was added to Node.js specifically to address the error handling and cleanup problems
    with `pipe()`. Here''s the basic usage:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`stream.pipeline()`是组合流的现代方法。这个函数被添加到Node.js中，专门用来解决`pipe()`的错误处理和清理问题。以下是基本用法：'
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Instead of chaining `pipe()` calls, you pass all your streams as arguments
    to `pipeline()`, followed by a callback that''s invoked when the pipeline completes
    or errors. That''s the signature:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是链式调用`pipe()`，你将所有流作为参数传递给`pipeline()`，然后是一个在管道完成或出错时被调用的回调函数。这是签名：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`pipeline()` does three things that `pipe()` doesn''t:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipeline()`做了`pipe()`没有做的三件事：'
- en: '**Automatic error propagation**: If any stream in the pipeline emits an error,
    `pipeline()` stops the pipeline and invokes the callback with that error. You
    don''t need separate error handlers on each stream.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自动错误传播**：如果管道中的任何流发出错误，`pipeline()`会停止管道并调用回调函数传递该错误。你不需要在每个流上单独的错误处理器。'
- en: '**Automatic cleanup**: When an error occurs (or when the pipeline completes),
    `pipeline()` calls `destroy()` on all streams in the pipeline. File handles get
    closed, buffers freed, connections torn down.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自动清理**：当发生错误（或当管道完成时），`pipeline()`会在管道中的所有流上调用`destroy()`。文件句柄被关闭，缓冲区被释放，连接被拆除。'
- en: '**Single completion callback**: One callback for everything.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**单个完成回调**：一个回调处理所有事情。'
- en: 'Let''s see what this looks like in practice:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在实践中是什么样子：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If any of these streams errors - the file read fails, the gzip encounters corrupt
    data, the file write hits a disk-full error - the callback is invoked with the
    error, and all three streams are destroyed. If everything succeeds, the callback
    is invoked with `err` as `undefined`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些流中的任何一个发生错误 - 文件读取失败，gzip遇到损坏的数据，文件写入遇到磁盘满错误 - 回调函数会带错误被调用，并且所有三个流都被销毁。如果一切顺利，回调函数会带`err`作为`undefined`被调用。
- en: This is simpler than the equivalent `pipe()` code with manual error handling.
    No separate error listeners, no manual destroy calls, no tracking which stream
    to listen to for completion.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这比等效的带有手动错误处理的`pipe()`代码要简单。没有单独的错误监听器，没有手动销毁调用，不需要跟踪哪个流要监听完成。
- en: 'There''s also a promise-based version of `pipeline()` in the `stream/promises`
    module:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在`stream/promises`模块中，也有`pipeline()`的基于Promise的版本：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The promise-based version returns a promise that resolves when the pipeline
    completes or rejects when any stream errors. This fits naturally with async/await
    code. You wrap the `pipeline()` call in a try/catch, and errors are handled like
    any other promise rejection.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Promise的版本在管道完成或任何流发生错误时返回一个解析的Promise。这自然地与async/await代码配合使用。你将`pipeline()`调用包裹在try/catch中，错误处理就像任何其他Promise拒绝一样。
- en: This is the **recommended pattern** for modern Node.js code. Use `stream/promises`
    and `async/await` for clean, readable pipeline composition.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是现代Node.js代码的**推荐模式**。使用`stream/promises`和`async/await`进行干净、可读的管道组合。
- en: How pipeline() Works Internally
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`pipeline()`的内部工作原理'
- en: 'Understanding the internals helps you reason about behavior and debug issues.
    When you call `pipeline(s1, s2, s3, callback)`, the function essentially:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 理解内部机制有助于您推理行为并调试问题。当您调用`pipeline(s1, s2, s3, callback)`时，该函数本质上：
- en: Connects streams using the same `pipe()` mechanics you learned in earlier chapters—the
    same automatic backpressure handling we covered in the Writable Streams chapter
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您在早期章节中学到的相同`pipe()`机制连接流——与我们在可写流章节中涵盖的相同自动背压处理。
- en: Attaches error listeners to all streams for coordinated error handling
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为所有流附加错误监听器以进行协调的错误处理
- en: Calls `destroy()` on all streams when any error occurs or when completion happens
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当发生任何错误或完成时，对所有流调用`destroy()`。
- en: Invokes the callback once with either an error or undefined
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一次调用回调，传递错误或未定义。
- en: The key difference from manual `pipe()` is the **error coordination and automatic
    cleanup**. You get the same backpressure handling (which we covered extensively
    in the Writable Streams chapter), but with production-grade error management.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动`pipe()`的主要区别是**错误协调和自动清理**。您获得相同的背压处理（我们在可写流章节中广泛讨论过），但带有生产级的错误管理。
- en: 'Here''s a highly simplified conceptual model showing the basic behavior. **Important**:
    This is a pedagogical simplification to illustrate the concept, not the actual
    implementation. The real Node.js `pipeline()` implementation (based on the `pump`
    library) is significantly more sophisticated and handles many edge cases not shown
    here - including async iterables, generators, complex error scenarios, once-only
    callback guarantees, and proper stream type detection:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个高度简化的概念模型，展示了基本行为。**重要**：这是一个教学简化，用于说明概念，而不是实际的实现。实际的Node.js `pipeline()`实现（基于`pump`库）要复杂得多，并处理了许多这里没有显示的边缘情况——包括异步迭代器、生成器、复杂错误场景、一次性的回调保证和适当的流类型检测：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is a conceptual model to help you understand the behavior - the real `pipeline()`
    implementation in Node.js is more sophisticated with its own stream connection
    logic and comprehensive edge case handling (once-only callback invocation, destroyed
    stream handling, complex error scenarios, etc.). Think of this as "what it does"
    rather than "how it does it."
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个概念模型，旨在帮助您理解行为——Node.js中实际的`pipeline()`实现要复杂得多，它有自己的流连接逻辑和全面的边缘情况处理（一次性的回调调用、销毁流处理、复杂错误场景等）。将其视为“它做什么”而不是“它是如何做的”。
- en: '`pipeline()` handles an edge case you should know about: a stream emitting
    an error after it''s already been destroyed. This can happen with async operations
    in custom streams where an error occurs after the stream has nominally completed.
    The real `pipeline()` implementation ensures the callback is only invoked once,
    even if multiple streams error simultaneously.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipeline()`处理您应该知道的边缘情况：一个在已销毁后发出错误的流。这可能在自定义流中的异步操作中发生，其中错误发生在流名义上完成之后。实际的`pipeline()`实现确保即使多个流同时出错，回调也只被调用一次。'
- en: Using pipeline() with Transform Functions
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`pipeline()`与转换函数
- en: You don't even need `Transform` stream instances - you can pass **async generator
    functions**, and `pipeline()` will treat them as transforms.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您甚至不需要`Transform`流实例——您可以传递**异步生成器函数**，`pipeline()`会将它们视为转换。
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The async generator in the middle is automatically converted to a `Transform`
    stream. For each chunk from the source, the generator transforms it (in this case,
    uppercases it) and yields the result. The yielded values become chunks in the
    output stream.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 中间的异步生成器自动转换为`Transform`流。对于来自源的数据块，生成器将其转换（在这种情况下，转换为大写）并产生结果。产生的值成为输出流中的块。
- en: 'This works well for simple transformations. Instead of creating a custom `Transform`
    class, you write a generator function inline. It reads like a loop: **for each
    input chunk, produce an output chunk.**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于简单的转换效果很好。您不需要创建自定义的`Transform`类，而是可以内联编写生成器函数。它看起来像是一个循环：**对于每个输入块，生成一个输出块**。
- en: 'You can also use regular async functions that return async iterables:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用返回异步迭代器的常规异步函数：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This works because `pipeline()` recognizes async iterables and automatically
    wraps them in Transform streams internally.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这之所以有效，是因为`pipeline()`识别异步迭代器，并在内部自动将它们包装在转换流中。
- en: 'You can even chain multiple generator transforms:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您甚至可以链式多个生成器转换：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The first generator converts buffers to strings and splits them into lines,
    handling chunk boundaries with a buffer. The second filters out lines starting
    with "#". Each generator is a pipeline stage, and `pipeline()` handles the plumbing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个生成器将缓冲区转换为字符串并将它们分割成行，使用缓冲区处理块边界。第二个生成器过滤掉以“#”开头的行。每个生成器都是一个管道阶段，`pipeline()`负责管道连接。
- en: This is the recommended way to build stream pipelines in modern Node.js. For
    simple transformations, use inline generators. For complex stateful transforms,
    create a `Transform` class. Mix and match as needed.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是现代Node.js中构建流管道的推荐方式。对于简单的转换，使用内联生成器。对于复杂的具有状态的转换，创建一个`Transform`类。根据需要混合匹配。
- en: '**Important note about generator functions**: Only values produced with `yield`
    are sent to the output stream. If a generator uses `return` to produce a final
    value, that value is **not** yielded to the pipeline - it''s only accessible to
    code directly consuming the generator. In pipeline transforms, always use `yield`
    for output chunks.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于生成器函数的重要说明**：只有使用`yield`产生的值才会发送到输出流。如果生成器使用`return`来产生最终值，该值**不会**被发送到管道——它只能被直接消耗生成器的代码访问。在管道转换中，始终使用`yield`来输出数据块。'
- en: 'Quick aside: if you''re coming from Python, this is like itertools but actually
    built into the language. End digression.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 快速旁白：如果您来自Python，这就像itertools，但实际上是内置在语言中的。结束旁白。
- en: Error Handling in Stream Pipelines
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流管道中的错误处理
- en: Error handling gets tricky with pipelines because they introduce error scenarios
    that don't exist in single-stream code. You already know from the Readable and
    Writable chapters that streams emit `error` events for various failures (file
    not found, disk full, network dropped, etc.).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道中处理错误变得复杂，因为它们引入了单流代码中不存在的错误场景。您已经从可读和可写章节中了解到，流为各种失败情况发出`error`事件（文件未找到、磁盘已满、网络断开等）。
- en: 'In pipelines, these errors can come from **multiple sources simultaneously**:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道中，这些错误可能来自**多个来源同时**：
- en: The **source stream** might fail to read (file doesn't exist, permission denied,
    network dropped)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源流**可能无法读取（文件不存在、权限被拒绝、网络断开）'
- en: A **transform stream** might encounter invalid data (parse error, validation
    failure)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换流**可能遇到无效数据（解析错误、验证失败）'
- en: The **destination stream** might fail to write (disk full, broken pipe, remote
    endpoint closed)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标流**可能无法写入（磁盘已满、管道损坏、远程端点关闭）'
- en: Each of these errors manifests as an `error` event on the stream that encountered
    it. With `pipe()`, you'd have to handle each separately. With `pipeline()`, all
    errors are caught and passed to your callback or promise rejection.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些错误中的每一个都会在遇到它的流上表现为一个`error`事件。使用`pipe()`，您必须单独处理每个错误。使用`pipeline()`，所有错误都会被捕获并传递到您的回调或承诺拒绝。
- en: When an error occurs midway through a pipeline, what happens to partial data?
    Suppose you're reading 100MB file, transforming it, and writing the result. At
    50MB, the transform encounters corrupt data and errors. What happened to the first
    50MB?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当管道中途发生错误时，部分数据会怎样？假设您正在读取一个100MB的文件，对其进行转换，并写入结果。在50MB时，转换遇到损坏的数据和错误。前50MB发生了什么？
- en: The answer depends on the destination stream's behavior. If it's writing to
    a file, the file now contains 50MB of **partial output**. The file exists, but
    it's incomplete and possibly invalid. `pipeline()` **doesn't roll back** partial
    writes - it can't. The data is already written to the underlying resource.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 答案取决于目标流的行为。如果它正在写入文件，则文件现在包含50MB的**部分输出**。文件存在，但不完整且可能无效。`pipeline()`**不会回滚**部分写入——它不能。数据已经写入底层资源。
- en: 'This means you need to handle partial data in your application logic. One pattern
    is to write to a temporary file and rename it to the final name only on success:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着您需要在应用程序逻辑中处理部分数据。一种模式是写入一个临时文件，只有在成功后将其重命名为最终名称：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If the pipeline succeeds, the temp file is renamed to the final name. If it
    fails, the temp file is deleted. This ensures that either the complete output
    exists or nothing exists - no partial files.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果管道成功，临时文件将被重命名为最终名称。如果失败，临时文件将被删除。这确保了要么完整的输出存在，要么什么都不存在——没有部分文件。
- en: 'Another pattern is to use transactions when writing to a database. Write all
    rows within a transaction, and commit only if the pipeline completes successfully:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种模式是在向数据库写入时使用事务。在一个事务中写入所有行，只有当管道成功完成时才提交：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`pipeline()` only handles **stream-level cleanup** - calling `destroy()` on
    streams. It doesn''t handle **domain-level cleanup** (deleting partial files,
    rolling back transactions). **That''s your responsibility.**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipeline()`只处理**流级别清理**——在流上调用`destroy()`。它不处理**域级别清理**（删除部分文件、回滚事务）。**这是你的责任**。'
- en: Error propagation is where this gets interesting. When a stream in a pipeline
    errors, `pipeline()` immediately calls `destroy()` on all other streams. This
    causes each stream to emit a `close` event, and any pending operations are canceled.
    This is correct - if one stage fails, the entire pipeline should stop.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 错误传播是这里有趣的地方。当管道中的流出错时，`pipeline()`会立即对其他所有流调用`destroy()`。这导致每个流发出`close`事件，并取消任何挂起的操作。这是正确的——如果一个阶段失败，整个管道应该停止。
- en: But what if you want to handle errors from different streams differently? For
    example, if the source fails, you want to log "read error," but if the destination
    fails, you want to log "write error." With `pipeline()`, you only get one error
    in the callback - the first error that occurred.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你想要对不同流中的错误进行不同的处理呢？例如，如果源失败，你想要记录“读取错误”，但如果目标失败，你想要记录“写入错误”。使用`pipeline()`，你只能在回调中获取一个错误——即第一个发生的错误。
- en: 'If you need to distinguish between error sources, you can check the error object''s
    properties or use error wrapping in your custom streams:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要区分错误来源，你可以检查错误对象的属性或在自定义流中使用错误包装：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: By tagging errors with a `code` property, you can distinguish them in the error
    handler.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`code`属性标记错误，你可以在错误处理程序中区分它们。
- en: 'Another pattern is using `stream.finished()` to detect when a specific stream
    completes or errors, even within a larger pipeline:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种模式是使用`stream.finished()`来检测特定流何时完成或出错，即使在更大的管道中：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `finished()` utility attaches listeners to a stream and invokes a callback
    when the stream ends, errors, or is destroyed. This lets you monitor individual
    streams within a pipeline.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`finished()`实用工具将监听器附加到流上，并在流结束时、出错或被销毁时调用回调。这让你可以监控管道中的单个流。'
- en: stream.finished()
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`stream.finished()`'
- en: '`stream.finished()` deserves a closer look.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`stream.finished()`值得仔细研究。'
- en: 'The `finished()` function takes a stream and a callback, and invokes the callback
    when the stream completes (either successfully or with an error):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`finished()`函数接受一个流和一个回调，并在流完成时（无论是成功还是出错）调用回调：'
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: What does "finished" mean? For a `Readable`, it means the stream has ended (pushed
    `null` or destroyed). For a `Writable`, it means the stream has finished writing
    and emitted `finish` (or been destroyed). For a `Duplex` or `Transform`, it means
    both sides have completed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: “完成”是什么意思？对于`Readable`，这意味着流已结束（推送了`null`或被销毁）。对于`Writable`，这意味着流已完成写入并发出`finish`（或被销毁）。对于`Duplex`或`Transform`，这意味着两边都已完成。
- en: This is safer than attaching listeners to `end` or `finish` directly, because
    `finished()` also listens for `error`, `close`, and `destroy` events, and handles
    the complex logic of determining whether the stream truly completed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这比直接将监听器附加到`end`或`finish`更安全，因为`finished()`还监听`error`、`close`和`destroy`事件，并处理确定流是否真正完成的复杂逻辑。
- en: 'There''s also a promise-based version:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个基于Promise的版本：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The promise resolves when the stream completes successfully or rejects if the
    stream errors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当流成功完成时，Promise会解决；如果流出错，则会拒绝。
- en: '**Note on event listeners**: `stream.finished()` intentionally leaves dangling
    event listeners (particularly `error`, `end`, `finish`, and `close`) after the
    callback is invoked or the promise settles. This design choice allows `finished()`
    to catch unexpected error events from incorrect stream implementations, preventing
    crashes. For most use cases with short-lived streams, this is not a concern as
    the streams will be garbage collected. However, for memory-sensitive applications
    or long-lived streams, you can use the `cleanup` option to remove these listeners
    automatically:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于事件监听器的说明**：`stream.finished()`故意在回调调用或Promise解决后留下悬空的事件监听器（尤其是`error`、`end`、`finish`和`close`）。这种设计选择允许`finished()`捕获来自不正确流实现的意外错误事件，防止崩溃。对于大多数具有短暂生命周期的流的用例，这不是一个问题，因为流将被垃圾回收。然而，对于对内存敏感的应用程序或长期存在的流，你可以使用`cleanup`选项自动删除这些监听器：'
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Why use `finished()` instead of just listening for `end` or `finish`? Because
    streams can end in multiple ways. A stream might emit `end` naturally, or it might
    be destroyed due to an error, or it might be destroyed explicitly via `destroy()`.
    The `finished()` utility handles all these cases and gives you a single callback
    or promise that represents "this stream is done, one way or another."
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用 `finished()` 而不是仅仅监听 `end` 或 `finish`？因为流可以通过多种方式结束。流可能会自然地发出 `end`，或者由于错误而被销毁，或者通过
    `destroy()` 显式销毁。`finished()` 工具处理所有这些情况，并给你一个表示“这个流无论如何都完成了”的单个回调或承诺。
- en: 'This is useful when you need to know when a specific stream in a complex pipeline
    has completed, even if the overall pipeline is still running. For example, if
    you''re piping a source to multiple destinations (a tee or broadcast pattern),
    you can use `finished()` to know when each destination has consumed all its data:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这在需要知道一个复杂管道中的特定流何时完成时很有用，即使整体管道仍在运行。例如，如果你将源流传输到多个目的地（如 tee 或广播模式），你可以使用 `finished()`
    来知道每个目的地是否已消耗完所有数据：
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This waits for both destinations to finish writing before proceeding.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这在继续之前等待两个目标都完成写入。
- en: Error Recovery in Pipelines
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道中的错误恢复
- en: Not all errors are fatal - some deserve a retry. Some errors are **transient**
    and can be retried. Others indicate a fundamental problem and require the pipeline
    to fail.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有错误都是致命的——一些值得重试。一些错误是**瞬态的**，可以重试。其他错误则表明存在根本问题，需要管道失败。
- en: For example, if you're reading from a network source and the connection drops,
    that might be transient. Retrying the connection could succeed. But if you're
    reading a file and get `EACCES` (permission denied), retrying won't help - the
    file's permissions won't magically change.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你从网络源读取数据并且连接断开，这可能是瞬时的。重试连接可能会成功。但是，如果你读取文件并得到 `EACCES`（权限被拒绝），重试是没有帮助的——文件的权限不会神奇地改变。
- en: The first step is **categorizing errors**. Is this an *operational error* (expected
    failure condition) or a *programmer error* (bug in the code)? Is it transient
    or permanent?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是**分类错误**。这是操作错误（预期的失败条件）还是程序员错误（代码中的错误）？它是瞬态的还是永久的？
- en: 'For transient errors, you can implement retry logic around the pipeline:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于瞬态错误，你可以在管道周围实现重试逻辑：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This function attempts the pipeline up to `maxRetries` times. If a transient
    error occurs, it waits (with exponential backoff) and retries. If a non-transient
    error occurs, or if all retries are exhausted, it throws the error.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数尝试管道最多 `maxRetries` 次。如果发生瞬态错误，它将等待（使用指数退避）并重试。如果发生非瞬态错误，或者所有重试都耗尽，它将抛出错误。
- en: Note that the `source()` is a function that creates a new source stream. You
    can't reuse a stream after it's errored and been destroyed. Each retry needs fresh
    stream instances.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`source()` 是一个创建新源流的函数。一旦流出错并被销毁，就不能再重用该流。每次重试都需要新的流实例。
- en: 'Another recovery pattern is fallback. If one data source fails, try an alternative
    source:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种恢复模式是回退。如果一个数据源失败，尝试一个替代源：
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This is useful for redundant data sources, like trying a CDN first and falling
    back to an origin server if the CDN is unavailable.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于冗余数据源很有用，例如首先尝试 CDN，如果 CDN 不可用，则回退到原始服务器。
- en: 'For destination failures, you might want to retry writes to a different location:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于目标失败，你可能想要尝试将写入重试到不同的位置：
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Again, note that `source()` is a function creating a new source. After the first
    pipeline fails, the original source is destroyed, so you need a new one for the
    retry.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，`source()` 是一个创建新源的函数。在第一个管道失败后，原始源被销毁，因此你需要一个新的源来进行重试。
- en: 'The **key principle**: decide upfront which errors are recoverable and implement
    your retry or fallback logic **at the pipeline level**, not at the individual
    stream level. Streams don''t know about your application''s retry policy - you
    have to coordinate it externally.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键原则**：事先决定哪些错误是可恢复的，并在管道级别实现你的重试或回退逻辑，而不是在单个流级别。流不知道你的应用程序的重试策略——你必须在外部协调它。'
- en: Partial Data Concerns
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部分数据关注
- en: Partial data needs attention because it's easy to get wrong. When a pipeline
    fails midway through, any data already written to the destination remains there.
    The pipeline doesn't automatically clean it up.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 部分数据需要关注，因为它很容易出错。当管道在中间失败时，已经写入目标的数据将保留在那里。管道不会自动清理它。
- en: This is a problem for **data integrity**. If you're writing a database export
    and the pipeline fails at 60%, you have a 60%-complete file. If you later retry
    and succeed, you might end up with duplicate data (the first 60% twice) or you
    might overwrite the partial file with a complete one, depending on how you open
    the output file.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对**数据完整性**的问题。如果你正在写入数据库导出，并且管道在60%处失败，你将有一个60%完成的文件。如果你稍后重试并成功，你可能会得到重复的数据（前60%两次）或者你可能用完整的文件覆盖了部分文件，这取决于你如何打开输出文件。
- en: 'Here are **strategies for handling this**:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是**处理此问题的策略**：
- en: '**1\. Write to a temporary location and atomic rename**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. 写入到临时位置并原子重命名**'
- en: '[PRE26]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This is the **safest pattern** for file outputs. The final file only exists
    if the pipeline completed successfully.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是文件输出的**最安全模式**。最终文件只有在管道成功完成后才存在。
- en: '**2\. Use append mode and idempotent operations**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 使用追加模式和幂等操作**'
- en: 'If your output supports appending (like log files), and your operations are
    idempotent (writing the same data twice is harmless), you can just retry from
    the beginning and append:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的输出支持追加（例如日志文件），并且你的操作是幂等的（写两次相同的数据无害），你可以从头开始重试并追加：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If the pipeline fails and you retry, the second attempt appends more data. If
    you have duplicate detection downstream, this is fine.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果管道失败并且你重试，第二次尝试会追加更多数据。如果你在下游有重复检测，这没问题。
- en: '**3\. Use transactional destinations**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. 使用事务性目标**'
- en: 'Databases, message queues, and some cloud storage systems support transactions.
    Write within a transaction and commit only on success:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库、消息队列和一些云存储系统支持事务。在事务中写入，仅在成功时提交：
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The destination doesn't persist any data until the pipeline succeeds.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 目标不会在管道成功之前持久化任何数据。
- en: '**4\. Write a completion marker**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**4. 写入完成标记**'
- en: 'For scenarios where you can''t use transactions, write a marker file indicating
    successful completion:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无法使用事务的场景，写入一个标记文件以指示成功完成：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Before processing the output file, check for the marker. If the marker doesn't
    exist, the file is incomplete and should be discarded or retried.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理输出文件之前，检查标记。如果标记不存在，文件不完整，应丢弃或重试。
- en: The pattern you choose depends on your destination's capabilities and your consistency
    requirements. The key is to be explicit about what happens on partial failure
    and design your pipeline to handle it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择的模式取决于你的目标的能力和你的一致性要求。关键是明确部分失败时会发生什么，并设计你的管道来处理它。
- en: Destroying Streams
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 销毁流
- en: You've already learned about `stream.destroy()` in the Readable and Writable
    chapters. As a reminder, calling `destroy()` on any stream transitions it to a
    destroyed state, emits `close`, and optionally emits `error` if you pass one.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在可读和可写章节中学习了`stream.destroy()`。作为提醒，对任何流调用`destroy()`会将它转换为销毁状态，发出`close`事件，并且如果传递了一个错误，则可选地发出`error`。
- en: 'What''s specific to pipelines is that when you destroy **any** stream in a
    `pipeline()`, the pipeline function automatically destroys all other streams and
    invokes the callback with the error:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 管道特有的特定之处在于，当你销毁`pipeline()`中的**任何**流时，管道函数会自动销毁所有其他流，并带错误调用回调：
- en: '[PRE30]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: When you call `source.destroy()`, the source stream stops reading, emits `close`,
    and if you passed an error, emits `error`. The `pipeline()` function sees the
    error and destroys all other streams in the pipeline (in this case, `dest`). The
    callback is invoked with the error.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用`source.destroy()`时，源流停止读取，发出`close`事件，如果你传递了一个错误，则发出`error`事件。`pipeline()`函数看到错误并销毁管道中的所有其他流（在这种情况下，`dest`）。回调会带错误被调用。
- en: This automatic cleanup is another advantage of `pipeline()` over manual `pipe()`
    chaining. With `pipe()`, you'd have to track all streams and destroy them manually.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自动清理是`pipeline()`相对于手动`pipe()`链的另一个优点。使用`pipe()`，你必须跟踪所有流并手动销毁它们。
- en: This is useful for implementing cancellation. If a user cancels an operation,
    you destroy the pipeline's source stream. The pipeline stops gracefully, cleans
    up resources, and your callback is invoked to handle the cancellation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于实现取消很有用。如果用户取消操作，你销毁管道的源流。管道优雅地停止，清理资源，并且你的回调被调用以处理取消。
- en: 'You can also destroy with no error:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以不带错误地销毁：
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In this case, the stream is destroyed but no `error` event is emitted. The `pipeline()`
    callback is still invoked, but `err` is `null`. This is useful for stopping a
    pipeline without treating it as a failure.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，流被销毁了，但没有发出`error`事件。`pipeline()`回调仍然被调用，但`err`是`null`。这对于在不将其视为失败的情况下停止管道很有用。
- en: 'One important detail: `destroy()` is idempotent. Calling it multiple times
    on the same stream does nothing after the first call. The stream is destroyed
    once, and subsequent destroy calls are ignored.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的细节是：`destroy()` 是幂等的。在同一个流上多次调用它，在第一次调用之后不会有任何作用。流只销毁一次，后续的销毁调用会被忽略。
- en: Another thing - when you destroy a stream, buffered data is just... gone. If
    a `Writable` has buffered writes that haven't been flushed yet, they're lost.
    If a `Readable` has buffered data that hasn't been consumed yet, it's lost. Destroy
    means **"stop immediately and throw away any state,"** not "gracefully finish
    pending operations."
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件事——当你销毁一个流时，缓冲数据就消失了。如果一个 `Writable` 有尚未刷新的缓冲写入，它们就会丢失。如果一个 `Readable` 有尚未消费的缓冲数据，它也会丢失。销毁意味着
    **“立即停止并丢弃任何状态，”** 而不是“优雅地完成挂起的操作。”
- en: 'If you need graceful shutdown (finish writing buffered data before closing),
    use `end()` instead of `destroy()`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要优雅地关闭（在关闭前完成缓冲数据的写入），请使用 `end()` 而不是 `destroy()`：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: But `end()` only works on Writable streams. For Readable streams, there's no
    graceful stop - you either consume all data or you destroy.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 但 `end()` 只在可写流上有效。对于可读流，没有优雅的停止方式——你只能消耗所有数据或者销毁。
- en: Async Iteration Pipelines
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步迭代管道
- en: In the Readable Streams chapter, we explored using `for await...of` to consume
    streams with automatic backpressure handling. This pattern is also an alternative
    to `pipe()` and `pipeline()` for building stream processing logic.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在可读流章节中，我们探讨了使用 `for await...of` 来消费具有自动背压处理的流。这种模式也是构建流处理逻辑的 `pipe()` 和 `pipeline()`
    的替代方案。
- en: 'As a refresher: when you iterate over a Readable stream with `for await...of`,
    the iterator protocol automatically implements backpressure. The loop doesn''t
    pull the next chunk until the current iteration completes. If your processing
    is async, the stream waits:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 作为复习：当你使用 `for await...of` 迭代可读流时，迭代协议会自动实现背压。循环不会拉取下一个块，直到当前迭代完成。如果你的处理是异步的，流会等待：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We won't repeat the full backpressure mechanics here—refer to the "Backpressure
    in Async Iteration" section in the Readable Streams chapter for the complete explanation
    of how the iterator protocol manages flow control.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会重复完整的背压机制——有关迭代协议如何管理流控制的完整解释，请参阅可读流章节中的“异步迭代中的背压”部分。
- en: 'What''s relevant to this chapter is using this pattern specifically for **pipeline
    construction** rather than `pipeline()`. You can build pipelines by reading from
    a source with `for await...of`, transforming each chunk, and writing to a destination:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本章相关的是特别使用这种模式进行 **管道构建** 而不是 `pipeline()`。你可以通过从源读取、转换每个块并写入目标来构建管道：
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This is a manual pipeline. You're pulling from the source, transforming, and
    writing to the destination, with explicit backpressure handling (wait for `drain`
    if `write()` returns false). This pattern gives you fine-grained control over
    the data flow, but it requires you to manage backpressure manually—forgetting
    the `drain` check leads to unbounded memory growth, as we discussed in the Writable
    Streams chapter.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个手动管道。你从源拉取数据，进行转换，并写入目标，具有显式的背压处理（如果 `write()` 返回 false，则等待 `drain`）。这种模式让你对数据流有精细的控制，但需要你手动管理背压——忘记
    `drain` 检查会导致内存无限制增长，正如我们在可写流章节中讨论的那样。
- en: 'A cleaner pattern is to use `stream.Readable.from()` with an async generator:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更干净的模式是使用 `stream.Readable.from()` 与异步生成器：
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The async generator is automatically wrapped in a Readable stream, and `pipeline()`
    handles the plumbing. This combines the clarity of `for await...of` with the robustness
    of `pipeline()`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 异步生成器会自动包装在一个可读流中，`pipeline()` 处理管道连接。这结合了 `for await...of` 的清晰性和 `pipeline()`
    的健壮性。
- en: 'You can chain multiple generator transforms:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以链式调用多个生成器转换：
- en: '[PRE36]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Each generator is a pipeline stage. This is readable. Each stage is a simple
    `for await` loop that yields transformed chunks. The composition is handled by
    `pipeline()`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 每个生成器是一个管道阶段。这是可读的。每个阶段是一个简单的 `for await` 循环，产生转换后的块。组合由 `pipeline()` 处理。
- en: 'This pattern is especially nice for **`objectMode` pipelines** where each chunk
    is a structured object:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式对于 **`objectMode` 管道**特别有用，其中每个块都是一个结构化对象：
- en: '[PRE37]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Each stage is a function from source to async iterable. The `pipeline()` function
    stitches them together. This is functional pipeline composition, and it's often
    clearer than creating Transform classes.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 每个阶段都是一个从源到异步可迭代对象的函数。`pipeline()` 函数将它们连接起来。这是函数式管道组合，通常比创建转换类更清晰。
- en: Backpressure with Async Iteration in Pipelines
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道中的异步迭代背压
- en: 'We covered how backpressure works with `for await...of` in the Readable Streams
    chapter. The key points to remember when using this pattern in pipelines:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在可读流章节中，我们介绍了 `for await...of` 如何与背压一起工作。在管道中使用此模式时需要记住的关键点：
- en: '**Await async work** - The iterator pulls one chunk at a time; if you await
    async operations, backpressure flows automatically from your processing speed
    back to the source'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**等待异步工作** - 迭代器一次拉取一个块；如果您等待异步操作，则从您的处理速度自动向源流动背压'
- en: '**Don''t accumulate promises** - Avoid `promises.push(processAsync(chunk))`
    patterns that break backpressure by reading the entire stream into memory before
    processing'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不要累积承诺** - 避免使用 `promises.push(processAsync(chunk))` 模式，该模式在处理之前将整个流读入内存，从而破坏背压'
- en: '**Use controlled concurrency** - For parallel processing with bounded concurrency,
    use libraries like `p-limit` to limit in-flight operations'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用受控并发** - 对于具有有限并发的并行处理，请使用 `p-limit` 等库来限制正在进行的操作'
- en: 'From the Readable Streams chapter: **you must await async operations** inside
    the loop. Without `await`, you lose backpressure:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从可读流章节：**您必须在循环内等待异步操作**。没有 `await`，您会失去背压：
- en: '[PRE38]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The advantage of this approach over `pipeline()` is explicit control over data
    flow. The disadvantage is that you must manage error handling yourself—there's
    no automatic cleanup like `pipeline()` provides.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `pipeline()` 相比，这种方法的优势在于对数据流的显式控制。缺点是您必须自己管理错误处理——没有 `pipeline()` 提供的自动清理。
- en: 'For the full mechanics of how the async iterator protocol implements backpressure,
    see the "Backpressure in Async Iteration: How It Works" section in the Readable
    Streams chapter.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 关于异步迭代协议如何实现背压的完整机制，请参阅可读流章节中的“异步迭代中的背压：它是如何工作的”部分。
- en: Composable Transforms
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可组合的转换
- en: In the Transform Streams chapter, we covered how to implement custom transforms.
    Now let's look at building **reusable pipeline components** through factory functions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换流章节中，我们介绍了如何实现自定义转换。现在让我们看看如何通过工厂函数构建**可重用管道组件**。
- en: 'The pattern is straightforward—create functions that return configured stream
    instances:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 模式很简单——创建返回配置流实例的函数：
- en: '[PRE39]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Each call to `createCSVParser()` returns a fresh Transform instance. You can't
    reuse a stream instance across multiple pipelines (once a stream ends or errors,
    it's done), but you can reuse the factory function.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 每次调用 `createCSVParser()` 都会返回一个新的转换实例。您不能在多个管道中重用流实例（一旦流结束或出错，它就完成了），但您可以重用工厂函数。
- en: 'You can make factories configurable:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使工厂可配置：
- en: '[PRE40]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now the transform is parameterized. Different pipelines can extract different
    fields.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转换是参数化的。不同的管道可以提取不同的字段。
- en: 'For more complex pipelines, you can compose pipeline segments:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的管道，您可以组合管道段：
- en: '[PRE41]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The `createProcessingPipeline()` function encapsulates the entire transformation
    sequence. You pass in a source and destination, and it wires up all the intermediate
    transforms.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`createProcessingPipeline()` 函数封装了整个转换序列。您传入源和目的地，然后它会连接所有中间转换。'
- en: 'This is a higher-order function pattern: functions that create and compose
    streams. It''s useful for building modular, testable streaming code.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个高阶函数模式：创建和组合流的函数。这对于构建模块化、可测试的流代码很有用。
- en: 'You can also compose generator functions:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以组合生成器函数：
- en: '[PRE42]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Each generator is a function from async iterable to async iterable. You compose
    them by passing one's output as another's input. The `pipeline()` function handles
    the plumbing.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 每个生成器都是一个从异步可迭代到异步可迭代的功能。您通过将一个函数的输出作为另一个函数的输入来组合它们。`pipeline()` 函数处理管道连接。
- en: This functional composition style fits naturally with Node.js's streaming model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这种函数组合风格与 Node.js 的流模型自然契合。
- en: Pipeline Segments
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道段
- en: Let's formalize the concept of a pipeline segment. A segment is a reusable piece
    of a pipeline - it might be a single transform, or a chain of transforms, or conditional
    logic that routes to different transforms.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们正式化管道段的概念。一个段是管道的可重用部分——它可能是一个转换，或一系列转换，或条件逻辑，将路由到不同的转换。
- en: 'Here''s a segment that validates objects and either passes them through or
    routes them to an error destination:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个验证对象并决定是将其传递通过还是将其路由到错误目的地的段：
- en: '[PRE43]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This transform validates each object. Valid objects are pushed downstream.
    Invalid objects are written to an error destination (like a log file or error
    stream). This is a branching segment: one input, two outputs.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换验证每个对象。有效的对象被推送到下游。无效的对象被写入错误目的地（如日志文件或错误流）。这是一个分支段：一个输入，两个输出。
- en: 'You use it like this:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样使用它：
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Valid objects go to `dest`. Invalid objects go to `errorLog`. The pipeline continues
    even when invalid objects are encountered - they're just routed elsewhere.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的对象发送到 `dest`。无效的对象发送到 `errorLog`。即使遇到无效对象，管道也会继续运行——它们只是被路由到其他地方。
- en: 'Another pattern is conditional segments that choose different transforms based
    on runtime state:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个模式是条件段，根据运行时状态选择不同的转换：
- en: '[PRE45]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Based on a condition function, each object is sent through either `trueTransform`
    or `falseTransform`. This is a routing segment.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 根据条件函数，每个对象要么通过 `trueTransform`，要么通过 `falseTransform`。这是一个路由段。
- en: These patterns - branching, routing, conditional - are building blocks for complex
    data flows. You compose them into larger pipelines, creating sophisticated processing
    logic while keeping each segment focused and reusable.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模式——分支、路由、条件——是复杂数据流的构建块。你将它们组合成更大的管道，创建复杂的处理逻辑，同时保持每个段集中和可重用。
- en: Tee and Broadcast Patterns
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tee 和广播模式
- en: Sometimes you need to send the same data to multiple destinations. This is called
    a tee (like a T-junction in plumbing) or broadcast pattern.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你需要将相同的数据发送到多个目的地。这被称为 tee（类似于管道中的 T 形连接）或广播模式。
- en: 'The simplest way to tee a stream is to pipe it to multiple destinations:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Tee 流的最简单方法是将其管道连接到多个目的地：
- en: '[PRE46]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Both `dest1` and `dest2` receive the same data from `source`. The source emits
    each chunk once, and both pipes forward it to their respective destinations.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`dest1` 和 `dest2` 都从 `source` 接收相同的数据。源每次只发射每个数据块一次，两个管道将其分别转发到各自的目的地。'
- en: 'But there''s a catch: backpressure. If `dest1` is slow and signals backpressure,
    the source pauses. But `dest2` might be fast and ready for more data. By pausing
    the source, you''re slowing down `dest2` unnecessarily.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一个问题：背压。如果 `dest1` 慢并且发出背压信号，则源暂停。但 `dest2` 可能很快，准备好接收更多数据。通过暂停源，你无必要地减慢了
    `dest2` 的速度。
- en: The source can't pause for one destination and continue for another. It's **all
    or nothing**. So when you tee a stream with `pipe()`, the **slowest destination
    controls the pace** for all destinations.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 源不能只为一个目的地暂停，而继续为另一个目的地服务。它是 **全有或全无** 的。所以当你使用 `pipe()` 将流 tee 时，**最慢的目的地控制所有目的地的节奏**。
- en: If this is acceptable (all destinations need the same data, and you're okay
    with the slowest one setting the pace), then simple piping works fine. But if
    you want independent backpressure per destination, you need a more sophisticated
    approach.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是可以接受的（所有目的地都需要相同的数据，并且你接受最慢的一个设置节奏），那么简单的管道就足够了。但如果你想要每个目的地都有独立的背压，你需要更复杂的方法。
- en: 'One technique is to use `PassThrough` streams as intermediaries:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一种技术是使用 `PassThrough` 流作为中间代理：
- en: '[PRE47]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now `dest1` and `dest2` have independent backpressure. If `dest1` is slow, `pass1`
    buffers. If `dest2` is fast, `pass2` doesn't buffer. The source isn't paused by
    either destination.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `dest1` 和 `dest2` 有独立的背压。如果 `dest1` 慢，`pass1` 缓冲。如果 `dest2` 快，`pass2` 不缓冲。源不会被任何一个目的地暂停。
- en: But this breaks source-level backpressure. If both destinations are slow, `pass1`
    and `pass2` both buffer, and the source isn't aware. You're buffering in memory
    unbounded.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 但这打破了源级别的背压。如果两个目的地都慢，`pass1` 和 `pass2` 都会缓冲，而源并不知情。你在内存中无限制地缓冲。
- en: 'For truly independent destinations with bounded memory, you need to use a fan-out
    stream that monitors backpressure from all destinations and pauses the source
    only when all destinations signal backpressure:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有有限内存的真正独立目的地，你需要使用一个扇出流，该流监控所有目的地的背压，并且仅在所有目的地发出背压信号时才暂停源：
- en: '[PRE48]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This writable forwards each chunk to multiple destinations. The key difference
    from simple piping is how it handles backpressure: `write()` returns a boolean
    indicating whether you can continue writing. If it returns `false`, the destination''s
    buffer is full, and you should wait for the `drain` event before writing more.
    This implementation collects promises for any destinations signaling backpressure
    and waits for all of them to drain before invoking the callback, which creates
    backpressure on the source.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这个可写流将每个块转发到多个目的地。与简单管道的关键区别在于它如何处理背压：`write()` 返回一个布尔值，指示你是否可以继续写入。如果它返回 `false`，目的地的缓冲区已满，你应该在写入更多之前等待
    `drain` 事件。此实现收集任何表示背压的 destinations 的承诺，并在调用回调之前等待所有这些承诺都排空，这会在源上创建背压。
- en: 'You use it like this:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样使用它：
- en: '[PRE49]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This is a more complex pattern, and it's not common in application code. Most
    of the time, you either accept that the slowest destination controls the pace,
    or you accept unbounded buffering in PassThrough intermediaries. True fan-out
    with independent backpressure per destination is complex and usually only needed
    in specialized scenarios like logging or monitoring systems.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个更复杂的模式，在应用程序代码中并不常见。大多数时候，你或者接受最慢的目的地控制节奏，或者接受 PassThrough 中间件的未限定缓冲。真正的扇出，每个目的地都有独立的背压，是复杂的，通常只在像日志或监控系统这样的特定场景中需要。
- en: AbortSignal Integration
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 终止信号集成
- en: 'Node.js streams support `AbortSignal` for cancellation. You can pass a signal
    to the promise-based `pipeline()`, and if the signal is aborted, the pipeline
    is destroyed:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js 流支持 `AbortSignal` 用于取消。你可以将信号传递给基于承诺的 `pipeline()`，如果信号被终止，管道将被销毁：
- en: '[PRE50]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: When you call `controller.abort()`, the pipeline is immediately destroyed. All
    streams are torn down, the promise rejects with an **`AbortError`**, and any pending
    operations are cancelled.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用 `controller.abort()` 时，管道立即被销毁。所有流都被拆除，承诺因 **`AbortError`** 而拒绝，并且任何挂起的操作都被取消。
- en: This is useful for user-initiated cancellation, timeouts, or resource cleanup
    in long-running operations.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于用户发起的取消、超时或在长时间运行的操作中的资源清理很有用。
- en: 'You can also create a timeout signal:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以创建一个超时信号：
- en: '[PRE51]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: If the pipeline doesn't complete within 5 seconds, it's automatically aborted.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果管道在 5 秒内没有完成，它将自动终止。
- en: 'For complex scenarios with multiple cancellation sources, you can use `AbortSignal.any()`:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有多个取消源的复杂场景，你可以使用 `AbortSignal.any()`：
- en: '[PRE52]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This creates a composite signal that aborts if either the user cancels or the
    timeout expires.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这会创建一个复合信号，如果用户取消或超时到期，则会终止。
- en: AbortSignal integration makes cancellation explicit and standardized. Instead
    of manually calling `destroy()` on streams, you abort a signal, and the pipeline
    handles the cleanup.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 终止信号集成使取消变得明确和标准化。而不是手动在流上调用 `destroy()`，你终止一个信号，管道处理清理。
- en: Real-World Pipeline Examples
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际的管道示例
- en: Let's implement a few complete pipelines that demonstrate all the concepts we've
    covered.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现几个完整的管道，以展示我们已覆盖的所有概念。
- en: '**1) Log File Processing**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**1) 日志文件处理**'
- en: 'Read a large log file, parse each line as JSON, filter by log level, and write
    to separate output files:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 读取大日志文件，将每一行解析为 JSON，按日志级别过滤，并写入单独的输出文件：
- en: '[PRE53]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `parseLines` generator handles a common challenge in line-based stream
    processing: chunks don''t align with line boundaries. A chunk might end mid-line,
    splitting `{"level":"ERROR"...` across two chunks. The solution uses buffer accumulation:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`parseLines` 生成器处理基于行流的常见挑战：块与行边界不对齐。一个块可能在行中间结束，将 `{"level":"ERROR"...` 分割到两个块中。解决方案使用缓冲区累积：'
- en: '[PRE54]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: When you split on `\n`, the last array element is either empty (if the chunk
    ended with a newline) or an incomplete line. Popping that element and saving it
    means the next chunk appends to it, completing the line. After the loop ends,
    any remaining buffered data gets processed—handling files that don't end with
    a newline.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 `\n` 上分割时，最后一个数组元素要么是空的（如果块以换行符结束），要么是不完整的行。弹出该元素并保存它意味着下一个块将附加到它，完成该行。循环结束后，任何剩余的缓冲数据都会被处理——处理不以换行符结尾的文件。
- en: 'The `try/catch` around `JSON.parse()` is critical:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`JSON.parse()` 周围的 `try/catch` 是关键的：'
- en: '[PRE55]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Without error handling, a single malformed JSON line crashes the entire pipeline,
    losing all progress. With it, the pipeline logs the error and continues. Real-world
    log files contain corrupted entries, so the pipeline needs to handle invalid data
    without stopping.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 没有错误处理，单个格式错误的 JSON 行会导致整个管道崩溃，丢失所有进度。有了它，管道记录错误并继续。现实世界的日志文件包含损坏的条目，因此管道需要在不停机的情况下处理无效数据。
- en: '`LevelSplitter` both filters data to a side channel and passes all data through:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`LevelSplitter` 既可以过滤数据到侧通道，也可以传递所有数据：'
- en: '[PRE56]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Every log entry continues down the main pipeline, but ERROR-level logs are
    *also* written to `errors.log`. This creates a branching pipeline:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 每个日志条目都会沿着主管道继续向下传递，但 ERROR 级别的日志也会写入 `errors.log`。这创建了一个分支管道：
- en: '[PRE57]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: This approach is memory-efficient. Reading the file once and splitting in-stream
    uses constant memory. Two separate pipelines would double I/O and memory usage.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法内存效率高。一次读取文件并在流中分割使用恒定内存。两个独立的管道会加倍 I/O 和内存使用。
- en: 'The `{ objectMode: true }` option is essential because this transform receives
    JavaScript objects from `parseLines`, not buffers. When writing to the side destinations,
    we convert back to JSON strings with `JSON.stringify(log) + "\n"`. Parse once,
    work with objects in the pipeline, serialize only when writing to disk.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`{ objectMode: true }` 选项至关重要，因为此转换从 `parseLines` 接收 JavaScript 对象，而不是缓冲区。当写入侧目的地时，我们使用
    `JSON.stringify(log) + "\n"` 将其转换回 JSON 字符串。一次解析，在管道中处理对象，仅在写入磁盘时进行序列化。'
- en: 'The splitters chain in sequence:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 分割器按顺序链接：
- en: '[PRE58]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Each splitter calls `this.push(log)`, passing objects through. The final destination
    `all.log` receives objects too, but since Writable streams automatically call
    `toString()` on objects, you'd want to add a final transform that serializes to
    JSON for proper formatting (we've simplified this for clarity, but in production
    you'd add `async function* serializeJSON(source) { for await (const obj of source)
    yield JSON.stringify(obj) + "\n"; }` before the final destination).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分割器调用 `this.push(log)`，传递对象。最终目的地 `all.log` 也会接收对象，但由于可写流会自动在对象上调用 `toString()`，因此你可能想要添加一个最终转换，将其序列化为
    JSON 以进行适当的格式化（为了清晰起见，我们简化了这一点，但在生产中你会在最终目的地之前添加 `async function* serializeJSON(source)
    { for await (const obj of source) yield JSON.stringify(obj) + "\n"; }`）。
- en: '**2) CSV Import with Validation**'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**2) 带验证的 CSV 导入**'
- en: 'Read a CSV file, parse rows, validate, and insert into a database with batching:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 读取 CSV 文件，解析行，验证，并以批量方式插入到数据库中：
- en: '[PRE59]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This pipeline uses composable generator functions for data transformation and
    batching for database operations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道使用可组合的生成器函数进行数据转换和批量处理数据库操作。
- en: 'Unlike `parseLines`, `parseCSV` maintains state across chunks—it needs to remember
    the header row:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `parseLines` 不同，`parseCSV` 在块之间保持状态——它需要记住标题行：
- en: '[PRE60]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The `headers` variable persists for the generator''s lifetime, capturing the
    first line and using it to structure all subsequent rows. Raw CSV:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`headers` 变量在生成器的整个生命周期中持续存在，捕获第一行并使用它来结构化所有后续行。原始 CSV：'
- en: '[PRE61]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Becomes structured objects:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 变成结构化对象：
- en: '[PRE62]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: We don't load the entire CSV into memory—each row is processed as data flows
    through.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会将整个 CSV 文件加载到内存中——每一行都在数据流通过时进行处理。
- en: 'The `validate` generator filters without modifying data:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '`validate` 生成器在不对数据进行修改的情况下进行过滤：'
- en: '[PRE63]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Valid rows continue downstream. Invalid rows are logged but not yielded, preventing
    bad data from reaching the database.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的行继续向下流。无效的行被记录但不会产生，防止不良数据到达数据库。
- en: Throwing an error on invalid data would crash the pipeline and lose all progress.
    Real-world data is messy—logging and continuing lets you review errors after the
    import completes.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在无效数据上抛出错误会导致管道崩溃并丢失所有进度。现实世界的数据很混乱——在导入完成后，记录日志并继续可以让你检查错误。
- en: 'The `batch` generator is essential for database operations:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch` 生成器对于数据库操作至关重要：'
- en: '[PRE64]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This transforms a stream of individual items into a stream of batches:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这将单个项的流转换为批次的流：
- en: '[PRE65]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Database round-trips are expensive. Batching into groups of 100 can provide
    a 100x speedup over inserting one row at a time. The batch size is a trade-off:
    too small means many round-trips and slow performance; too large means high memory
    usage, timeout risk, and harder error recovery. Most databases work well with
    batch sizes between 100-1000.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库往返很昂贵。将数据批量分组到 100 可以提供比逐行插入快 100 倍的速度。批量大小是一个权衡：太小意味着许多往返和慢速性能；太大意味着内存使用高，超时风险高，错误恢复更困难。大多数数据库在
    100-1000 之间的批量大小下表现良好。
- en: The `if (batch.length > 0)` after the loop handles the final partial batch.
    Without this check, trailing rows get silently dropped.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 循环之后的`if (batch.length > 0)`处理了最后的部分批次。如果没有这个检查，尾部行会被静默丢弃。
- en: 'The `DatabaseWriter` writable handles async I/O:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`DatabaseWriter`的可写处理异步I/O：'
- en: '[PRE66]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The `_write` method can be async, but you must still call the callback. Call
    `callback()` with no arguments on success, or `callback(err)` to propagate errors.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`_write`方法可以是异步的，但你仍然必须调用回调。在成功时，使用无参数调用`callback()`，或者在出现错误时调用`callback(err)`以传播错误。'
- en: While `await this.db.insertMany(batch)` runs, the stream is paused. The next
    batch won't be sent until the current insert completes, preventing database overload.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 当`await this.db.insertMany(batch)`运行时，流会被暂停。下一个批次将在当前插入完成之前不会发送，从而防止数据库过载。
- en: 'The arrow functions `(source) => validate(source, mySchema)` let you pass additional
    arguments to generators:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 箭头函数`(source) => validate(source, mySchema)`让你可以向生成器传递额外的参数：
- en: '[PRE67]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: You're creating specialized versions of generic generators for this specific
    pipeline.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在为这个特定管道创建通用生成器的专用版本。
- en: This entire pipeline uses roughly constant memory regardless of file size. CSV
    parsing keeps only the current line in memory. Validation processes one row at
    a time. Batching holds at most 100 rows. Writing processes only the current batch.
    A 10GB CSV file uses the same memory as a 10MB file.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 整个管道使用大约恒定的内存，无论文件大小如何。CSV解析只保留当前行在内存中。验证过程一次处理一行。批处理最多保留100行。写入过程只处理当前批次。一个10GB的CSV文件使用的内存与一个10MB的文件相同。
