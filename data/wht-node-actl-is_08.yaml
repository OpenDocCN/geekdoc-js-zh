- en: Working with Buffers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ç¼“å†²åŒºä¸€èµ·å·¥ä½œ
- en: åŸæ–‡ï¼š[https://www.thenodebook.com/buffers/working-with-buffers](https://www.thenodebook.com/buffers/working-with-buffers)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://www.thenodebook.com/buffers/working-with-buffers](https://www.thenodebook.com/buffers/working-with-buffers)
- en: â„¹ï¸Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: â„¹ï¸å¤‡æ³¨
- en: This chapter takes a deep dive into Buffers. If any part feels unclear or overwhelming,
    donâ€™t worry, re-read the section, or revisit it later after going through other
    (sub) chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« æ·±å…¥æ¢è®¨äº†ç¼“å†²åŒºã€‚å¦‚æœä»»ä½•éƒ¨åˆ†æ„Ÿè§‰ä¸æ¸…æ¥šæˆ–ä»¤äººä¸çŸ¥æ‰€æªï¼Œè¯·ä¸è¦æ‹…å¿ƒï¼Œé‡æ–°é˜…è¯»è¯¥éƒ¨åˆ†ï¼Œæˆ–è€…åœ¨å…¶ä»–ï¼ˆå­ï¼‰ç« èŠ‚é˜…è¯»å®Œæ¯•åå†æ¬¡å›é¡¾ã€‚
- en: 'I''m pretty sure you''re here because either you wish to learn how do I apply
    all the knowledge learnt in the previous chapters, or something in your Node.js
    service is consuming excessive amounts of memory. Or maybe because your high-throughput
    binary protocol parser is executing at extremely slow speeds. The culprit is almost
    always a fundamental misunderstanding of how Node.js `Buffer`s handle memory.
    This chapter is your guide out of that hell. We''re going to dismantle the most
    dangerous assumption in Node.js development: that `Buffer.slice()` behaves like
    `Array.prototype.slice()`. It doesn''t. Not even close. One creates a new, independent
    copy of data; the other creates a "view" - a mere window into the *exact same
    underlying memory* as the original. This is the heart of zero-copy operations.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¾ˆç¡®å®šä½ åœ¨è¿™é‡Œæ˜¯å› ä¸ºä½ å¸Œæœ›å­¦ä¹ å¦‚ä½•å°†ä¹‹å‰ç« èŠ‚ä¸­å­¦åˆ°çš„æ‰€æœ‰çŸ¥è¯†åº”ç”¨åˆ°å®è·µä¸­ï¼Œæˆ–è€…ä½ çš„Node.jsæœåŠ¡æ­£åœ¨æ¶ˆè€—è¿‡é‡çš„å†…å­˜ã€‚æˆ–è€…ä¹Ÿè®¸æ˜¯å› ä¸ºä½ çš„é«˜ååé‡äºŒè¿›åˆ¶åè®®è§£æå™¨æ‰§è¡Œé€Ÿåº¦ææ…¢ã€‚ç½ªé­ç¥¸é¦–å‡ ä¹æ€»æ˜¯å¯¹Node.js
    `Buffer`å¦‚ä½•å¤„ç†å†…å­˜çš„æ ¹æœ¬æ€§è¯¯è§£ã€‚æœ¬ç« å°†å¼•å¯¼ä½ èµ°å‡ºè¿™ä¸ªåœ°ç‹±ã€‚æˆ‘ä»¬å°†æ‹†è§£Node.jså¼€å‘ä¸­æœ€å±é™©çš„å‡è®¾ï¼šå³`Buffer.slice()`çš„è¡Œä¸ºç±»ä¼¼äº`Array.prototype.slice()`ã€‚å®ƒå¹¶ä¸ç›¸åŒã€‚ä¸€ä¸ªåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ã€ç‹¬ç«‹çš„æ•°æ®å‰¯æœ¬ï¼›å¦ä¸€ä¸ªåˆ›å»ºäº†ä¸€ä¸ªâ€œè§†å›¾â€â€”â€”ä»…ä»…æ˜¯åŸå§‹æ•°æ®çš„çª—å£ã€‚è¿™æ˜¯é›¶æ‹·è´æ“ä½œçš„æ ¸å¿ƒã€‚
- en: ğŸš¨Caution
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš¨è­¦å‘Š
- en: '`Buffer.slice` is deprecated in favor of `Buffer.subarray()`. However, understanding
    its behavior remains essential for maintaining legacy codebases and comprehending
    view mechanics.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.slice`å·²è¢«å¼ƒç”¨ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯`Buffer.subarray()`ã€‚ç„¶è€Œï¼Œç†è§£å…¶è¡Œä¸ºå¯¹äºç»´æŠ¤é—ç•™ä»£ç åº“å’Œäº†è§£è§†å›¾æœºåˆ¶ä»ç„¶æ˜¯è‡³å…³é‡è¦çš„ã€‚'
- en: When used correctly, these views are really powerful, letting you process massive
    amounts of data with almost no memory overhead. When misunderstood, they create
    the most insidious, hard-to-debug memory leaks you'll ever encounter - leaks where
    a tiny 10-byte slice holds a 1GB buffer hostage in memory, preventing the garbage
    collector from reclaiming it. We'll walk through the war stories, the late-night
    debugging sessions, and the production outages that forged this knowledge. You'll
    learn the critical difference between a view (`slice`, `subarray`) and a true
    copy (`Buffer.copy()`), and when to use each. We'll explore the intimate relationship
    between `Buffer`s and `TypedArray`s, how they share the same memory foundation
    (`ArrayBuffer`), and how that can be both a superpower and a source of subtle
    data corruption. By the end of this, you won't just know the API; you'll have
    developed a deep, almost instinctual respect for memory semantics. You'll understand
    why your service is using 10GB of memory for 1GB of data and, more importantly,
    how to fix it for good.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ­£ç¡®ä½¿ç”¨æ—¶ï¼Œè¿™äº›è§†å›¾éå¸¸å¼ºå¤§ï¼Œè®©ä½ å‡ ä¹æ— éœ€å†…å­˜å¼€é”€å°±èƒ½å¤„ç†å¤§é‡æ•°æ®ã€‚å½“è¢«è¯¯è§£æ—¶ï¼Œå®ƒä»¬ä¼šåˆ›å»ºä½ é‡åˆ°çš„æœ€éšè”½ã€æœ€éš¾è°ƒè¯•çš„å†…å­˜æ³„æ¼â€”â€”ä¸€ä¸ªåªæœ‰10å­—èŠ‚çš„åˆ‡ç‰‡å´å¯ä»¥é”å®š1GBçš„ç¼“å†²åŒºåœ¨å†…å­˜ä¸­ï¼Œé˜»æ­¢åƒåœ¾å›æ”¶å™¨å›æ”¶å®ƒã€‚æˆ‘ä»¬å°†å›é¡¾è¿™äº›æˆ˜äº‰æ•…äº‹ã€æ·±å¤œè°ƒè¯•ä¼šè¯å’Œç”Ÿäº§ä¸­æ–­ï¼Œè¿™äº›ç»å†å¡‘é€ äº†è¿™äº›çŸ¥è¯†ã€‚ä½ å°†äº†è§£è§†å›¾ï¼ˆ`slice`ã€`subarray`ï¼‰å’ŒçœŸæ­£çš„å‰¯æœ¬ï¼ˆ`Buffer.copy()`ï¼‰ä¹‹é—´çš„å…³é”®åŒºåˆ«ï¼Œä»¥åŠä½•æ—¶ä½¿ç”¨æ¯ä¸ªã€‚æˆ‘ä»¬å°†æ¢è®¨`Buffer`å’Œ`TypedArray`ä¹‹é—´çš„å¯†åˆ‡å…³ç³»ï¼Œå®ƒä»¬å…±äº«ç›¸åŒçš„å†…å­˜åŸºç¡€ï¼ˆ`ArrayBuffer`ï¼‰ï¼Œä»¥åŠè¿™æ—¢å¯ä»¥æ˜¯ä¸€ç§è¶…çº§èƒ½åŠ›ï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªå¾®å¦™çš„å†…å­˜æŸåçš„æ¥æºã€‚åˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œä½ ä¸ä»…ä¼šäº†è§£APIï¼Œè€Œä¸”ä¼šå¯¹å†…å­˜è¯­ä¹‰æœ‰æ·±åˆ»çš„ã€å‡ ä¹æœ¬èƒ½çš„å°Šé‡ã€‚ä½ å°†ç†è§£ä¸ºä»€ä¹ˆä½ çš„æœåŠ¡ä½¿ç”¨10GBçš„å†…å­˜æ¥å¤„ç†1GBçš„æ•°æ®ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå¦‚ä½•æ°¸ä¹…ä¿®å¤å®ƒã€‚
- en: A Gigabyte-Scale Memory Leak
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‰å­—èŠ‚çº§å†…å­˜æ³„æ¼
- en: Ever seen a service that should use a tidy 500MB of RAM suddenly decide it needs
    10GB to live? It's an amazing feature of misunderstanding memory. Let's walk through
    how you could, with the best intentions and perfectly clean-looking code, build
    this exact disaster. It's the best way to learn how to prevent it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯å¦è§è¿‡ä¸€ä¸ªæœ¬åº”åªä½¿ç”¨æ•´æ´çš„500MB RAMçš„æœåŠ¡çªç„¶å†³å®šå®ƒéœ€è¦10GBæ‰èƒ½è¿è¡Œï¼Ÿè¿™æ˜¯å¯¹å†…å­˜ç†è§£çš„æƒŠäººè¯¯è§£ã€‚è®©æˆ‘ä»¬ä¸€æ­¥æ­¥äº†è§£ï¼Œå³ä½¿æ€€ç€æœ€å¥½çš„æ„å›¾å’Œçœ‹èµ·æ¥å®Œç¾çš„ä»£ç ï¼Œä½ å¦‚ä½•æ„å»ºè¿™ç§ç¾éš¾ã€‚è¿™æ˜¯å­¦ä¹ å¦‚ä½•é¢„é˜²å®ƒçš„æœ€ä½³æ–¹å¼ã€‚
- en: âš ï¸Warning
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸æ³¨æ„
- en: 'The buffer memory retention patterns described in this chapter are the #1 cause
    of production Node.js memory leaks. A single `Buffer.slice()` can hold gigabytes
    of memory hostage indefinitely.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä¸­æè¿°çš„ç¼“å†²åŒºå†…å­˜ä¿ç•™æ¨¡å¼æ˜¯ç”Ÿäº§ç¯å¢ƒä¸­Node.jså†…å­˜æ³„æ¼çš„é¦–è¦åŸå› ã€‚å•ä¸ª`Buffer.slice()`å¯ä»¥æ— é™æœŸåœ°é”å®šæ•°GBçš„å†…å­˜ã€‚
- en: 'Imagine a common scenario: a service that ingests large batches of data. Maybe
    they''re logs, maybe they''re multipart file uploads. The task is simple: for
    each incoming chunk (which could be several megabytes), you need to parse a small,
    fixed-size header to extract an identifier, like a session ID. You''d write a
    function that looks something like this. Be honest, you''ve probably written this
    code a dozen times.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸ªå¸¸è§çš„åœºæ™¯ï¼šä¸€ä¸ªå¤„ç†å¤§é‡æ•°æ®çš„æœåŠ¡ã€‚ä¹Ÿè®¸å®ƒä»¬æ˜¯æ—¥å¿—ï¼Œä¹Ÿè®¸æ˜¯å¤šéƒ¨åˆ†æ–‡ä»¶ä¸Šä¼ ã€‚ä»»åŠ¡å¾ˆç®€å•ï¼šå¯¹äºæ¯ä¸ªä¼ å…¥çš„å—ï¼ˆå¯èƒ½æ˜¯å‡ ä¸ªå…†å­—èŠ‚ï¼‰ï¼Œä½ éœ€è¦è§£æä¸€ä¸ªå°çš„å›ºå®šå¤§å°çš„å¤´éƒ¨æ¥æå–ä¸€ä¸ªæ ‡è¯†ç¬¦ï¼Œæ¯”å¦‚ä¼šè¯IDã€‚ä½ ä¼šç¼–å†™ä¸€ä¸ªçœ‹èµ·æ¥åƒè¿™æ ·çš„å‡½æ•°ã€‚è¯´å®è¯ï¼Œä½ å¯èƒ½å·²ç»å†™äº†åå‡ æ¬¡è¿™æ ·çš„ä»£ç ã€‚
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Stop right here. This exact line - `logBuffer.slice(0, 16)` - is where your
    production system begins its death spiral. Here''s what''s actually happening
    in the Node.js internals. When you call `slice()`, Node.js doesn''t allocate new
    memory. Instead, it creates a tiny JavaScript object (about 72 bytes on V8) that
    contains three critical pieces of information: a pointer to the parent buffer''s
    ArrayBuffer, an offset (0 in this case), and a length (16). This object lives
    on the V8 heap, but it maintains a strong reference to the external memory where
    `logBuffer` stores its actual data.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç«‹å³åœæ­¢ã€‚è¿™æ¡ç¡®åˆ‡çš„ä¸€è¡Œâ€”â€”`logBuffer.slice(0, 16)`â€”â€”æ˜¯ä½ çš„ç”Ÿäº§ç³»ç»Ÿå¼€å§‹æ­»äº¡èºæ—‹çš„åœ°æ–¹ã€‚ä¸‹é¢æ˜¯Node.jså†…éƒ¨å®é™…å‘ç”Ÿçš„äº‹æƒ…ã€‚å½“ä½ è°ƒç”¨`slice()`æ—¶ï¼ŒNode.jsä¸ä¼šåˆ†é…æ–°çš„å†…å­˜ã€‚ç›¸åï¼Œå®ƒåˆ›å»ºäº†ä¸€ä¸ªå¾®å°çš„JavaScriptå¯¹è±¡ï¼ˆåœ¨V8ä¸Šå¤§çº¦æ˜¯72å­—èŠ‚ï¼‰ï¼Œå…¶ä¸­åŒ…å«ä¸‰ä¸ªå…³é”®ä¿¡æ¯ï¼šæŒ‡å‘çˆ¶ç¼“å†²åŒºçš„ArrayBufferçš„æŒ‡é’ˆã€åç§»é‡ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ä¸º0ï¼‰å’Œé•¿åº¦ï¼ˆ16ï¼‰ã€‚è¿™ä¸ªå¯¹è±¡å­˜åœ¨äºV8å †ä¸Šï¼Œä½†å®ƒä¿æŒå¯¹å¤–éƒ¨å†…å­˜çš„å¼ºå¼•ç”¨ï¼Œå…¶ä¸­`logBuffer`å­˜å‚¨å…¶å®é™…æ•°æ®ã€‚
- en: The V8 garbage collector sees this reference and marks the entire parent buffer
    as "reachable." Even though you only care about 16 bytes, the GC must keep the
    entire multi-megabyte buffer alive. In V8's generational garbage collection system,
    this parent buffer gets promoted from the young generation to the old generation
    after surviving two scavenges, making it even harder to collect. I've seen this
    pattern keep 100MB buffers alive for hours in production, all for the sake of
    storing a handful of 16-byte session IDs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: V8åƒåœ¾å›æ”¶å™¨çœ‹åˆ°è¿™ä¸ªå¼•ç”¨å¹¶å°†æ•´ä¸ªçˆ¶ç¼“å†²åŒºæ ‡è®°ä¸ºâ€œå¯åˆ°è¾¾â€ã€‚å³ä½¿ä½ åªå…³å¿ƒ16ä¸ªå­—èŠ‚ï¼Œåƒåœ¾å›æ”¶å™¨ä¹Ÿå¿…é¡»ä¿æŒæ•´ä¸ªå¤šå…†å­—èŠ‚çš„ç¼“å†²åŒºå­˜æ´»ã€‚åœ¨V8çš„ä»£é™…åƒåœ¾å›æ”¶ç³»ç»Ÿä¸­ï¼Œè¿™ä¸ªçˆ¶ç¼“å†²åŒºåœ¨ç»å†ä¸¤æ¬¡æ¸…ç†åä»å¹´è½»ä»£æå‡åˆ°è€å¹´ä»£ï¼Œè¿™ä½¿å¾—å®ƒæ›´éš¾è¢«å›æ”¶ã€‚æˆ‘è§è¿‡è¿™ç§æ¨¡å¼åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è®©100MBçš„ç¼“å†²åŒºå­˜æ´»æ•°å°æ—¶ï¼Œåªæ˜¯ä¸ºäº†å­˜å‚¨ä¸€å°æŠŠ16å­—èŠ‚çš„ä¼šè¯IDã€‚
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code will inevitably cause production failures. It looks innocent, but
    `logBuffer.slice(0, 16)` is the line that will cause your production environment
    to fail completely. Here's what happens. You're processing, say, 100MB of logs
    per minute. Your service's memory usage (the Resident Set Size, or RSS) should
    stay relatively flat. Instead, you watch it climb, gigabyte by gigabyte. You're
    holding onto 10GB of memory to manage what should be, at most, a few megabytes
    of session IDs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç ä¸å¯é¿å…åœ°ä¼šå¯¼è‡´ç”Ÿäº§æ•…éšœã€‚å®ƒçœ‹èµ·æ¥å¾ˆæ— è¾œï¼Œä½†`logBuffer.slice(0, 16)`æ˜¯å¯¼è‡´ä½ çš„ç”Ÿäº§ç¯å¢ƒå®Œå…¨å¤±è´¥çš„é‚£ä¸€è¡Œã€‚ä¸‹é¢æ˜¯å®é™…å‘ç”Ÿçš„æƒ…å†µã€‚ä½ æ¯åˆ†é’Ÿå¤„ç†å¤§çº¦100MBçš„æ—¥å¿—ã€‚ä½ çš„æœåŠ¡å†…å­˜ä½¿ç”¨é‡ï¼ˆå¸¸é©»é›†å¤§å°ï¼Œæˆ–RSSï¼‰åº”è¯¥ä¿æŒç›¸å¯¹ç¨³å®šã€‚ç›¸åï¼Œä½ çœ‹åˆ°å®ƒä»¥åƒå…†å­—èŠ‚ä¸ºå•ä½æ”€å‡ã€‚ä½ ä¿ç•™10GBçš„å†…å­˜æ¥ç®¡ç†æœ¬åº”æœ€å¤šåªæœ‰å‡ å…†å­—èŠ‚çš„ä¼šè¯IDã€‚
- en: 'So you take a heap snapshot, and what you see makes no sense. The profiler
    shows you thousands of tiny 16-byte `Buffer` objects, but it claims they are collectively
    responsible for retaining gigabytes of memory. Your first thought is, "The profiler
    is broken." It isn''t. It''s showing you a fundamental truth: the slice is not
    a copy. It''s a view. That 16-byte `headerSlice` object is just a lightweight
    JavaScript wrapper, but it holds an internal reference to the original, multi-megabyte
    `logBuffer`. As long as that tiny slice is alive - say, sitting in your cache
    - the garbage collector cannot reclaim the *entire* large buffer.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½ è¿›è¡Œäº†ä¸€ä¸ªå †å¿«ç…§ï¼Œè€Œä½ çœ‹åˆ°çš„ä¸œè¥¿æ¯«æ— æ„ä¹‰ã€‚åˆ†æå™¨æ˜¾ç¤ºç»™ä½ æˆåƒä¸Šä¸‡çš„å¾®å°çš„16å­—èŠ‚`Buffer`å¯¹è±¡ï¼Œä½†å®ƒå£°ç§°å®ƒä»¬å…±åŒä¿ç•™äº†æ•°GBçš„å†…å­˜ã€‚ä½ çš„ç¬¬ä¸€ä¸ªæƒ³æ³•æ˜¯ï¼Œâ€œåˆ†æå™¨åäº†ã€‚â€å…¶å®å¹¶æ²¡æœ‰ã€‚å®ƒå‘ä½ å±•ç¤ºäº†ä¸€ä¸ªåŸºæœ¬çœŸç†ï¼šåˆ‡ç‰‡ä¸æ˜¯å‰¯æœ¬ï¼Œå®ƒæ˜¯ä¸€ä¸ªè§†å›¾ã€‚é‚£ä¸ª16å­—èŠ‚çš„`headerSlice`å¯¹è±¡åªæ˜¯ä¸€ä¸ªè½»é‡çº§çš„JavaScriptåŒ…è£…å™¨ï¼Œä½†å®ƒæŒæœ‰å¯¹åŸå§‹çš„å¤šå…†å­—èŠ‚`logBuffer`çš„å†…éƒ¨å¼•ç”¨ã€‚åªè¦é‚£ä¸ªå¾®å°çš„åˆ‡ç‰‡å­˜æ´»â€”â€”æ¯”å¦‚è¯´ï¼Œå®ƒååœ¨ä½ çš„ç¼“å­˜ä¸­â€”â€”åƒåœ¾å›æ”¶å™¨å°±æ— æ³•å›æ”¶æ•´ä¸ªå¤§çš„ç¼“å†²åŒºã€‚
- en: You weren't leaking a few bytes. You were leaking the massive parent buffer
    for every single request. Multiply that by thousands of requests, and you have
    a recipe for the exact 10GB memory leak we're dissecting. This is the consequence
    of misinterpreting one of the most common methods in the `Buffer` API. Let's dig
    into why.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¹¶æ²¡æœ‰æ³„éœ²å‡ ä¸ªå­—èŠ‚ã€‚ä½ æ³„éœ²äº†æ¯ä¸ªè¯·æ±‚çš„æ•´ä¸ªçˆ¶ç¼“å†²åŒºã€‚ä¹˜ä»¥æ•°åƒä¸ªè¯·æ±‚ï¼Œä½ å°±æœ‰äº†æˆ‘ä»¬æ­£åœ¨å‰–æçš„10GBå†…å­˜æ³„éœ²çš„é…æ–¹ã€‚è¿™æ˜¯è¯¯è§£`Buffer` APIä¸­æœ€å¸¸è§æ–¹æ³•ä¹‹ä¸€çš„åæœã€‚è®©æˆ‘ä»¬æ·±å…¥æ¢ç©¶åŸå› ã€‚
- en: 'Alright, let''s cut the fluff. As we established in the previous chapter, trying
    to handle raw binary data with JavaScript strings is a recipe for disaster. To
    understand the solution, the `Buffer`, you have to burn this into your brain:
    **Node.js `Buffer` memory does not live on the V8 heap.**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œè®©æˆ‘ä»¬å»æ‰åºŸè¯ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šä¸€ç« ä¸­æåˆ°çš„ï¼Œè¯•å›¾ç”¨JavaScriptå­—ç¬¦ä¸²å¤„ç†åŸå§‹äºŒè¿›åˆ¶æ•°æ®æ˜¯ç¾éš¾çš„é…æ–¹ã€‚è¦ç†è§£è§£å†³æ–¹æ¡ˆï¼Œå³`Buffer`ï¼Œä½ å¿…é¡»å°†å…¶åˆ»åœ¨ä½ çš„è„‘æµ·ä¸­ï¼š**Node.jsçš„`Buffer`å†…å­˜ä¸ç”Ÿæ´»åœ¨V8å †ä¸Š**ã€‚
- en: Understanding Buffer Memory Architecture
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç†è§£Bufferå†…å­˜æ¶æ„
- en: Remember how we talked about V8's world being built for small, interconnected
    JavaScript objects? Its Garbage Collector (GC) is a champ at cleaning up strings
    and objects, but it chokes on huge, monolithic blobs of binary data. A massive
    file read would trigger an application-freezing "stop-the-world" pause, killing
    your performance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å¾—æˆ‘ä»¬æåˆ°V8çš„ä¸–ç•Œæ˜¯ä¸ºå°å‹ã€ç›¸äº’è¿æ¥çš„JavaScriptå¯¹è±¡æ„å»ºçš„å—ï¼Ÿå®ƒçš„åƒåœ¾å›æ”¶å™¨ï¼ˆGCï¼‰åœ¨æ¸…ç†å­—ç¬¦ä¸²å’Œå¯¹è±¡æ–¹é¢æ˜¯ä¸€æŠŠå¥½æ‰‹ï¼Œä½†åœ¨å¤„ç†å·¨å¤§çš„ã€å•ä¸€çš„äºŒè¿›åˆ¶æ•°æ®å—æ—¶å´æ˜¾å¾—åŠ›ä¸ä»å¿ƒã€‚ä¸€æ¬¡å¤§é‡çš„æ–‡ä»¶è¯»å–ä¼šè§¦å‘ä¸€ä¸ªä½¿åº”ç”¨ç¨‹åºå†»ç»“çš„â€œåœæ­¢ä¸–ç•Œâ€æš‚åœï¼Œä»è€Œæ€æ­»ä½ çš„æ€§èƒ½ã€‚
- en: So, Node does the smart thing. It allocates that big chunk of memory **outside**
    the V8 heap, in C++ land, closer to the metal. This is often called "off-heap"
    or "external" memory.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒNodeåšäº†æ˜æ™ºçš„äº‹æƒ…ã€‚å®ƒå°†é‚£å¤§å—å†…å­˜**å¤–éƒ¨**åˆ†é…åœ¨C++é¢†åŸŸï¼Œæ›´æ¥è¿‘ç¡¬ä»¶ã€‚è¿™é€šå¸¸è¢«ç§°ä¸ºâ€œå †å¤–â€æˆ–â€œå¤–éƒ¨â€å†…å­˜ã€‚
- en: The `Buffer` object you play with in your JavaScript code, as we touched on
    before, is just a tiny, lightweight **handle** that lives on the V8 heap. It's
    like a keycard. The keycard itself is small and easy for the V8 GC to track. But
    it holds a reference that points to that massive block of raw memory outside.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åœ¨JavaScriptä»£ç ä¸­æ“ä½œçš„`Buffer`å¯¹è±¡ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œåªæ˜¯ä¸€ä¸ªå¾®å°çš„ã€è½»é‡çº§çš„**å¥æŸ„**ï¼Œå®ƒç”Ÿæ´»åœ¨V8å †ä¸Šã€‚å®ƒå°±åƒä¸€å¼ é’¥åŒ™å¡ã€‚é’¥åŒ™å¡æœ¬èº«å¾ˆå°ï¼ŒV8
    GCå¾ˆå®¹æ˜“è¿½è¸ªã€‚ä½†å®ƒæŒæœ‰æŒ‡å‘é‚£å—å·¨å¤§åŸå§‹å†…å­˜çš„å¼•ç”¨ã€‚
- en: 'This two-part system is the source of both incredible performance and epic
    confusion:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¸¤éƒ¨åˆ†çš„ç³»ç»Ÿæ˜¯æ—¢å¸¦æ¥æƒŠäººæ€§èƒ½åˆé€ æˆå·¨å¤§å›°æƒ‘çš„æºå¤´ï¼š
- en: Node can pass these giant memory blocks to the filesystem or network card without
    ever having to copy them into JavaScript's world. It's super-efficient.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nodeå¯ä»¥å°†è¿™äº›å·¨å¤§çš„å†…å­˜å—ç›´æ¥ä¼ é€’ç»™æ–‡ä»¶ç³»ç»Ÿæˆ–ç½‘å¡ï¼Œè€Œæ— éœ€å°†å®ƒä»¬å¤åˆ¶åˆ°JavaScriptçš„ä¸–ç•Œä¸­ã€‚è¿™éå¸¸é«˜æ•ˆã€‚
- en: The V8 garbage collector only sees the tiny keycard. If your code accidentally
    holds onto that keycard (like in a closure or a long-lived object), V8 won't clean
    it up. And as long as the keycard exists, it acts as an anchor, preventing that
    huge, multi-megabyte block of external memory from being freed. You're not leaking
    a few bytes of JavaScript objects; you're leaking the massive memory slabs they
    point to.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V8åƒåœ¾å›æ”¶å™¨åªèƒ½çœ‹åˆ°è¿™å¼ å¾®å°çš„é’¥åŒ™å¡ã€‚å¦‚æœä½ çš„ä»£ç æ„å¤–åœ°ä¿ç•™äº†è¿™å¼ é’¥åŒ™å¡ï¼ˆæ¯”å¦‚åœ¨é—­åŒ…æˆ–é•¿æœŸå­˜åœ¨çš„å¯¹è±¡ä¸­ï¼‰ï¼ŒV8å°±ä¸ä¼šæ¸…ç†å®ƒã€‚åªè¦é’¥åŒ™å¡å­˜åœ¨ï¼Œå®ƒå°±å……å½“é”šç‚¹ï¼Œé˜²æ­¢é‚£å·¨å¤§çš„ã€å¤šå…†å­—èŠ‚çš„å—å¤–éƒ¨å†…å­˜è¢«é‡Šæ”¾ã€‚ä½ å¹¶ä¸æ˜¯åœ¨æ³„æ¼å‡ ä¸ªå­—èŠ‚çš„JavaScriptå¯¹è±¡ï¼›ä½ æ˜¯åœ¨æ³„æ¼å®ƒä»¬æŒ‡å‘çš„å·¨å¤§å†…å­˜å—ã€‚
- en: 'The 8KB Speed Hack: Buffer Pooling'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8KBé€Ÿåº¦é»‘å®¢ï¼šBufferæ± 
- en: 'Now, as we covered when discussing allocation patterns, Node has a speed hack
    for smaller buffers: the **buffer pool**. To avoid constantly asking the OS for
    memory, Node pre-allocates an 8KB (`Buffer.poolSize`) slab. For any buffer smaller
    than 4KB, Node just slices a piece off this pool instead of bugging the OS for
    new memory.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨è®¨è®ºåˆ†é…æ¨¡å¼æ—¶æåˆ°çš„ï¼ŒNodeä¸ºè¾ƒå°çš„ç¼“å†²åŒºæœ‰ä¸€ä¸ªé€Ÿåº¦é»‘å®¢ï¼š**bufferæ± **ã€‚ä¸ºäº†é¿å…ä¸æ–­å‘æ“ä½œç³»ç»Ÿè¯·æ±‚å†…å­˜ï¼ŒNodeé¢„å…ˆåˆ†é…äº†ä¸€ä¸ª8KBï¼ˆ`Buffer.poolSize`ï¼‰çš„å†…å­˜å—ã€‚å¯¹äºä»»ä½•å°äº4KBçš„ç¼“å†²åŒºï¼ŒNodeåªæ˜¯ä»è¿™ä¸ªæ± ä¸­åˆ‡ä¸‹ä¸€å—ï¼Œè€Œä¸æ˜¯å»éº»çƒ¦æ“ä½œç³»ç»Ÿä¸ºæ–°å†…å­˜ã€‚
- en: This is a massive performance boost for applications that use lots of small
    buffers. This is also the exact mechanism that makes `Buffer.allocUnsafe()` so
    treacherous, a topic we dissected earlier. You're not getting fresh memory; you're
    getting a recycled piece of the pool that could be littered with secrets - like
    another user's session token - from another part of your application that ran
    moments before.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹äºä½¿ç”¨å¤§é‡å°ç¼“å†²åŒºçš„åº”ç”¨ç¨‹åºæ¥è¯´æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ€§èƒ½æå‡ã€‚è¿™ä¹Ÿæ˜¯ä½¿`Buffer.allocUnsafe()`å¦‚æ­¤å±é™©çš„ç¡®åˆ‡æœºåˆ¶ï¼Œæˆ‘ä»¬ä¹‹å‰å·²ç»å‰–æè¿‡ã€‚ä½ å¹¶æ²¡æœ‰å¾—åˆ°æ–°é²œå†…å­˜ï¼›ä½ å¾—åˆ°çš„æ˜¯æ± ä¸­å›æ”¶çš„ç¢ç‰‡ï¼Œå®ƒå¯èƒ½å……æ»¡äº†æ¥è‡ªåº”ç”¨ç¨‹åºå…¶ä»–éƒ¨åˆ†çš„ç§˜å¯†â€”â€”æ¯”å¦‚å¦ä¸€ä¸ªç”¨æˆ·çš„ä¼šè¯ä»¤ç‰Œâ€”â€”è€Œè¿™äº›éƒ¨åˆ†åœ¨å‡ ç§’é’Ÿå‰è¿è¡Œè¿‡ã€‚
- en: 'Views and References: `slice`, `subarray`, and `Buffer.from`'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§†å›¾å’Œå¼•ç”¨ï¼š`slice`ã€`subarray`å’Œ`Buffer.from`
- en: Now that we have a clearer picture of where buffer memory lives, let's talk
    about the tools you use to manipulate it. This is the point where theory becomes
    practice, and where most developers make an incorrect decision. The three main
    functions we need to understand are `Buffer.slice()`, `Buffer.subarray()`, and
    `Buffer.from()` (when used with another buffer or `ArrayBuffer`).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯¹ç¼“å†²åŒºå†…å­˜æ‰€åœ¨çš„ä½ç½®æœ‰äº†æ›´æ¸…æ™°çš„äº†è§£ï¼Œè®©æˆ‘ä»¬è°ˆè°ˆä½ ç”¨æ¥æ“ä½œå®ƒçš„å·¥å…·ã€‚è¿™æ˜¯ç†è®ºå˜æˆå®è·µçš„åœ°æ–¹ï¼Œä¹Ÿæ˜¯å¤§å¤šæ•°å¼€å‘è€…åšå‡ºé”™è¯¯å†³å®šçš„åœ°æ–¹ã€‚æˆ‘ä»¬éœ€è¦äº†è§£çš„ä¸‰ä¸ªä¸»è¦å‡½æ•°æ˜¯
    `Buffer.slice()`ã€`Buffer.subarray()` å’Œ `Buffer.from()`ï¼ˆå½“ä¸å¦ä¸€ä¸ªç¼“å†²åŒºæˆ– `ArrayBuffer`
    ä¸€èµ·ä½¿ç”¨æ—¶ï¼‰ã€‚
- en: 'Let''s start with the one that causes a lot of trouble: `slice()`. If you come
    from a JavaScript background, your habitual programming patterns suggest that
    `Array.prototype.slice()` creates a shallow copy. You slice an array, you get
    a new array, and you can modify one without affecting the other. This is a lie
    when it comes to buffers.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»å¼•èµ·å¾ˆå¤šéº»çƒ¦çš„é‚£ä¸ªå¼€å§‹ï¼š`slice()`ã€‚å¦‚æœä½ æœ‰ JavaScript çš„èƒŒæ™¯ï¼Œä½ çš„ä¹ æƒ¯ç¼–ç¨‹æ¨¡å¼å¯èƒ½ä¼šè®©ä½ è®¤ä¸º `Array.prototype.slice()`
    åˆ›å»ºäº†ä¸€ä¸ªæµ…æ‹·è´ã€‚ä½ åˆ‡å‰²ä¸€ä¸ªæ•°ç»„ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªæ–°çš„æ•°ç»„ï¼Œä½ å¯ä»¥ä¿®æ”¹å…¶ä¸­ä¸€ä¸ªè€Œä¸å½±å“å¦ä¸€ä¸ªã€‚å½“æ¶‰åŠåˆ°ç¼“å†²åŒºæ—¶ï¼Œè¿™å…¶å®æ˜¯ä¸€ä¸ªè°è¨€ã€‚
- en: '`Buffer.prototype.slice()` does **not** create a copy. It creates a **view**.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.prototype.slice()` **ä¸ä¼š** åˆ›å»ºä¸€ä¸ªæ‹·è´ã€‚å®ƒåˆ›å»ºäº†ä¸€ä¸ª**è§†å›¾**ã€‚'
- en: 'Let me say that again, because it''s the most critical sentence in this entire
    chapter: `Buffer.slice()` creates a view that shares memory with the original
    buffer. It carves out a new `Buffer` object, but this new object points to the
    *exact same bytes in the same underlying `ArrayBuffer`* as the original.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘å†é‡å¤ä¸€éï¼Œå› ä¸ºè¿™æ˜¯æœ¬ç« ä¸­æœ€å…³é”®çš„å¥å­ï¼š`Buffer.slice()` åˆ›å»ºäº†ä¸€ä¸ªä¸åŸå§‹ç¼“å†²åŒºå…±äº«å†…å­˜çš„è§†å›¾ã€‚å®ƒåˆ‡å‰²å‡ºä¸€ä¸ªæ–°çš„ `Buffer`
    å¯¹è±¡ï¼Œä½†è¿™ä¸ªæ–°å¯¹è±¡æŒ‡å‘ä¸åŸå§‹å¯¹è±¡**å®Œå…¨ç›¸åŒçš„å­—èŠ‚**ï¼Œåœ¨ç›¸åŒçš„åº•å±‚ `ArrayBuffer` ä¸­ã€‚
- en: ğŸš¨Caution
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš¨æ³¨æ„
- en: '`Buffer.slice()` is NOT like `Array.slice()`. Arrays create copies, Buffers
    create views. Modifying a sliced buffer modifies the original. This single misunderstanding
    causes the majority of Node.js memory leaks and data corruption bugs in production.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.slice()` å¹¶ä¸åƒæ˜¯ `Array.slice()`ã€‚æ•°ç»„åˆ›å»ºæ‹·è´ï¼Œç¼“å†²åŒºåˆ›å»ºè§†å›¾ã€‚ä¿®æ”¹åˆ‡å‰²çš„ç¼“å†²åŒºä¼šä¿®æ”¹åŸå§‹ç¼“å†²åŒºã€‚è¿™ä¸ªè¯¯è§£å¯¼è‡´äº†
    Node.js ç”Ÿäº§ç¯å¢ƒä¸­å¤§å¤šæ•°å†…å­˜æ³„æ¼å’Œæ•°æ®æŸåé”™è¯¯ã€‚'
- en: Let me show you the innocent-looking code that nearly cost me my sanity.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘å±•ç¤ºä¸€ä¸‹é‚£ä¸ªçœ‹ä¼¼æ— è¾œçš„ä»£ç ï¼Œå®ƒå‡ ä¹è®©æˆ‘å¤±å»äº†ç†æ™ºã€‚
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That `Buffer.alloc()` call just triggered a cascade of events in Node's internals.
    First, Node.js checks if the requested size (52,428,800 bytes) is larger than
    `Buffer.poolSize >>> 1` (4096 bytes). It is, so Node bypasses the buffer pool
    entirely. It makes a direct call to the C++ layer to allocate 50MB of memory.
    On Linux, this typically results in an `mmap()` system call for large allocations,
    which maps anonymous pages into your process's address space. The kernel doesn't
    actually allocate physical RAM yet - it uses a technique called "demand paging"
    where physical pages are only allocated when you first write to them. This is
    why `Buffer.alloc()` zeroes the memory - it forces the kernel to allocate real
    physical pages immediately.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¸ª `Buffer.alloc()` è°ƒç”¨åœ¨ Node çš„å†…éƒ¨è§¦å‘äº†è¿é”ååº”ã€‚é¦–å…ˆï¼ŒNode.js æ£€æŸ¥è¯·æ±‚çš„å¤§å°ï¼ˆ52,428,800 å­—èŠ‚ï¼‰æ˜¯å¦å¤§äº
    `Buffer.poolSize >>> 1`ï¼ˆ4096 å­—èŠ‚ï¼‰ã€‚å®ƒæ˜¯ï¼Œæ‰€ä»¥ Node å®Œå…¨ç»•è¿‡äº†ç¼“å†²æ± ã€‚å®ƒç›´æ¥è°ƒç”¨ C++ å±‚æ¥åˆ†é… 50MB çš„å†…å­˜ã€‚åœ¨
    Linux ä¸Šï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´ä¸€ä¸ª `mmap()` ç³»ç»Ÿè°ƒç”¨ï¼Œç”¨äºå¤§åˆ†é…ï¼Œå®ƒå°†åŒ¿åé¡µé¢æ˜ å°„åˆ°ä½ çš„è¿›ç¨‹åœ°å€ç©ºé—´ã€‚å†…æ ¸å®é™…ä¸Šå¹¶æ²¡æœ‰åˆ†é…ç‰©ç† RAMï¼Œå®ƒä½¿ç”¨äº†ä¸€ç§ç§°ä¸ºâ€œæŒ‰éœ€åˆ†é¡µâ€çš„æŠ€æœ¯ï¼Œå…¶ä¸­ç‰©ç†é¡µé¢åªæœ‰åœ¨ç¬¬ä¸€æ¬¡å†™å…¥æ—¶æ‰ä¼šåˆ†é…ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ
    `Buffer.alloc()` ä¼šå°†å†…å­˜æ¸…é›¶â€”â€”å®ƒè¿«ä½¿å†…æ ¸ç«‹å³åˆ†é…çœŸå®çš„ç‰©ç†é¡µé¢ã€‚
- en: The `write()` operation then copies your string data into this buffer using
    optimized SIMD instructions when available. V8's string encoding machinery converts
    the UTF-8 JavaScript string into raw bytes. For ASCII characters, this is a straight
    copy. For multi-byte UTF-8 characters, the encoder has to carefully track byte
    boundaries to avoid splitting characters. This encoding happens in a tight C++
    loop that's been optimized to process multiple bytes per CPU cycle using vector
    instructions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶å `write()` æ“ä½œä¼šå°†ä½ çš„å­—ç¬¦ä¸²æ•°æ®å¤åˆ¶åˆ°è¿™ä¸ªç¼“å†²åŒºä¸­ï¼Œå½“å¯ç”¨æ—¶ä½¿ç”¨ä¼˜åŒ–çš„ SIMD æŒ‡ä»¤ã€‚V8 çš„å­—ç¬¦ä¸²ç¼–ç æœºåˆ¶å°† UTF-8 JavaScript
    å­—ç¬¦ä¸²è½¬æ¢ä¸ºåŸå§‹å­—èŠ‚ã€‚å¯¹äº ASCII å­—ç¬¦ï¼Œè¿™æ˜¯ä¸€ä¸ªç›´æ¥çš„å¤åˆ¶ã€‚å¯¹äºå¤šå­—èŠ‚ UTF-8 å­—ç¬¦ï¼Œç¼–ç å™¨å¿…é¡»ä»”ç»†è·Ÿè¸ªå­—èŠ‚è¾¹ç•Œï¼Œä»¥é¿å…åˆ†å‰²å­—ç¬¦ã€‚è¿™ç§ç¼–ç åœ¨ä¸€ä¸ªç´§å¯†çš„
    C++ å¾ªç¯ä¸­å‘ç”Ÿï¼Œè¯¥å¾ªç¯å·²è¢«ä¼˜åŒ–ï¼Œä»¥ä½¿ç”¨å‘é‡æŒ‡ä»¤åœ¨æ¯æ¬¡ CPU å‘¨æœŸä¸­å¤„ç†å¤šä¸ªå­—èŠ‚ã€‚
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That operation is blazingly fast because it doesn't need to allocate 5 bytes
    and copy data into them. It just creates a tiny new JavaScript object with a different
    offset and length that points back to the original 50MB `ArrayBuffer`. Now, what
    happens if we modify the view?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¸ªæ“ä½œéå¸¸å¿«ï¼Œå› ä¸ºå®ƒä¸éœ€è¦åˆ†é… 5 ä¸ªå­—èŠ‚å¹¶å°†æ•°æ®å¤åˆ¶è¿›å»ã€‚å®ƒåªæ˜¯åˆ›å»ºäº†ä¸€ä¸ªå¸¦æœ‰ä¸åŒåç§»é‡å’Œé•¿åº¦çš„å¾®å°æ–° JavaScript å¯¹è±¡ï¼Œè¯¥å¯¹è±¡æŒ‡å‘åŸå§‹çš„
    50MB `ArrayBuffer`ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬ä¿®æ”¹è¿™ä¸ªè§†å›¾ä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿ
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This single write operation just corrupted your original 50MB buffer. When
    you call `write()` on the slice, Node.js calculates the absolute position in the
    parent ArrayBuffer: slice''s base offset (9) plus the write position (0) equals
    byte position 9 in the parent''s memory. The string "99999" gets encoded to UTF-8
    bytes [0x39, 0x39, 0x39, 0x39, 0x39] and written directly into the parent buffer''s
    memory at positions 9-13\. There''s no copy-on-write mechanism, no protection,
    no warning. The write happens through a direct memory pointer operation in C++,
    bypassing all of JavaScript''s safety mechanisms. In production, I''ve seen this
    pattern corrupt binary protocol headers, overwrite critical metadata, and even
    expose sensitive data from one request to another.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å•ä¸ªå†™å…¥æ“ä½œå°±ç ´åäº†ä½ çš„åŸå§‹50MBç¼“å†²åŒºã€‚å½“ä½ å¯¹åˆ‡ç‰‡è°ƒç”¨`write()`æ—¶ï¼ŒNode.jsä¼šè®¡ç®—çˆ¶`ArrayBuffer`çš„ç»å¯¹ä½ç½®ï¼šåˆ‡ç‰‡çš„åŸºæœ¬åç§»é‡ï¼ˆ9ï¼‰åŠ ä¸Šå†™å…¥ä½ç½®ï¼ˆ0ï¼‰ç­‰äºçˆ¶å†…å­˜ä¸­çš„å­—èŠ‚ä½ç½®9ã€‚å­—ç¬¦ä¸²"99999"è¢«ç¼–ç ä¸ºUTF-8å­—èŠ‚[0x39,
    0x39, 0x39, 0x39, 0x39]ï¼Œå¹¶ç›´æ¥å†™å…¥çˆ¶ç¼“å†²åŒºçš„å†…å­˜ä½ç½®9-13ã€‚æ²¡æœ‰å†™æ—¶å¤åˆ¶æœºåˆ¶ï¼Œæ²¡æœ‰ä¿æŠ¤ï¼Œæ²¡æœ‰è­¦å‘Šã€‚å†™å…¥æ˜¯é€šè¿‡C++ä¸­çš„ç›´æ¥å†…å­˜æŒ‡é’ˆæ“ä½œå®Œæˆçš„ï¼Œç»•è¿‡äº†JavaScriptçš„æ‰€æœ‰å®‰å…¨æœºåˆ¶ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘è§è¿‡è¿™ç§æ¨¡å¼ç ´åäºŒè¿›åˆ¶åè®®å¤´ï¼Œè¦†ç›–å…³é”®å…ƒæ•°æ®ï¼Œç”šè‡³ä»ä¸€ä¸ªè¯·æ±‚æ³„éœ²æ•æ„Ÿæ•°æ®åˆ°å¦ä¸€ä¸ªè¯·æ±‚ã€‚
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Did you see that? We changed the `userIdSlice`, and it mutated the `massiveBuffer`.
    They are two Buffer objects that reference the exact same memory location. When
    you modify the data through one Buffer reference, the change is immediately visible
    through the other Buffer reference because they both point to the same underlying
    ArrayBuffer. The V8 documentation on `TypedArray` views, which `Buffer`s are based
    on, confirms this shared memory behavior is intentional and fundamental to their
    design.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çœ‹åˆ°äº†å—ï¼Ÿæˆ‘ä»¬æ”¹å˜äº†`userIdSlice`ï¼Œå®ƒä¿®æ”¹äº†`massiveBuffer`ã€‚å®ƒä»¬æ˜¯ä¸¤ä¸ªå¼•ç”¨ç›¸åŒå†…å­˜ä½ç½®çš„Bufferå¯¹è±¡ã€‚å½“ä½ é€šè¿‡ä¸€ä¸ªBufferå¼•ç”¨ä¿®æ”¹æ•°æ®æ—¶ï¼Œé€šè¿‡å¦ä¸€ä¸ªBufferå¼•ç”¨ç«‹å³å¯ä»¥çœ‹åˆ°æ›´æ”¹ï¼Œå› ä¸ºå®ƒä»¬éƒ½æŒ‡å‘ç›¸åŒçš„åº•å±‚`ArrayBuffer`ã€‚åŸºäº`TypedArray`è§†å›¾çš„V8æ–‡æ¡£ï¼Œ`Buffer`å°±æ˜¯åŸºäºè¿™ä¸ªè§†å›¾çš„ï¼Œç¡®è®¤è¿™ç§å…±äº«å†…å­˜è¡Œä¸ºæ˜¯æ•…æ„çš„ï¼Œå¹¶ä¸”æ˜¯å…¶è®¾è®¡çš„åŸºæœ¬åŸåˆ™ã€‚
- en: So what about `subarray()`? In current Node.js versions, `Buffer.prototype.subarray()`
    is effectively the same as `Buffer.prototype.slice()`. Both create a view into
    the same memory, not a copy. The Node.js documentation recommends `subarray()`
    for clarity when you want to signal that you're working within the `TypedArray`
    specification, as `subarray` is the standard `TypedArray` method for creating
    views.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆ`subarray()`å‘¢ï¼Ÿåœ¨å½“å‰çš„Node.jsç‰ˆæœ¬ä¸­ï¼Œ`Buffer.prototype.subarray()`å®é™…ä¸Šä¸`Buffer.prototype.slice()`ç›¸åŒã€‚ä¸¤è€…éƒ½åˆ›å»ºäº†å¯¹ç›¸åŒå†…å­˜çš„è§†å›¾ï¼Œè€Œä¸æ˜¯å‰¯æœ¬ã€‚Node.jsæ–‡æ¡£å»ºè®®åœ¨ä½ æƒ³è¡¨æ˜ä½ æ­£åœ¨`TypedArray`è§„èŒƒå†…å·¥ä½œæ—¶ä½¿ç”¨`subarray()`ï¼Œå› ä¸º`subarray`æ˜¯åˆ›å»ºè§†å›¾çš„æ ‡å‡†`TypedArray`æ–¹æ³•ã€‚
- en: â„¹ï¸Note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: â„¹ï¸æ³¨æ„
- en: '`Buffer.slice()` and `Buffer.subarray()` are functionally identical. Both create
    views, not copies. Use `subarray()` for consistency with TypedArray conventions.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.slice()`å’Œ`Buffer.subarray()`åœ¨åŠŸèƒ½ä¸Šç›¸åŒã€‚ä¸¤è€…éƒ½åˆ›å»ºè§†å›¾ï¼Œè€Œä¸æ˜¯å‰¯æœ¬ã€‚ä¸ºäº†ä¸`TypedArray`çº¦å®šä¿æŒä¸€è‡´ï¼Œè¯·ä½¿ç”¨`subarray()`ã€‚'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'That `subarray()` call creates a new Buffer object with just three properties
    that matter: a reference to `mainBuffer`''s ArrayBuffer, an offset of 1, and a
    length of 2\. The total cost is about 72 bytes on the V8 heap for the JavaScript
    object itself. No memory is allocated for the actual data. The view''s internal
    `[[ViewedArrayBuffer]]` slot points directly to the parent''s backing store. When
    you access `sub[0]`, V8 performs pointer arithmetic: it takes the base address
    of the parent''s memory, adds the view''s offset (1 byte), and reads from that
    location. This happens entirely in compiled machine code without any JavaScript
    overhead.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¸ª`subarray()`è°ƒç”¨åˆ›å»ºäº†ä¸€ä¸ªåªåŒ…å«ä¸‰ä¸ªé‡è¦å±æ€§çš„æ–°çš„Bufferå¯¹è±¡ï¼šå¯¹`mainBuffer`çš„`ArrayBuffer`çš„å¼•ç”¨ã€åç§»é‡ä¸º1ã€é•¿åº¦ä¸º2ã€‚æ€»æˆæœ¬å¤§çº¦æ˜¯V8å †ä¸Šçš„72å­—èŠ‚ç”¨äºJavaScriptå¯¹è±¡æœ¬èº«ã€‚æ²¡æœ‰ä¸ºå®é™…æ•°æ®åˆ†é…å†…å­˜ã€‚è§†å›¾çš„å†…éƒ¨`[[ViewedArrayBuffer]]`æ§½ç›´æ¥æŒ‡å‘çˆ¶çš„å­˜å‚¨ã€‚å½“ä½ è®¿é—®`sub[0]`æ—¶ï¼ŒV8æ‰§è¡ŒæŒ‡é’ˆè¿ç®—ï¼šå®ƒä»çˆ¶å†…å­˜çš„åŸºå€å¼€å§‹ï¼ŒåŠ ä¸Šè§†å›¾çš„åç§»é‡ï¼ˆ1å­—èŠ‚ï¼‰ï¼Œå¹¶ä»è¯¥ä½ç½®è¯»å–ã€‚è¿™å®Œå…¨åœ¨ç¼–è¯‘çš„æœºå™¨ä»£ç ä¸­å®Œæˆï¼Œæ²¡æœ‰ä»»ä½•JavaScriptå¼€é”€ã€‚
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The third character is `Buffer.from()`. This one is tricky because its behavior
    changes completely depending on the input type you provide.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ä¸ªå­—ç¬¦æ˜¯`Buffer.from()`ã€‚è¿™ä¸ªå‡½æ•°æœ‰ç‚¹æ£˜æ‰‹ï¼Œå› ä¸ºå®ƒçš„è¡Œä¸ºå®Œå…¨å–å†³äºä½ æä¾›çš„è¾“å…¥ç±»å‹ã€‚
- en: '`Buffer.from(string)`: **Allocates new memory** and copies the string data
    into it.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.from(string)`: **åˆ†é…æ–°çš„å†…å­˜**å¹¶å°†å­—ç¬¦ä¸²æ•°æ®å¤åˆ¶åˆ°å…¶ä¸­ã€‚'
- en: '`Buffer.from(array)`: **Allocates new memory** and copies the byte values.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.from(array)`: **åˆ†é…æ–°çš„å†…å­˜**å¹¶å¤åˆ¶å­—èŠ‚æ•°æ®ã€‚'
- en: '`Buffer.from(arrayBuffer)`: **Creates a VIEW** that shares memory with the
    provided `ArrayBuffer`. This is a zero-copy operation.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.from(arrayBuffer)`: **åˆ›å»ºä¸€ä¸ªä¸æä¾›çš„`ArrayBuffer`å…±äº«å†…å­˜çš„è§†å›¾**ã€‚è¿™æ˜¯ä¸€ä¸ªé›¶å¤åˆ¶æ“ä½œã€‚'
- en: '`Buffer.from(buffer)`: **Allocates new memory** and copies the data from the
    source buffer. This is a full copy!'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.from(buffer)`: **åˆ†é…æ–°çš„å†…å­˜** å¹¶ä»æºç¼“å†²åŒºå¤åˆ¶æ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ‹·è´ï¼'
- en: âš ï¸Warning
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸è­¦å‘Š
- en: '`Buffer.from(arrayBuffer)` creates a VIEW (shares memory), but `Buffer.from(buffer)`
    creates a COPY (new memory). This inconsistency is a common source of bugs. Always
    verify which behavior you''re getting based on your input type.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.from(arrayBuffer)` åˆ›å»ºä¸€ä¸ªè§†å›¾ï¼ˆå…±äº«å†…å­˜ï¼‰ï¼Œä½† `Buffer.from(buffer)` åˆ›å»ºä¸€ä¸ªæ‹·è´ï¼ˆæ–°å†…å­˜ï¼‰ã€‚è¿™ç§ä¸ä¸€è‡´æ€§æ˜¯å¸¸è§çš„é”™è¯¯æ¥æºã€‚å§‹ç»ˆæ ¹æ®æ‚¨çš„è¾“å…¥ç±»å‹éªŒè¯æ‚¨è·å¾—çš„è¡Œä¸ºã€‚'
- en: The distinction between `Buffer.from(arrayBuffer)` and `Buffer.from(buffer)`
    is a common source of bugs. The former is a zero-copy view, while the latter is
    a full-copy operation. The `TypedArray` view that silently corrupted our binary
    protocol taught me to never trust without measuring. We had a function that was
    sometimes passed an `ArrayBuffer` and sometimes a `Buffer`, and the subtle difference
    in `Buffer.from()` semantics was causing unexpected copies in our hot path, tanking
    performance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.from(arrayBuffer)` å’Œ `Buffer.from(buffer)` ä¹‹é—´çš„åŒºåˆ«æ˜¯å¸¸è§çš„é”™è¯¯æ¥æºã€‚å‰è€…æ˜¯ä¸€ä¸ªé›¶æ‹·è´è§†å›¾ï¼Œè€Œåè€…æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ‹·è´æ“ä½œã€‚é‚£ä¸ªé™é»˜åœ°ç ´åäº†æˆ‘ä»¬äºŒè¿›åˆ¶åè®®çš„
    `TypedArray` è§†å›¾æ•™ä¼šäº†æˆ‘ä¸è¦ä¸æµ‹é‡å°±ä¿¡ä»»ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå‡½æ•°ï¼Œæœ‰æ—¶ä¼šä¼ å…¥ä¸€ä¸ª `ArrayBuffer`ï¼Œæœ‰æ—¶ä¼šä¼ å…¥ä¸€ä¸ª `Buffer`ï¼Œè€Œ `Buffer.from()`
    è¯­ä¹‰çš„å¾®å¦™å·®å¼‚åœ¨æˆ‘ä»¬çš„çƒ­ç‚¹è·¯å¾„ä¸­å¯¼è‡´äº†æ„å¤–çš„æ‹·è´ï¼Œä»è€Œé™ä½äº†æ€§èƒ½ã€‚'
- en: Zero-Copy Operations
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›¶æ‹·è´æ“ä½œ
- en: The term "zero-copy" is misleadingly appealing. It sounds like achieving performance
    gains without any costs. You're not. There's a trade-off, and you need to understand
    it. A zero-copy operation means you are not copying the *data payload*. You are,
    however, still creating a new JavaScript object - the view itself. This object
    has a small memory footprint on the V8 heap, but its creation is orders of magnitude
    faster than allocating a new memory block and then iterating over the original
    data to copy it byte by byte.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: â€œé›¶æ‹·è´â€è¿™ä¸ªæœ¯è¯­å¬èµ·æ¥å¾ˆæœ‰å¸å¼•åŠ›ï¼Œä½†å…·æœ‰è¯¯å¯¼æ€§ã€‚å®ƒå¬èµ·æ¥åƒæ˜¯åœ¨ä¸ä»˜å‡ºä»»ä½•ä»£ä»·çš„æƒ…å†µä¸‹å®ç°æ€§èƒ½æå‡ã€‚äº‹å®å¹¶éå¦‚æ­¤ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªæƒè¡¡ï¼Œä½ éœ€è¦ç†è§£å®ƒã€‚é›¶æ‹·è´æ“ä½œæ„å‘³ç€ä½ ä¸ä¼šå¤åˆ¶
    *æ•°æ®è´Ÿè½½*ã€‚ç„¶è€Œï¼Œä½ ä»ç„¶åœ¨åˆ›å»ºä¸€ä¸ªæ–°çš„ JavaScript å¯¹è±¡â€”â€”è§†å›¾æœ¬èº«ã€‚è¿™ä¸ªå¯¹è±¡åœ¨ V8 å †ä¸Šæœ‰å¾ˆå°çš„å†…å­˜å ç”¨ï¼Œä½†å®ƒçš„åˆ›å»ºé€Ÿåº¦æ¯”åˆ†é…æ–°çš„å†…å­˜å—ç„¶åé€å­—èŠ‚è¿­ä»£åŸå§‹æ•°æ®æ¥å¤åˆ¶å®ƒå¿«å¾—å¤šã€‚
- en: Let's quantify this. Let's say we have a 10MB buffer and we want a 1KB chunk
    from it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é‡åŒ–ä¸€ä¸‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª 10MB çš„ç¼“å†²åŒºï¼Œæˆ‘ä»¬æƒ³è¦ä»ä¸­è·å– 1KB çš„å—ã€‚
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This allocation triggers a single `mmap()` syscall for 10,485,760 bytes. The
    kernel reserves virtual address space but doesn't allocate physical pages yet
    - that happens on first write through demand paging. Node.js tracks this allocation
    in its external memory accounting, adding 10MB to `process.memoryUsage().external`.
    V8's garbage collector is notified through `Isolate::AdjustAmountOfExternalAllocatedMemory()`,
    which influences when the next major GC cycle triggers. If external memory grows
    too fast, V8 will panic and force a synchronous GC, blocking your event loop for
    potentially hundreds of milliseconds.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåˆ†é…è§¦å‘äº†å•ä¸ª `mmap()` ç³»ç»Ÿè°ƒç”¨ï¼Œç”¨äº 10,485,760 å­—èŠ‚ã€‚å†…æ ¸ä¿ç•™äº†è™šæ‹Ÿåœ°å€ç©ºé—´ï¼Œä½†å°šæœªåˆ†é…ç‰©ç†é¡µé¢â€”â€”è¿™å‘ç”Ÿåœ¨ç¬¬ä¸€æ¬¡é€šè¿‡æŒ‰éœ€åˆ†é¡µå†™å…¥æ—¶ã€‚Node.js
    é€šè¿‡ `Isolate::AdjustAmountOfExternalAllocatedMemory()` é€šçŸ¥ V8 çš„åƒåœ¾æ”¶é›†å™¨ï¼Œè¿™ä¼šå½±å“ä¸‹ä¸€æ¬¡ä¸»è¦ GC
    å‘¨æœŸçš„è§¦å‘ã€‚å¦‚æœå¤–éƒ¨å†…å­˜å¢é•¿è¿‡å¿«ï¼ŒV8 å°†å´©æºƒå¹¶å¼ºåˆ¶è¿›è¡ŒåŒæ­¥ GCï¼Œè¿™å¯èƒ½ä¼šé˜»å¡ä½ çš„äº‹ä»¶å¾ªç¯æ•°ç™¾æ¯«ç§’ã€‚
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `subarray()` operation completes in nanoseconds. It allocates exactly 72
    bytes on the V8 heap for the new Buffer object and sets three fields: buffer pointer,
    offset (5000), and length (1024). No memory barrier, no cache invalidation, no
    TLB flush. The CPU can keep this entire operation in L1 cache. The performance
    counter shows ~0.007ms because that''s mostly the overhead of `console.time()`
    itself - the actual subarray operation takes less than 100 nanoseconds on modern
    CPUs.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`subarray()` æ“ä½œåœ¨çº³ç§’çº§åˆ«å®Œæˆã€‚å®ƒä¸ºæ–°çš„ Buffer å¯¹è±¡åœ¨ V8 å †ä¸Šåˆ†é…äº†æ­£å¥½ 72 å­—èŠ‚ï¼Œå¹¶è®¾ç½®äº†ä¸‰ä¸ªå­—æ®µï¼šç¼“å†²åŒºæŒ‡é’ˆã€åç§»é‡ï¼ˆ5000ï¼‰å’Œé•¿åº¦ï¼ˆ1024ï¼‰ã€‚æ²¡æœ‰å†…å­˜å±éšœï¼Œæ²¡æœ‰ç¼“å­˜å¤±æ•ˆï¼Œæ²¡æœ‰
    TLB æ¸…é™¤ã€‚CPU å¯ä»¥å°†æ•´ä¸ªæ“ä½œä¿æŒåœ¨ L1 ç¼“å­˜ä¸­ã€‚æ€§èƒ½è®¡æ•°å™¨æ˜¾ç¤º ~0.007msï¼Œå› ä¸ºé‚£ä¸»è¦æ˜¯ `console.time()` è‡ªèº«çš„å¼€é”€â€”â€”å®é™…çš„å­æ•°ç»„æ“ä½œåœ¨ç°ä»£
    CPU ä¸Šä¸åˆ° 100 çº³ç§’ã€‚'
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'On a modern Node.js install, the results are telling:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç°ä»£ Node.js å®‰è£…ä¸­ï¼Œç»“æœæ˜¯æœ‰è¯´æœåŠ›çš„ï¼š
- en: '`view creation`: `0.007ms`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`è§†å›¾åˆ›å»º`: `0.007ms`'
- en: '`copy creation`: `0.024ms`'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ‹·è´åˆ›å»º`: `0.024ms`'
- en: ğŸ’¡Tip
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡æç¤º
- en: Use `performance.timerify()` or `perf_hooks` module to accurately measure buffer
    operations in production. The `console.time()` method is convenient but less precise
    for sub-millisecond measurements.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `performance.timerify()` æˆ– `perf_hooks` æ¨¡å—åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å‡†ç¡®æµ‹é‡ç¼“å†²åŒºæ“ä½œã€‚`console.time()`
    æ–¹æ³•å¾ˆæ–¹ä¾¿ï¼Œä½†å¯¹äºäºšæ¯«ç§’çº§çš„æµ‹é‡æ¥è¯´ä¸å¤Ÿç²¾ç¡®ã€‚
- en: The cost of creating a view is effectively constant time, O(1). It doesn't matter
    if you're viewing 10 bytes or 10 megabytes; you're just creating a small JS object
    with some pointers and offsets. The cost of a copy, however, is linear time, O(n).
    It's directly proportional to the amount of data you're copying. For a 1MB chunk
    from a 100MB buffer, the view is still nearly instantaneous while the copy takes
    a measurable slice of a millisecond. In a hot path, this adds up. Our own telemetry
    has shown that replacing unnecessary copies with views in a critical parsing loop
    can cut CPU usage by 30%.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºè§†å›¾çš„æˆæœ¬å®é™…ä¸Šæ˜¯å¸¸æ•°æ—¶é—´ï¼ŒO(1)ã€‚æ— è®ºä½ æŸ¥çœ‹ 10 å­—èŠ‚è¿˜æ˜¯ 10 å…†å­—èŠ‚ï¼Œä½ åªæ˜¯åœ¨åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ä¸€äº›æŒ‡é’ˆå’Œåç§»çš„å°å‹ JS å¯¹è±¡ã€‚ç„¶è€Œï¼Œå¤åˆ¶çš„æˆæœ¬æ˜¯çº¿æ€§æ—¶é—´ï¼ŒO(n)ã€‚å®ƒä¸ä½ è¦å¤åˆ¶çš„æ•°æ®é‡æˆæ­£æ¯”ã€‚å¯¹äºä¸€ä¸ªä»
    100MB ç¼“å†²åŒºä¸­æå–çš„ 1MB æ•°æ®å—ï¼Œè§†å›¾ä»ç„¶å‡ ä¹æ˜¯ç¬æ—¶çš„ï¼Œè€Œå¤åˆ¶åˆ™éœ€è¦æ¶ˆè€—ä¸€ä¸ªå¯æµ‹é‡çš„æ¯«ç§’æ—¶é—´ã€‚åœ¨ä¸€ä¸ªçƒ­ç‚¹è·¯å¾„ä¸­ï¼Œè¿™ä¼šç´¯ç§¯èµ·æ¥ã€‚æˆ‘ä»¬è‡ªå·±çš„é¥æµ‹æ•°æ®æ˜¾ç¤ºï¼Œåœ¨å…³é”®è§£æå¾ªç¯ä¸­å°†ä¸å¿…è¦çš„å¤åˆ¶æ›¿æ¢ä¸ºè§†å›¾å¯ä»¥å‡å°‘
    CPU ä½¿ç”¨ç‡ 30%ã€‚
- en: But here's the trade-off, the common mistake that developers make. You hear
    "zero-copy" and think "faster." So you embark on an "optimization" pass, replacing
    copies with views wherever you see them. But what you're really doing is trading
    CPU cycles for memory management complexity. The view is fast because it doesn't
    have to manage its own memory; it simply borrows the parent's. This creates a
    strong reference that the garbage collector must respect. As long as your view
    is alive, the entire parent buffer is pinned in memory.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯è¿™é‡Œæœ‰æƒè¡¡ï¼Œå¼€å‘è€…å¸¸è§çš„é”™è¯¯ã€‚ä½ å¬åˆ°â€œé›¶å¤åˆ¶â€å°±ä¼šæƒ³åˆ°â€œæ›´å¿«â€ã€‚æ‰€ä»¥ä½ å¼€å§‹è¿›è¡Œâ€œä¼˜åŒ–â€éå†ï¼Œåœ¨çœ‹åˆ°å¤åˆ¶çš„åœ°æ–¹ç”¨è§†å›¾æ›¿æ¢å®ƒä»¬ã€‚ä½†ä½ å®é™…ä¸Šæ˜¯åœ¨ç”¨ CPU
    å‘¨æœŸæ¢å–å†…å­˜ç®¡ç†å¤æ‚æ€§ã€‚è§†å›¾ä¹‹æ‰€ä»¥å¿«ï¼Œæ˜¯å› ä¸ºå®ƒä¸éœ€è¦ç®¡ç†è‡ªå·±çš„å†…å­˜ï¼›å®ƒåªæ˜¯å€Ÿç”¨çˆ¶çº§çš„ã€‚è¿™åˆ›å»ºäº†ä¸€ä¸ªå¼ºå¼•ç”¨ï¼Œåƒåœ¾æ”¶é›†å™¨å¿…é¡»å°Šé‡ã€‚åªè¦ä½ çš„è§†å›¾æ˜¯æ´»è·ƒçš„ï¼Œæ•´ä¸ªçˆ¶ç¼“å†²åŒºå°±ä¼šåœ¨å†…å­˜ä¸­è¢«å›ºå®šã€‚
- en: 'This is the fundamental trade-off: you trade memory safety for speed. You are
    telling the runtime, "Trust me, I know what I''m doing. Keep this giant chunk
    of memory around because I need this tiny piece of it." The runtime will do exactly
    what you ask. And if you''re not careful, it will trust you all the way to an
    out-of-memory exception. The correct "optimization" is not to use views everywhere,
    but to understand when the cost of a small, explicit copy is infinitely cheaper
    than the memory cost of retaining a giant parent buffer.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åŸºæœ¬çš„æƒè¡¡ï¼šä½ ç”¨å†…å­˜å®‰å…¨æ€§æ¢å–é€Ÿåº¦ã€‚ä½ æ˜¯åœ¨å‘Šè¯‰è¿è¡Œæ—¶ï¼Œâ€œç›¸ä¿¡æˆ‘ï¼Œæˆ‘çŸ¥é“æˆ‘åœ¨åšä»€ä¹ˆã€‚ä¿ç•™è¿™å—å·¨å¤§çš„å†…å­˜å—ï¼Œå› ä¸ºæˆ‘éœ€è¦å®ƒçš„ä¸€å°éƒ¨åˆ†ã€‚â€è¿è¡Œæ—¶ä¼šå®Œå…¨æŒ‰ç…§ä½ çš„è¦æ±‚å»åšã€‚è€Œä¸”å¦‚æœä½ ä¸å°å¿ƒï¼Œå®ƒç”šè‡³ä¼šä¸€ç›´ä¿¡ä»»ä½ ç›´åˆ°å‡ºç°å†…å­˜ä¸è¶³å¼‚å¸¸ã€‚æ­£ç¡®çš„â€œä¼˜åŒ–â€ä¸æ˜¯åœ¨æ‰€æœ‰åœ°æ–¹éƒ½ä½¿ç”¨è§†å›¾ï¼Œè€Œæ˜¯è¦ç†è§£ä½•æ—¶å°è€Œæ˜ç¡®çš„å¤åˆ¶çš„æˆæœ¬è¿œè¿œä½äºä¿ç•™å¤§å‹çˆ¶ç¼“å†²åŒºçš„å†…å­˜æˆæœ¬ã€‚
- en: Buffers, TypedArrays, and the Memory They Share
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºã€ç±»å‹åŒ–æ•°ç»„ä»¥åŠå®ƒä»¬å…±äº«çš„å†…å­˜
- en: Okay, so we've established that `Buffer`s are Node's special sauce for handling
    binary data. But as we saw in the first chapter, they don't live in a vacuum.
    They're part of a bigger family called `TypedArray`s, and understanding this relationship
    is crucial.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œæ‰€ä»¥æˆ‘ä»¬å·²ç»ç¡®å®š `Buffer` æ˜¯ Node å¤„ç†äºŒè¿›åˆ¶æ•°æ®çš„ç‰¹æ®Šè°ƒæ–™ã€‚ä½†æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« ä¸­çœ‹åˆ°çš„ï¼Œå®ƒä»¬å¹¶ä¸æ˜¯å­¤ç«‹å­˜åœ¨çš„ã€‚å®ƒä»¬æ˜¯æ›´å¤§å®¶æ— `TypedArray`
    çš„ä¸€éƒ¨åˆ†ï¼Œç†è§£è¿™ç§å…³ç³»è‡³å…³é‡è¦ã€‚
- en: Since Node.js v3.0, the `Buffer` class is a direct subclass of `Uint8Array`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä» Node.js v3.0 ç‰ˆæœ¬ä»¥æ¥ï¼Œ`Buffer` ç±»æ˜¯ `Uint8Array` çš„ç›´æ¥å­ç±»ã€‚
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: And that's not just trivia for your next job interview. It's the golden ticket
    to interoperability. It means you can pass a Node `Buffer` to any modern API -
    in the browser, in WebAssembly, wherever - that expects a `Uint8Array`, and it
    just works.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ä»…ä»…æ˜¯ä½ ä¸‹æ¬¡é¢è¯•çš„ triviaã€‚è¿™æ˜¯äº’æ“ä½œæ€§çš„é‡‘ç¥¨ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥å°† Node `Buffer` ä¼ é€’ç»™ä»»ä½•æœŸæœ› `Uint8Array` çš„ç°ä»£
    API - åœ¨æµè§ˆå™¨ä¸­ã€åœ¨ WebAssembly ä¸­ã€åœ¨ä»»ä½•åœ°æ–¹ - å®ƒéƒ½èƒ½æ­£å¸¸å·¥ä½œã€‚
- en: 'The key concept is this: the raw slab of memory itself is an `ArrayBuffer`.
    `Buffer`, `Uint8Array`, `Int32Array`, etc., are all just different **views** you
    can place over that same raw memory. Think of the `ArrayBuffer` as a raw hunk
    of steel. A `Buffer` is a stencil that lets you see it as a sequence of individual
    bytes. An `Int32Array` is a different stencil that groups those bytes into 4-byte
    chunks and shows you 32-bit numbers.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®æ¦‚å¿µæ˜¯è¿™æ ·çš„ï¼šåŸå§‹çš„å†…å­˜å—æœ¬èº«æ˜¯ä¸€ä¸ª `ArrayBuffer`ã€‚`Buffer`ã€`Uint8Array`ã€`Int32Array` ç­‰ï¼Œéƒ½æ˜¯ä½ å¯ä»¥æ”¾ç½®åœ¨ç›¸åŒåŸå§‹å†…å­˜ä¸Šçš„ä¸åŒ
    **è§†å›¾**ã€‚å°† `ArrayBuffer` æƒ³è±¡æˆä¸€å—åŸå§‹çš„é’¢é“ã€‚ä¸€ä¸ª `Buffer` æ˜¯ä¸€ä¸ªæ¨¡æ¿ï¼Œè®©ä½ å¯ä»¥çœ‹åˆ°å®ƒä½œä¸ºä¸€ä¸ªå­—èŠ‚åºåˆ—ã€‚ä¸€ä¸ª `Int32Array`
    æ˜¯ä¸€ä¸ªä¸åŒçš„æ¨¡æ¿ï¼Œå°†é‚£äº›å­—èŠ‚åˆ†ç»„ä¸º 4 å­—èŠ‚å—ï¼Œå¹¶æ˜¾ç¤º 32 ä½æ•°å­—ã€‚
- en: This is incredibly powerful. It's also how you can silently corrupt all your
    data without a single error being thrown.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éå¸¸å¼ºå¤§ã€‚è¿™ä¹Ÿæ˜¯ä½ å¯ä»¥åœ¨ä¸æŠ›å‡ºä»»ä½•é”™è¯¯çš„æƒ…å†µä¸‹é™é»˜åœ°æŸåæ‰€æœ‰æ•°æ®çš„æ–¹æ³•ã€‚
- en: Let's walk through the crime scene. Imagine we get a 12-byte message from the
    network.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹çœ‹çŠ¯ç½ªç°åœºã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬ä»ç½‘ç»œä¸­è·å–äº†ä¸€ä¸ª 12 å­—èŠ‚çš„æ¶ˆæ¯ã€‚
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, let's create a couple of views to work with this memory.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬åˆ›å»ºå‡ ä¸ªè§†å›¾æ¥å¤„ç†è¿™ä¸ªå†…å­˜ã€‚
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Everything's clean. The views point to different, non-overlapping parts of the
    same memory slab. But then a bug slips in - a classic off-by-one or a typo in
    an offset calculation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åˆ‡éƒ½å¾ˆå¹²å‡€ã€‚è§†å›¾æŒ‡å‘åŒä¸€å†…å­˜å—çš„ä¸åŒã€ä¸é‡å çš„éƒ¨åˆ†ã€‚ä½†éšåå‡ºç°äº†ä¸€ä¸ªé”™è¯¯â€”â€”ä¸€ä¸ªç»å…¸çš„â€œåç§»é‡é”™è¯¯â€æˆ–åç§»é‡è®¡ç®—ä¸­çš„æ‰“å­—é”™è¯¯ã€‚
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: And **boom**. Silent data corruption.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶å**å˜­**ã€‚é™é»˜çš„æ•°æ®æŸåã€‚
- en: Your code "worked." No exceptions, no crashes. But because your `buggyStringView`
    overlapped with your `intView`, writing that string just obliterated your integer.
    The bytes for "CANC" (`[0x43, 0x41, 0x4E, 0x43]`) are now squatting in the exact
    same memory where your number used to be. In production, this is the kind of bug
    that corrupts financial data or invalidates security tokens and takes weeks to
    track down.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çš„ä»£ç â€œå·¥ä½œâ€äº†ã€‚æ²¡æœ‰å¼‚å¸¸ï¼Œæ²¡æœ‰å´©æºƒã€‚ä½†æ˜¯å› ä¸ºä½ çš„`buggyStringView`ä¸ä½ çš„`intView`é‡å ï¼Œå†™å…¥é‚£ä¸ªå­—ç¬¦ä¸²å°±å½»åº•æ‘§æ¯äº†ä½ çš„æ•´æ•°ã€‚ä»£è¡¨â€œCANCâ€(`[0x43,
    0x41, 0x4E, 0x43]`)çš„å­—èŠ‚ç°åœ¨å æ®äº†ä½ çš„æ•°å­—æ›¾ç»æ‰€åœ¨çš„ç¡®åˆ‡å†…å­˜ä½ç½®ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™ç§ç±»å‹çš„é”™è¯¯ä¼šç ´åè´¢åŠ¡æ•°æ®æˆ–ä½¿å®‰å…¨ä»¤ç‰Œæ— æ•ˆï¼Œå¹¶éœ€è¦å‡ å‘¨æ—¶é—´æ‰èƒ½è¿½è¸ªåˆ°ã€‚
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The lesson here is simple and brutal: when you start creating multiple views
    over a single `ArrayBuffer`, you''ve fired the automated memory manager and hired
    yourself for the job. You are responsible for every offset and every length. Get
    it wrong, and you''re in for a nightmare of debugging data that looks right one
    millisecond and is garbage the next.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æ•™è®­ç®€å•è€Œæ®‹é…·ï¼šå½“ä½ å¼€å§‹å¯¹å•ä¸ª`ArrayBuffer`åˆ›å»ºå¤šä¸ªè§†å›¾æ—¶ï¼Œä½ å·²ç»å¯åŠ¨äº†è‡ªåŠ¨å†…å­˜ç®¡ç†å™¨ï¼Œå¹¶ä¸ºè‡ªå·±è˜è¯·äº†è¿™é¡¹å·¥ä½œã€‚ä½ å¿…é¡»å¯¹æ¯ä¸ªåç§»é‡å’Œæ¯ä¸ªé•¿åº¦è´Ÿè´£ã€‚æé”™äº†ï¼Œä½ å°†é™·å…¥è°ƒè¯•çœ‹ä¼¼æ­£ç¡®ä½†ä¸‹ä¸€æ¯«ç§’å°±å˜æˆåƒåœ¾æ•°æ®çš„å™©æ¢¦ã€‚
- en: ğŸš¨Caution
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš¨è­¦å‘Š
- en: Multiple TypedArray views over the same ArrayBuffer can silently corrupt each
    other's data. There are NO runtime checks for overlapping views. A single off-by-one
    error in offset calculation can corrupt critical data without throwing any errors.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŒä¸€`ArrayBuffer`ä¸Šåˆ›å»ºå¤šä¸ª`TypedArray`è§†å›¾å¯èƒ½ä¼šé™é»˜åœ°ç›¸äº’ç ´åæ•°æ®ã€‚æ²¡æœ‰å¯¹é‡å è§†å›¾è¿›è¡Œè¿è¡Œæ—¶æ£€æŸ¥ã€‚åœ¨åç§»é‡è®¡ç®—ä¸­å•ä¸ªâ€œåç§»é‡é”™è¯¯â€å°±å¯èƒ½å¯¼è‡´å…³é”®æ•°æ®æŸåè€Œä¸ä¼šæŠ›å‡ºä»»ä½•é”™è¯¯ã€‚
- en: When Views Share Memory (and When They Don't)
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å½“è§†å›¾å…±äº«å†…å­˜ï¼ˆä»¥åŠå®ƒä»¬ä¸å…±äº«å†…å­˜æ—¶ï¼‰
- en: 'By now, you should be healthily paranoid about shared memory. The rule of thumb
    is: **if an operation doesn''t explicitly say it "copies" or "allocates," assume
    it shares memory.** Let''s build a mental map of the common `Buffer` and `TypedArray`
    operations and put them into "View" (shares memory) or "Copy" (allocates new memory)
    buckets.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œä½ åº”è¯¥å¯¹å…±äº«å†…å­˜ä¿æŒå¥åº·çš„è­¦æƒ•ã€‚ä¸€èˆ¬æ¥è¯´ï¼š**å¦‚æœä¸€ä¸ªæ“ä½œæ²¡æœ‰æ˜ç¡®è¯´æ˜å®ƒâ€œå¤åˆ¶â€æˆ–â€œåˆ†é…â€ï¼Œå°±å‡è®¾å®ƒå…±äº«å†…å­˜**ã€‚è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå¸¸è§çš„`Buffer`å’Œ`TypedArray`æ“ä½œçš„æ€ç»´å¯¼å›¾ï¼Œå¹¶å°†å®ƒä»¬æ”¾å…¥â€œè§†å›¾â€ï¼ˆå…±äº«å†…å­˜ï¼‰æˆ–â€œå¤åˆ¶â€ï¼ˆåˆ†é…æ–°å†…å­˜ï¼‰ç±»åˆ«ä¸­ã€‚
- en: '**Operations that Create Views (Zero-Copy):**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºè§†å›¾çš„æ“ä½œï¼ˆé›¶å¤åˆ¶ï¼‰ï¼š**'
- en: '`Buffer.prototype.slice(start, end)`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.prototype.slice(start, end)`'
- en: '`Buffer.prototype.subarray(start, end)`'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.prototype.subarray(start, end)`'
- en: '`new Uint8Array(arrayBuffer, byteOffset, length)` (and all other `TypedArray`
    constructors that take an `ArrayBuffer`)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new Uint8Array(arrayBuffer, byteOffset, length)` (ä»¥åŠæ‰€æœ‰å…¶ä»–æ¥å—`ArrayBuffer`çš„`TypedArray`æ„é€ å‡½æ•°)'
- en: '`Buffer.from(arrayBuffer, byteOffset, length)`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.from(arrayBuffer, byteOffset, length)`'
- en: These are your high-performance, high-risk tools. They are incredibly fast for
    creating sub-sections of existing data for temporary processing. The key word
    here is *temporary*. If the view you create is short-lived and goes out of scope
    quickly, you get all the performance benefits without the memory retention risk.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯ä½ çš„é«˜æ€§èƒ½ã€é«˜é£é™©å·¥å…·ã€‚å®ƒä»¬åœ¨åˆ›å»ºç°æœ‰æ•°æ®çš„ä¸´æ—¶å­éƒ¨åˆ†è¿›è¡Œä¸´æ—¶å¤„ç†æ—¶é€Ÿåº¦æå¿«ã€‚è¿™é‡Œçš„å…³é”®è¯æ˜¯*ä¸´æ—¶*ã€‚å¦‚æœä½ åˆ›å»ºçš„è§†å›¾ç”Ÿå‘½å‘¨æœŸçŸ­ï¼Œå¾ˆå¿«å°±ä¼šè¶…å‡ºèŒƒå›´ï¼Œä½ å°†è·å¾—æ‰€æœ‰æ€§èƒ½ä¼˜åŠ¿ï¼Œè€Œä¸ä¼šæ‰¿æ‹…å†…å­˜ä¿ç•™é£é™©ã€‚
- en: '**Operations that Create Copies (Allocating New Memory):**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºå‰¯æœ¬çš„æ“ä½œï¼ˆåˆ†é…æ–°å†…å­˜ï¼‰ï¼š**'
- en: '`Buffer.alloc(size)`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.alloc(size)`'
- en: '`Buffer.from(string)`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.from(string)`'
- en: '`Buffer.from(array)`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.from(array)`'
- en: '`Buffer.from(buffer)`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.from(buffer)`'
- en: '`Buffer.prototype.copy()` (the method itself, which copies *into* an existing
    buffer)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer.prototype.copy()` (è¯¥æ–¹æ³•æœ¬èº«ï¼Œå®ƒå°†å‰¯æœ¬å¤åˆ¶åˆ°ç°æœ‰çš„ç¼“å†²åŒº)'
- en: '`Uint8Array.prototype.slice(start, end)` (Note the critical difference! `TypedArray.slice()`
    *copies*, whereas `Buffer.slice()` *views*.)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Uint8Array.prototype.slice(start, end)` (æ³¨æ„å…³é”®çš„åŒºåˆ«ï¼`TypedArray.slice()`ä¼šå¤åˆ¶ï¼Œè€Œ`Buffer.slice()`ä¼šåˆ›å»ºè§†å›¾ã€‚)'
- en: ğŸš¨Caution
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš¨è­¦å‘Š
- en: 'CRITICAL CONFUSION: `TypedArray.prototype.slice()` creates a COPY, but `Buffer.prototype.slice()`
    creates a VIEW. This is because Buffer overrides the TypedArray slice method.
    If you accidentally call the TypedArray version, you get opposite behavior!'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³é”®æ··æ·†**ï¼š`TypedArray.prototype.slice()`åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ï¼Œä½†`Buffer.prototype.slice()`åˆ›å»ºä¸€ä¸ªè§†å›¾ã€‚è¿™æ˜¯å› ä¸ºBufferè¦†ç›–äº†TypedArrayçš„åˆ‡ç‰‡æ–¹æ³•ã€‚å¦‚æœä½ æ„å¤–åœ°è°ƒç”¨äº†TypedArrayç‰ˆæœ¬ï¼Œä½ ä¼šå¾—åˆ°ç›¸åçš„è¡Œä¸ºï¼'
- en: This last point is a landmine. I've seen it burn senior engineers. Because `Buffer`
    is a `Uint8Array`, it inherits both methods, but Buffer overrides `slice()` to
    create views instead of copies. If you were to somehow call the `Uint8Array` prototype's
    slice method directly on a buffer (via `Uint8Array.prototype.slice.call(buf, ...)`),
    you'd get a copy instead of a view. This inconsistency between `Buffer.slice()`
    and `TypedArray.slice()` is a design quirk that can cost you your sanity. The
    Node.js team has gone to great lengths to make `Buffer`'s behavior internally
    consistent, but this fundamental difference with standard TypedArrays remains.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ç‚¹æ˜¯ä¸€ä¸ªé™·é˜±ã€‚æˆ‘è§è¿‡å®ƒçƒ§æ¯èµ„æ·±å·¥ç¨‹å¸ˆã€‚å› ä¸º`Buffer`æ˜¯`Uint8Array`ï¼Œå®ƒç»§æ‰¿äº†è¿™ä¸¤ç§æ–¹æ³•ï¼Œä½†`Buffer`è¦†ç›–äº†`slice()`ä»¥åˆ›å»ºè§†å›¾è€Œä¸æ˜¯å¤åˆ¶ã€‚å¦‚æœä½ ç›´æ¥åœ¨ç¼“å†²åŒºä¸Šè°ƒç”¨`Uint8Array`åŸå‹ä¸Šçš„`slice`æ–¹æ³•ï¼ˆé€šè¿‡`Uint8Array.prototype.slice.call(buf,
    ...)`ï¼‰ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªå¤åˆ¶è€Œä¸æ˜¯è§†å›¾ã€‚`Buffer.slice()`å’Œ`TypedArray.slice()`ä¹‹é—´çš„è¿™ç§ä¸ä¸€è‡´æ€§æ˜¯ä¸€ä¸ªå¯èƒ½å¯¼è‡´ä½ ç²¾ç¥å´©æºƒçš„è®¾è®¡æ€ªç™–ã€‚Node.jså›¢é˜Ÿå·²ç»åšäº†å¾ˆå¤šåŠªåŠ›æ¥ç¡®ä¿`Buffer`çš„è¡Œä¸ºåœ¨å†…éƒ¨ä¿æŒä¸€è‡´ï¼Œä½†ä¸æ ‡å‡†TypedArraysçš„æ ¹æœ¬åŒºåˆ«ä»ç„¶å­˜åœ¨ã€‚
- en: Let's look at a scenario where the distinction is crucial. You're reading a
    large file, say a 1GB video file, and you just need to parse the first 1KB for
    metadata.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªåŒºåˆ†è‡³å…³é‡è¦çš„åœºæ™¯ã€‚ä½ æ­£åœ¨è¯»å–ä¸€ä¸ªå¤§æ–‡ä»¶ï¼Œæ¯”å¦‚è¯´ä¸€ä¸ª1GBçš„è§†é¢‘æ–‡ä»¶ï¼Œè€Œä½ åªéœ€è¦è§£æå‰1KBçš„å…ƒæ•°æ®ã€‚
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: That `readFileSync` call just blocked your event loop while Node.js read 1GB
    from disk. Under the hood, libuv opens the file with `open()`, gets the file size
    with `fstat()`, allocates a buffer of that size, and then reads the entire file
    with a single `read()` syscall (or multiple reads for very large files). The entire
    1GB is loaded into a single contiguous ArrayBuffer. Your process's RSS just jumped
    by 1GB, and the OS might have even started swapping other processes to disk to
    make room. This synchronous operation can freeze your server for several seconds
    on slow disks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¸ª`readFileSync`è°ƒç”¨é˜»å¡äº†ä½ çš„äº‹ä»¶å¾ªç¯ï¼ŒåŒæ—¶Node.jsä»ç£ç›˜è¯»å–äº†1GBã€‚åœ¨åº•å±‚ï¼Œlibuvä½¿ç”¨`open()`æ‰“å¼€æ–‡ä»¶ï¼Œä½¿ç”¨`fstat()`è·å–æ–‡ä»¶å¤§å°ï¼Œåˆ†é…ç›¸åº”å¤§å°çš„ç¼“å†²åŒºï¼Œç„¶åä½¿ç”¨å•ä¸ª`read()`ç³»ç»Ÿè°ƒç”¨ï¼ˆæˆ–å¯¹äºéå¸¸å¤§çš„æ–‡ä»¶ï¼Œä½¿ç”¨å¤šä¸ªè¯»å–ï¼‰è¯»å–æ•´ä¸ªæ–‡ä»¶ã€‚æ•´ä¸ª1GBè¢«åŠ è½½åˆ°ä¸€ä¸ªå•ä¸€çš„è¿ç»­ArrayBufferä¸­ã€‚ä½ çš„è¿›ç¨‹çš„RSSå€¼å¢åŠ äº†1GBï¼Œæ“ä½œç³»ç»Ÿç”šè‡³å¯èƒ½å¼€å§‹å°†å…¶ä»–è¿›ç¨‹äº¤æ¢åˆ°ç£ç›˜ä¸Šä»¥è…¾å‡ºç©ºé—´ã€‚è¿™ç§åŒæ­¥æ“ä½œå¯ä»¥åœ¨æ…¢é€Ÿç£ç›˜ä¸Šå†»ç»“ä½ çš„æœåŠ¡å™¨å‡ ç§’é’Ÿã€‚
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This creates a 72-byte Buffer object that holds a reference to the entire 1GB
    ArrayBuffer. The view''s retained size is 1GB, but its shallow size is just 72
    bytes. If you pass this to a cache, a global variable, or any long-lived data
    structure, you''ve just created a memory leak. The garbage collector sees the
    reference chain: your cache â†’ metadataView â†’ videoBuffer''s ArrayBuffer, and concludes
    the entire 1GB must be kept alive. I''ve debugged production systems where hundreds
    of these tiny views collectively retained tens of gigabytes of memory.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ä¼šåˆ›å»ºä¸€ä¸ª72å­—èŠ‚çš„Bufferå¯¹è±¡ï¼Œå®ƒæŒæœ‰å¯¹æ•´ä¸ª1GB ArrayBufferçš„å¼•ç”¨ã€‚è§†å›¾çš„ä¿ç•™å¤§å°æ˜¯1GBï¼Œä½†å…¶æµ…å±‚å¤§å°ä»…ä¸º72å­—èŠ‚ã€‚å¦‚æœä½ å°†æ­¤ä¼ é€’ç»™ç¼“å­˜ã€å…¨å±€å˜é‡æˆ–ä»»ä½•é•¿æœŸæ•°æ®ç»“æ„ï¼Œä½ åˆšåˆšå°±åˆ›å»ºäº†ä¸€ä¸ªå†…å­˜æ³„æ¼ã€‚åƒåœ¾æ”¶é›†å™¨çœ‹åˆ°å¼•ç”¨é“¾ï¼šä½ çš„ç¼“å­˜
    â†’ å…ƒæ•°æ®è§†å›¾ â†’ è§†é¢‘ç¼“å†²åŒºçš„ArrayBufferï¼Œå¹¶å¾—å‡ºæ•´ä¸ª1GBå¿…é¡»ä¿æŒæ´»è·ƒçš„ç»“è®ºã€‚æˆ‘è°ƒè¯•è¿‡ç”Ÿäº§ç³»ç»Ÿï¼Œå…¶ä¸­æ•°ç™¾ä¸ªè¿™æ ·çš„å°è§†å›¾å…±åŒä¿ç•™äº†æ•°åGBçš„å†…å­˜ã€‚
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The correct approach here, if you need to hold onto that metadata for any length
    of time, is to perform a strategic copy.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ éœ€è¦é•¿æ—¶é—´ä¿ç•™è¿™äº›å…ƒæ•°æ®ï¼Œè¿™é‡Œæ­£ç¡®çš„åšæ³•æ˜¯è¿›è¡Œæˆ˜ç•¥æ€§çš„å¤åˆ¶ã€‚
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `Buffer.alloc(1024)` allocates exactly 1024 bytes from Node's buffer pool
    (since it's under 4KB). This memory is zeroed for security. The `copy()` operation
    then triggers a highly optimized `memcpy()` in C++ that can move data at several
    GB/s on modern hardware. The CPU's SIMD instructions copy 32 or 64 bytes per cycle,
    making this 1KB copy complete in microseconds. Most importantly, `metadataCopy`
    has its own independent ArrayBuffer with no reference to the original 1GB buffer.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.alloc(1024)`ä»Nodeçš„ç¼“å†²æ± ä¸­åˆ†é…äº†æ­£å¥½1024å­—èŠ‚ï¼ˆå› ä¸ºå®ƒçš„å°ºå¯¸å°äº4KBï¼‰ã€‚å‡ºäºå®‰å…¨è€ƒè™‘ï¼Œè¿™æ®µå†…å­˜è¢«æ¸…é›¶ã€‚ç„¶å`copy()`æ“ä½œè§¦å‘ä¸€ä¸ªé«˜åº¦ä¼˜åŒ–çš„C++ä¸­çš„`memcpy()`ï¼Œåœ¨ç°ä»£ç¡¬ä»¶ä¸Šå¯ä»¥ä»¥æ•°GB/sçš„é€Ÿåº¦ç§»åŠ¨æ•°æ®ã€‚CPUçš„SIMDæŒ‡ä»¤æ¯æ¬¡å¤åˆ¶32æˆ–64å­—èŠ‚ï¼Œè¿™ä½¿å¾—è¿™ä¸ª1KBçš„å¤åˆ¶åœ¨å¾®ç§’å†…å®Œæˆã€‚æœ€é‡è¦çš„æ˜¯ï¼Œ`metadataCopy`æœ‰ä¸€ä¸ªç‹¬ç«‹çš„ArrayBufferï¼Œæ²¡æœ‰å¼•ç”¨åŸå§‹çš„1GBç¼“å†²åŒºã€‚'
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ğŸ’¡Tip
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡æç¤º
- en: 'Rule of thumb: Use views for temporary processing within a function. Use copies
    for any data that needs to be stored, cached, or passed to async operations. The
    small CPU cost of copying prevents massive memory leaks.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ç»éªŒæ³•åˆ™ï¼šåœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨è§†å›¾è¿›è¡Œä¸´æ—¶å¤„ç†ã€‚å¯¹äºéœ€è¦å­˜å‚¨ã€ç¼“å­˜æˆ–ä¼ é€’ç»™å¼‚æ­¥æ“ä½œçš„æ•°æ®ï¼Œä½¿ç”¨å¤åˆ¶ã€‚å¤åˆ¶çš„å¾®å°CPUæˆæœ¬å¯ä»¥é˜²æ­¢å·¨å¤§çš„å†…å­˜æ³„æ¼ã€‚
- en: This decision framework - "Is this data short-lived or long-lived?" - is the
    key to wielding views and copies effectively. For temporary, in-function processing,
    views are your best friend. For data that needs to be stored, cached, or passed
    between different parts of your application, an explicit copy is your insurance
    policy against massive memory leaks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå†³ç­–æ¡†æ¶â€”â€”â€œè¿™äº›æ•°æ®æ˜¯çŸ­æœŸå­˜åœ¨è¿˜æ˜¯é•¿æœŸå­˜åœ¨ï¼Ÿâ€æ˜¯æœ‰æ•ˆä½¿ç”¨è§†å›¾å’Œå‰¯æœ¬çš„å…³é”®ã€‚å¯¹äºä¸´æ—¶å‡½æ•°å†…å¤„ç†ï¼Œè§†å›¾æ˜¯ä½ çš„æœ€ä½³é€‰æ‹©ã€‚å¯¹äºéœ€è¦å­˜å‚¨ã€ç¼“å­˜æˆ–åœ¨åº”ç”¨ç¨‹åºçš„ä¸åŒéƒ¨åˆ†ä¹‹é—´ä¼ é€’çš„æ•°æ®ï¼Œæ˜¾å¼å¤åˆ¶æ˜¯ä½ çš„ä¿é™©æ”¿ç­–ï¼Œä»¥é˜²å¤§è§„æ¨¡å†…å­˜æ³„æ¼ã€‚
- en: Copy Semantics and Buffer.copy()
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤åˆ¶è¯­ä¹‰å’Œ Buffer.copy()
- en: So, we've established that sometimes you absolutely need a copy. The primary
    tool for this in Node.js is `Buffer.prototype.copy()`. It's a low-level, high-performance
    method designed to be the `memcpy` of the JavaScript world. Its signature is `buf.copy(targetBuffer,
    targetStart, sourceStart, sourceEnd)`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å·²ç»ç¡®å®šæœ‰æ—¶ä½ ç»å¯¹éœ€è¦å‰¯æœ¬ã€‚åœ¨ Node.js ä¸­ï¼Œè¿™ä¸ªä¸»è¦å·¥å…·æ˜¯ `Buffer.prototype.copy()`ã€‚å®ƒæ˜¯ä¸€ä¸ªä½çº§ã€é«˜æ€§èƒ½çš„æ–¹æ³•ï¼Œæ—¨åœ¨æˆä¸º
    JavaScript ä¸–ç•Œçš„ `memcpy`ã€‚å®ƒçš„ç­¾åæ˜¯ `buf.copy(targetBuffer, targetStart, sourceStart,
    sourceEnd)`ã€‚
- en: It's important to note that `copy()` writes into an *existing* `targetBuffer`.
    You must allocate the destination buffer yourself before you call it. This gives
    you fine-grained control but also adds a step to the process.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œ`copy()` å°†å†™å…¥ä¸€ä¸ª *ç°æœ‰* çš„ `targetBuffer`ã€‚åœ¨è°ƒç”¨å®ƒä¹‹å‰ï¼Œä½ å¿…é¡»è‡ªå·±åˆ†é…ç›®æ ‡ç¼“å†²åŒºã€‚è¿™ç»™äº†ä½ ç»†ç²’åº¦çš„æ§åˆ¶ï¼Œä½†ä¹Ÿå¢åŠ äº†å¤„ç†æ­¥éª¤ã€‚
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `Buffer.from(string)` encodes the 26-character alphabet into 26 bytes of
    UTF-8 (all ASCII, so one byte per character). Node allocates this from its buffer
    pool since it's under 4KB. The `Buffer.alloc(10)` creates another small buffer,
    also from the pool but from a different offset. These two buffers might actually
    be slices of the same underlying 8KB pool slab, but they're non-overlapping regions
    with independent lifecycles.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.from(string)` å°† 26 ä¸ªå­—ç¬¦çš„å­—æ¯è¡¨ç¼–ç ä¸º 26 å­—èŠ‚çš„ UTF-8ï¼ˆå…¨éƒ¨æ˜¯ ASCIIï¼Œæ‰€ä»¥æ¯ä¸ªå­—ç¬¦ä¸€ä¸ªå­—èŠ‚ï¼‰ã€‚Node
    ä»å…¶ç¼“å†²åŒºæ± ä¸­åˆ†é…å®ƒï¼Œå› ä¸ºå®ƒçš„å°ºå¯¸å°äº 4KBã€‚`Buffer.alloc(10)` åˆ›å»ºå¦ä¸€ä¸ªå°ç¼“å†²åŒºï¼Œä¹Ÿæ¥è‡ªæ± ï¼Œä½†æ¥è‡ªä¸åŒçš„åç§»é‡ã€‚è¿™ä¸¤ä¸ªç¼“å†²åŒºå®é™…ä¸Šå¯èƒ½æ˜¯åŒä¸€
    8KB æ± ç‰‡çš„ä¸åŒåˆ‡ç‰‡ï¼Œä½†å®ƒä»¬æ˜¯éé‡å çš„åŒºåŸŸï¼Œå…·æœ‰ç‹¬ç«‹çš„ç”Ÿå‘½å‘¨æœŸã€‚'
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This `copy()` operation resolves to a single `memcpy(target_ptr + 0, source_ptr
    + 0, 10)` call in C++. Modern CPUs optimize this with SIMD instructions, moving
    multiple bytes per cycle. The operation completes in nanoseconds for such small
    buffers. The data is physically duplicated - changes to `source` won't affect
    `target` and vice versa.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª `copy()` æ“ä½œåœ¨ C++ ä¸­ä¼šè§£æä¸ºå•ä¸ª `memcpy(target_ptr + 0, source_ptr + 0, 10)` è°ƒç”¨ã€‚ç°ä»£
    CPU ä½¿ç”¨ SIMD æŒ‡ä»¤è¿›è¡Œä¼˜åŒ–ï¼Œæ¯æ¬¡å¾ªç¯ç§»åŠ¨å¤šä¸ªå­—èŠ‚ã€‚å¯¹äºå¦‚æ­¤å°çš„ç¼“å†²åŒºï¼Œæ“ä½œå¯ä»¥åœ¨çº³ç§’å†…å®Œæˆã€‚æ•°æ®åœ¨ç‰©ç†ä¸Šæ˜¯å¤åˆ¶çš„â€”â€”å¯¹ `source` çš„æ›´æ”¹ä¸ä¼šå½±å“
    `target`ï¼Œåä¹‹äº¦ç„¶ã€‚
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The performance of `Buffer.copy()` is heavily optimized in Node's C++ core.
    For copying data between buffers, it will almost always be faster than any manual,
    byte-by-byte loop you could write in JavaScript. Memory profiling results show
    that the time taken is directly proportional to the number of bytes copied, and
    the constant factor is very low.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Node çš„ C++ å†…æ ¸å¯¹ `Buffer.copy()` çš„æ€§èƒ½è¿›è¡Œäº†å¤§é‡ä¼˜åŒ–ã€‚å¯¹äºåœ¨ç¼“å†²åŒºä¹‹é—´å¤åˆ¶æ•°æ®ï¼Œå®ƒå‡ ä¹æ€»æ˜¯æ¯”ä½ åœ¨ JavaScript ä¸­ç¼–å†™çš„ä»»ä½•æ‰‹åŠ¨ã€é€å­—èŠ‚å¾ªç¯è¦å¿«ã€‚å†…å­˜åˆ†æç»“æœæ˜¾ç¤ºï¼Œæ‰€éœ€æ—¶é—´ä¸å¤åˆ¶çš„å­—èŠ‚æ•°æˆæ­£æ¯”ï¼Œè€Œå¸¸æ•°å› å­éå¸¸ä½ã€‚
- en: 'However, there''s a more convenient way to create a copy that many people reach
    for: `Buffer.from(buffer)`. As we touched on earlier, this specific overload of
    `Buffer.from()` is explicitly a copy operation.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæœ‰ä¸€ç§æ›´æ–¹ä¾¿çš„æ–¹æ³•æ¥åˆ›å»ºå‰¯æœ¬ï¼Œè®¸å¤šäººéƒ½ä¼šé€‰æ‹©ï¼š`Buffer.from(buffer)`ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œè¿™ä¸ªç‰¹å®šçš„ `Buffer.from()`
    é‡è½½æ˜ç¡®æ˜¯ä¸€ä¸ªå¤åˆ¶æ“ä½œã€‚
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We've talked about `Buffer.from` too many times now, but - the `Buffer.from(buffer)`
    constructor is deceptive in its simplicity. Internally, it allocates a new ArrayBuffer
    of the exact same size as the original (28 bytes here), then performs a `memcpy()`
    of the entire contents. This happens in Node's C++ layer through the `node::Buffer::Copy()`
    function. The new buffer is completely independent - it has its own backing store
    with no references to the original. This is crucial for memory isolation and preventing
    the retention issues we've been discussing.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»å¤šæ¬¡è®¨è®ºäº† `Buffer.from`ï¼Œä½†â€”â€”`Buffer.from(buffer)` æ„é€ å‡½æ•°åœ¨å…¶ç®€å•æ€§ä¸Šå…·æœ‰æ¬ºéª—æ€§ã€‚å†…éƒ¨ï¼Œå®ƒåˆ†é…äº†ä¸€ä¸ªä¸åŸå§‹ç¼“å†²åŒºå¤§å°å®Œå…¨ç›¸åŒçš„æ–°
    ArrayBufferï¼ˆè¿™é‡Œä¸º 28 å­—èŠ‚ï¼‰ï¼Œç„¶åæ‰§è¡Œæ•´ä¸ªå†…å®¹çš„ `memcpy()`ã€‚è¿™å‘ç”Ÿåœ¨ Node çš„ C++ å±‚ï¼Œé€šè¿‡ `node::Buffer::Copy()`
    å‡½æ•°ã€‚æ–°çš„ç¼“å†²åŒºæ˜¯å®Œå…¨ç‹¬ç«‹çš„â€”â€”å®ƒæœ‰è‡ªå·±çš„åç«¯å­˜å‚¨ï¼Œæ²¡æœ‰å¯¹åŸå§‹ç¼“å†²åŒºçš„å¼•ç”¨ã€‚è¿™å¯¹äºå†…å­˜éš”ç¦»å’Œé˜²æ­¢æˆ‘ä»¬ä¸€ç›´åœ¨è®¨è®ºçš„ä¿ç•™é—®é¢˜è‡³å…³é‡è¦ã€‚
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Internally, `Buffer.from(buffer)` is essentially doing an `alloc` and a `copy`
    for you. It's syntactic sugar for the two-step process. In most cases, the performance
    difference is negligible, and the convenience of a one-liner often wins. However,
    if you are in an extremely hot path where you need to reuse an existing destination
    buffer to avoid allocation overhead (a technique called buffer pooling), then
    using `Buffer.copy()` directly is the way to go.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å†…éƒ¨ï¼Œ`Buffer.from(buffer)`åŸºæœ¬ä¸Šä¸ºä½ æ‰§è¡Œäº†ä¸€ä¸ª`alloc`å’Œä¸€ä¸ª`copy`ã€‚è¿™æ˜¯ä¸¤æ­¥è¿‡ç¨‹çš„è¯­æ³•ç³–ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ€§èƒ½å·®å¼‚æ˜¯å¯ä»¥å¿½ç•¥ä¸è®¡çš„ï¼Œä¸€è¡Œä»£ç çš„ä¾¿åˆ©æ€§é€šå¸¸æ›´èƒœä¸€ç­¹ã€‚ç„¶è€Œï¼Œå¦‚æœä½ å¤„äºä¸€ä¸ªæç«¯çš„çƒ­è·¯å¾„ä¸­ï¼Œéœ€è¦é‡ç”¨ç°æœ‰çš„ç›®æ ‡ç¼“å†²åŒºä»¥é¿å…åˆ†é…å¼€é”€ï¼ˆä¸€ç§ç§°ä¸ºç¼“å†²åŒºæ± åŒ–çš„æŠ€æœ¯ï¼‰ï¼Œé‚£ä¹ˆç›´æ¥ä½¿ç”¨`Buffer.copy()`æ˜¯æœ€ä½³é€‰æ‹©ã€‚
- en: 'Knowing *when* to copy is the art. The science is knowing *how*. The rule is
    simple: if the data needs to outlive its original, massive parent buffer, you
    must give it a new home. Allocate a new buffer of the exact size you need and
    copy the data into it. This breaks the link to the parent, allowing the garbage
    collector to do its job.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ¥é“ä½•æ—¶å¤åˆ¶æ˜¯ä¸€ç§è‰ºæœ¯ã€‚ç§‘å­¦åœ¨äºçŸ¥é“å¦‚ä½•åšã€‚è§„åˆ™å¾ˆç®€å•ï¼šå¦‚æœæ•°æ®éœ€è¦æ¯”å…¶åŸå§‹çš„ã€åºå¤§çš„çˆ¶ç¼“å†²åŒºå­˜æ´»æ›´ä¹…ï¼Œä½ å¿…é¡»ç»™å®ƒä¸€ä¸ªæ–°çš„å®¶ã€‚åˆ†é…ä¸€ä¸ªæ‰€éœ€çš„ç¡®åˆ‡å¤§å°çš„æ–°çš„ç¼“å†²åŒºï¼Œå¹¶å°†æ•°æ®å¤åˆ¶åˆ°å…¶ä¸­ã€‚è¿™æ‰“ç ´äº†ä¸çˆ¶ç¼“å†²åŒºçš„é“¾æ¥ï¼Œå…è®¸åƒåœ¾å›æ”¶å™¨å®Œæˆå…¶å·¥ä½œã€‚
- en: ğŸ“ŒImportant
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“Œé‡è¦
- en: '`Buffer.copy()` requires a pre-allocated target buffer. Common pattern: `const
    copy = Buffer.alloc(size); source.copy(copy, 0, start, end);`. For convenience,
    use `Buffer.from(source.subarray(start, end))` to create a copy in one line.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.copy()`éœ€è¦ä¸€ä¸ªé¢„å…ˆåˆ†é…çš„ç›®æ ‡ç¼“å†²åŒºã€‚å¸¸è§æ¨¡å¼ï¼š`const copy = Buffer.alloc(size); source.copy(copy,
    0, start, end);`ã€‚ä¸ºäº†æ–¹ä¾¿ï¼Œå¯ä»¥ä½¿ç”¨`Buffer.from(source.subarray(start, end))`åœ¨ä¸€è¡Œä¸­åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ã€‚'
- en: 'It''s the solution we eventually implemented for our log parser. Instead of
    storing the `slice`, we did this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬æœ€ç»ˆä¸ºæˆ‘ä»¬çš„æ—¥å¿—è§£æå™¨å®ç°çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬ä¸æ˜¯å­˜å‚¨`slice`ï¼Œè€Œæ˜¯è¿™æ ·åšï¼š
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This pattern costs us 16 bytes of allocation plus a few nanoseconds for the
    `memcpy()`. The `Buffer.alloc(16)` gets memory from Node''s buffer pool (it''s
    under 4KB), and the memory is zeroed for security. The `copy()` operation then
    moves exactly 16 bytes from the source. The crucial difference: `sessionId` has
    its own ArrayBuffer with no reference to `logBuffer`. When this function returns
    and `logBuffer` goes out of scope, the entire multi-megabyte buffer can be immediately
    garbage collected. Your heap profiler will show 16-byte buffers with 16-byte retained
    sizes - exactly what you''d expect.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¨¡å¼ä½¿æˆ‘ä»¬åˆ†é…äº†16ä¸ªå­—èŠ‚çš„å†…å­˜ï¼Œå¹¶ä¸”`memcpy()`æ“ä½œéœ€è¦å‡ çº³ç§’ã€‚`Buffer.alloc(16)`ä»Nodeçš„ç¼“å†²æ± ï¼ˆå°äº4KBï¼‰ä¸­è·å–å†…å­˜ï¼Œå¹¶ä¸”å‡ºäºå®‰å…¨è€ƒè™‘ï¼Œå†…å­˜è¢«åˆå§‹åŒ–ä¸ºé›¶ã€‚ç„¶å`copy()`æ“ä½œä»æºä¸­ç²¾ç¡®åœ°ç§»åŠ¨16ä¸ªå­—èŠ‚ã€‚å…³é”®çš„åŒºåˆ«æ˜¯`sessionId`æœ‰ä¸€ä¸ªè‡ªå·±çš„ArrayBufferï¼Œæ²¡æœ‰å¯¹`logBuffer`çš„å¼•ç”¨ã€‚å½“è¿™ä¸ªå‡½æ•°è¿”å›å¹¶ä¸”`logBuffer`è¶…å‡ºä½œç”¨åŸŸæ—¶ï¼Œæ•´ä¸ªå¤šå…†å­—èŠ‚çš„ç¼“å†²åŒºå¯ä»¥ç«‹å³è¢«åƒåœ¾å›æ”¶ã€‚ä½ çš„å †åˆ†æå™¨å°†æ˜¾ç¤º16å­—èŠ‚çš„ç¼“å†²åŒºï¼Œä¿ç•™å¤§å°æ­£å¥½æ˜¯16å­—èŠ‚â€”â€”è¿™æ­£æ˜¯ä½ æ‰€æœŸæœ›çš„ã€‚
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: âš ï¸Warning
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸è­¦å‘Š
- en: Never use `Buffer.allocUnsafe()` for copies that might contain sensitive data.
    The uninitialized memory could expose passwords, tokens, or other secrets from
    previously freed buffers. Always use `Buffer.alloc()` for security-critical code.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æ°¸è¿œä¸è¦åœ¨å¯èƒ½åŒ…å«æ•æ„Ÿæ•°æ®çš„å¤åˆ¶æ“ä½œä¸­ä½¿ç”¨`Buffer.allocUnsafe()`ã€‚æœªåˆå§‹åŒ–çš„å†…å­˜å¯èƒ½ä¼šæš´éœ²æ¥è‡ªå…ˆå‰é‡Šæ”¾çš„ç¼“å†²åŒºçš„å¯†ç ã€ä»¤ç‰Œæˆ–å…¶ä»–ç§˜å¯†ã€‚å§‹ç»ˆä½¿ç”¨`Buffer.alloc()`æ¥ç¡®ä¿å®‰å…¨å…³é”®ä»£ç ã€‚
- en: This one-line change from `slice` to `alloc`+`copy` saved us gigabytes of RAM.
    It might seem less "efficient" on the surface because it's doing more work (allocating
    and copying), but in the grand scheme of the system's health, it was infinitely
    more efficient.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ä»`slice`åˆ°`alloc`+`copy`çš„ä¸€è¡Œæ›´æ”¹ä¸ºæˆ‘ä»¬èŠ‚çœäº†æ•°GBçš„RAMã€‚ä»è¡¨é¢ä¸Šçœ‹ï¼Œå®ƒå¯èƒ½çœ‹èµ·æ¥ä¸å¤ªâ€œé«˜æ•ˆâ€ï¼Œå› ä¸ºå®ƒåšäº†æ›´å¤šçš„å·¥ä½œï¼ˆåˆ†é…å’Œå¤åˆ¶ï¼‰ï¼Œä½†åœ¨æ•´ä¸ªç³»ç»Ÿå¥åº·çš„å¤§èƒŒæ™¯ä¸‹ï¼Œå®ƒè¦é«˜æ•ˆå¾—å¤šã€‚
- en: SharedArrayBuffer and Cross-Thread Views
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SharedArrayBufferå’Œè·¨çº¿ç¨‹è§†å›¾
- en: The plot thickens when we introduce Node.js worker threads. For a long time,
    JavaScript was single-threaded. If you wanted to do CPU-intensive work, you'd
    block the main event loop, and your application's performance would grind to a
    halt. Worker threads changed the game, allowing for true parallelism. But how
    do you share data between threads without expensive serialization and copying?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å¼•å…¥Node.jså·¥ä½œçº¿ç¨‹æ—¶ï¼Œæƒ…å†µå˜å¾—æ›´åŠ å¤æ‚ã€‚é•¿æœŸä»¥æ¥ï¼ŒJavaScriptéƒ½æ˜¯å•çº¿ç¨‹çš„ã€‚å¦‚æœä½ æƒ³è¦è¿›è¡ŒCPUå¯†é›†å‹å·¥ä½œï¼Œä½ ä¼šé˜»å¡ä¸»äº‹ä»¶å¾ªç¯ï¼Œä½ çš„åº”ç”¨ç¨‹åºçš„æ€§èƒ½ä¼šé™è‡³åœé¡¿ã€‚å·¥ä½œçº¿ç¨‹æ”¹å˜äº†æ¸¸æˆè§„åˆ™ï¼Œå…è®¸çœŸæ­£çš„å¹¶è¡Œæ€§ã€‚ä½†æ˜¯ï¼Œä½ å¦‚ä½•åœ¨ä¸éœ€è¦æ˜‚è´µçš„åºåˆ—åŒ–å’Œå¤åˆ¶çš„æƒ…å†µä¸‹åœ¨çº¿ç¨‹ä¹‹é—´å…±äº«æ•°æ®ï¼Ÿ
- en: The answer is `SharedArrayBuffer` (SAB). A regular `ArrayBuffer` cannot be accessed
    by multiple threads. If you pass one to a worker, a copy is made. A `SharedArrayBuffer`,
    however, is a special type of `ArrayBuffer` whose underlying memory block can
    be referenced and manipulated by multiple threads simultaneously.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆæ˜¯ `SharedArrayBuffer` (SAB)ã€‚æ™®é€šçš„ `ArrayBuffer` ä¸èƒ½è¢«å¤šä¸ªçº¿ç¨‹è®¿é—®ã€‚å¦‚æœä½ ä¼ é€’ä¸€ä¸ªç»™å·¥ä½œçº¿ç¨‹ï¼Œå°±ä¼šåˆ›å»ºä¸€ä¸ªå‰¯æœ¬ã€‚ç„¶è€Œï¼Œ`SharedArrayBuffer`
    æ˜¯ä¸€ç§ç‰¹æ®Šçš„ `ArrayBuffer` ç±»å‹ï¼Œå…¶åº•å±‚çš„å†…å­˜å—å¯ä»¥è¢«å¤šä¸ªçº¿ç¨‹åŒæ—¶å¼•ç”¨å’Œæ“ä½œã€‚
- en: âš ï¸Warning
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸è­¦å‘Š
- en: '`SharedArrayBuffer` was temporarily disabled in browsers (2018-2020) due to
    Spectre vulnerabilities. While re-enabled with security mitigations, it requires
    careful handling. In Node.js, always use `Atomics` operations to prevent race
    conditions and data corruption in multi-threaded scenarios.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº Spectre æ¼æ´ï¼Œ`SharedArrayBuffer` åœ¨æµè§ˆå™¨ä¸­ï¼ˆ2018-2020å¹´ï¼‰è¢«æš‚æ—¶ç¦ç”¨ã€‚è™½ç„¶é‡æ–°å¯ç”¨å¹¶åŠ å…¥äº†å®‰å…¨ç¼“è§£æªæ–½ï¼Œä½†ä»éœ€è¦è°¨æ…å¤„ç†ã€‚åœ¨
    Node.js ä¸­ï¼Œå§‹ç»ˆä½¿ç”¨ `Atomics` æ“ä½œæ¥é˜²æ­¢å¤šçº¿ç¨‹åœºæ™¯ä¸­çš„ç«æ€æ¡ä»¶å’Œæ•°æ®æŸåã€‚
- en: This is where our understanding of views becomes strong. You can create a `SharedArrayBuffer`
    on the main thread, pass it to a worker thread, and then both threads can create
    `TypedArray` or `Buffer` views over that *same block of memory*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬å¯¹è§†å›¾ç†è§£å˜å¾—å¼ºå¤§çš„åœ°æ–¹ã€‚ä½ å¯ä»¥åœ¨ä¸»çº¿ç¨‹ä¸Šåˆ›å»ºä¸€ä¸ª `SharedArrayBuffer`ï¼Œå°†å…¶ä¼ é€’ç»™å·¥ä½œçº¿ç¨‹ï¼Œç„¶åä¸¤ä¸ªçº¿ç¨‹éƒ½å¯ä»¥åœ¨ç›¸åŒçš„å†…å­˜å—ä¸Šåˆ›å»º
    `TypedArray` æˆ– `Buffer` è§†å›¾ã€‚
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This allocates 4 bytes of memory that can be simultaneously accessed by multiple
    JavaScript contexts. Unlike regular ArrayBuffer, this memory is mapped into multiple
    address spaces using platform-specific mechanisms (shared memory on POSIX, memory-mapped
    files on Windows). The allocation is page-aligned for atomic operations support.
    V8 tracks this specially - it can't move or compact this memory during garbage
    collection because multiple isolates might be accessing it simultaneously.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åˆ†é…äº† 4 å­—èŠ‚çš„å†…å­˜ï¼Œå¯ä»¥è¢«å¤šä¸ª JavaScript ä¸Šä¸‹æ–‡åŒæ—¶è®¿é—®ã€‚ä¸å¸¸è§„ `ArrayBuffer` ä¸åŒï¼Œè¿™ç§å†…å­˜ä½¿ç”¨å¹³å°ç‰¹å®šçš„æœºåˆ¶ï¼ˆPOSIX
    ä¸Šçš„å…±äº«å†…å­˜ï¼ŒWindows ä¸Šçš„å†…å­˜æ˜ å°„æ–‡ä»¶ï¼‰æ˜ å°„åˆ°å¤šä¸ªåœ°å€ç©ºé—´ã€‚åˆ†é…æ˜¯é¡µé¢å¯¹é½çš„ï¼Œä»¥æ”¯æŒåŸå­æ“ä½œã€‚V8 ä¼šç‰¹åˆ«è·Ÿè¸ªè¿™ä¸€ç‚¹ - åœ¨åƒåœ¾å›æ”¶æœŸé—´ï¼Œå®ƒä¸èƒ½ç§»åŠ¨æˆ–å‹ç¼©è¿™ç§å†…å­˜ï¼Œå› ä¸ºå¤šä¸ªéš”ç¦»åŒºå¯èƒ½åŒæ—¶è®¿é—®å®ƒã€‚
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This write is NOT atomic by default. On x86-64, a 32-bit aligned write is atomic
    at the hardware level, but JavaScript makes no such guarantees. Without using
    `Atomics.store()`, this write could be torn - another thread might see a partially
    written value. The value 123 is written directly to the shared memory without
    any synchronization primitives, meaning there's no guarantee when other threads
    will see this update due to CPU cache coherency delays.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™ä¸ªå†™å…¥æ“ä½œä¸æ˜¯åŸå­çš„ã€‚åœ¨ x86-64 ä¸Šï¼Œ32 ä½å¯¹é½çš„å†™å…¥åœ¨ç¡¬ä»¶çº§åˆ«ä¸Šæ˜¯åŸå­çš„ï¼Œä½† JavaScript å¹¶ä¸æä¾›è¿™æ ·çš„ä¿è¯ã€‚å¦‚æœä¸ä½¿ç”¨
    `Atomics.store()`ï¼Œè¿™ä¸ªå†™å…¥å¯èƒ½ä¼šè¢«æ’•è£‚ - å¦ä¸€ä¸ªçº¿ç¨‹å¯èƒ½ä¼šçœ‹åˆ°éƒ¨åˆ†å†™å…¥çš„å€¼ã€‚å€¼ 123 ç›´æ¥å†™å…¥å…±äº«å†…å­˜ï¼Œæ²¡æœ‰ä»»ä½•åŒæ­¥åŸè¯­ï¼Œè¿™æ„å‘³ç€ç”±äº
    CPU ç¼“å­˜ä¸€è‡´æ€§å»¶è¿Ÿï¼Œæ— æ³•ä¿è¯å…¶ä»–çº¿ç¨‹ä½•æ—¶ä¼šçœ‹åˆ°è¿™ä¸ªæ›´æ–°ã€‚
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `postMessage` doesn't copy the SharedArrayBuffer - it transfers a reference
    to the same memory. Both threads now have access to the same 4 bytes of RAM. This
    is fundamentally different from regular ArrayBuffer messaging, which clones the
    data. The worker thread gets its own Int32Array view, but it points to the exact
    same memory pages as the main thread's view.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`postMessage` ä¸ä¼šå¤åˆ¶ `SharedArrayBuffer` - å®ƒä¼ é€’äº†å¯¹ç›¸åŒå†…å­˜çš„å¼•ç”¨ã€‚ç°åœ¨ä¸¤ä¸ªçº¿ç¨‹éƒ½å¯ä»¥è®¿é—®ç›¸åŒçš„ 4 å­—èŠ‚ RAMã€‚è¿™ä¸å¸¸è§„
    `ArrayBuffer` æ¶ˆæ¯ä¼ é€’æœ‰æ ¹æœ¬çš„ä¸åŒï¼Œåè€…ä¼šå…‹éš†æ•°æ®ã€‚å·¥ä½œçº¿ç¨‹è·å¾—è‡ªå·±çš„ Int32Array è§†å›¾ï¼Œä½†å®ƒæŒ‡å‘ä¸ä¸»çº¿ç¨‹è§†å›¾å®Œå…¨ç›¸åŒçš„å†…å­˜é¡µã€‚'
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The worker immediately sees the value 123 that was written by the main thread.
    But this isn't guaranteed without proper synchronization. Due to CPU cache coherency
    protocols, there could be a delay between when one thread writes and when another
    thread sees the update. On weakly-ordered memory architectures (like ARM), you
    might not see the update at all without memory barriers.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥ä½œçº¿ç¨‹ç«‹å³çœ‹åˆ°ä¸»çº¿ç¨‹å†™å…¥çš„å€¼ 123ã€‚ä½†è¿™å¹¶ä¸ä¿è¯åœ¨æ²¡æœ‰é€‚å½“åŒæ­¥çš„æƒ…å†µä¸‹ã€‚ç”±äº CPU ç¼“å­˜ä¸€è‡´æ€§åè®®ï¼Œä¸€ä¸ªçº¿ç¨‹å†™å…¥å’Œå¦ä¸€ä¸ªçº¿ç¨‹çœ‹åˆ°æ›´æ–°ä¹‹é—´å¯èƒ½ä¼šæœ‰å»¶è¿Ÿã€‚åœ¨å¼±é¡ºåºå†…å­˜æ¶æ„ï¼ˆå¦‚
    ARMï¼‰ä¸Šï¼Œå¦‚æœæ²¡æœ‰å†…å­˜å±éšœï¼Œä½ å¯èƒ½æ ¹æœ¬çœ‹ä¸åˆ°æ›´æ–°ã€‚
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This is mind-bendingly powerful. We just modified memory in one thread and saw
    the result instantly in another, with zero copying and zero serialization overhead.
    This is the foundation for high-performance parallel computing in Node.js. You
    can have a worker thread performing complex calculations on a large dataset while
    the main thread reads the results as they become available.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éå¸¸å¼ºå¤§ï¼Œä»¤äººéš¾ä»¥ç½®ä¿¡ã€‚æˆ‘ä»¬åªæ˜¯åœ¨ä¸€æ¡çº¿ç¨‹ä¸Šä¿®æ”¹äº†å†…å­˜ï¼Œå¹¶åœ¨å¦ä¸€æ¡çº¿ç¨‹ä¸Šç«‹å³çœ‹åˆ°äº†ç»“æœï¼Œæ²¡æœ‰ä»»ä½•å¤åˆ¶å’Œåºåˆ—åŒ–å¼€é”€ã€‚è¿™æ˜¯ Node.js ä¸­é«˜æ€§èƒ½å¹¶è¡Œè®¡ç®—çš„åŸºç¡€ã€‚ä½ å¯ä»¥æœ‰ä¸€ä¸ªå·¥ä½œçº¿ç¨‹åœ¨å¤§å‹æ•°æ®é›†ä¸Šæ‰§è¡Œå¤æ‚è®¡ç®—ï¼Œè€Œä¸»çº¿ç¨‹åˆ™è¯»å–ç»“æœï¼Œè¿™äº›ç»“æœä¸€æ—¦å¯ç”¨å³å¯è¯»å–ã€‚
- en: 'However, this introduces a whole new class of problems: race conditions. Since
    two threads can read and write to the same memory at the same time, you need synchronization
    primitives to coordinate access. This is where `Atomics` come in. The `Atomics`
    object provides methods for performing atomic reads, writes, and read-modify-write
    operations on `SharedArrayBuffer` views. These operations are guaranteed to complete
    without being interrupted by another thread, preventing data corruption.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™å¼•å…¥äº†ä¸€ä¸ªå…¨æ–°çš„é—®é¢˜ç±»åˆ«ï¼šç«äº‰æ¡ä»¶ã€‚ç”±äºä¸¤ä¸ªçº¿ç¨‹å¯ä»¥åŒæ—¶è¯»å–å’Œå†™å…¥ç›¸åŒçš„å†…å­˜ï¼Œä½ éœ€è¦åŒæ­¥åŸè¯­æ¥åè°ƒè®¿é—®ã€‚è¿™å°±æ˜¯ `Atomics` å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚`Atomics`
    å¯¹è±¡æä¾›äº†åœ¨ `SharedArrayBuffer` è§†å›¾ä¸Šæ‰§è¡ŒåŸå­è¯»å–ã€å†™å…¥å’Œè¯»å–-ä¿®æ”¹-å†™å…¥æ“ä½œçš„æ–¹æ³•ã€‚è¿™äº›æ“ä½œä¿è¯åœ¨æ²¡æœ‰è¢«å¦ä¸€ä¸ªçº¿ç¨‹ä¸­æ–­çš„æƒ…å†µä¸‹å®Œæˆï¼Œä»è€Œé˜²æ­¢æ•°æ®æŸåã€‚
- en: ğŸ“ŒImportant
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“Œé‡è¦
- en: Without `Atomics`, SharedArrayBuffer access is NOT thread-safe. Regular array
    indexing (`array[0] = value`) can cause data races. Always use `Atomics.store()`,
    `Atomics.load()`, and other atomic operations for thread-safe access.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰ä½¿ç”¨ `Atomics`ï¼Œ`SharedArrayBuffer` è®¿é—®ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ã€‚å¸¸è§„æ•°ç»„ç´¢å¼•ï¼ˆ`array[0] = value`ï¼‰å¯èƒ½å¯¼è‡´æ•°æ®ç«äº‰ã€‚å§‹ç»ˆä½¿ç”¨
    `Atomics.store()`ã€`Atomics.load()` å’Œå…¶ä»–åŸå­æ“ä½œè¿›è¡Œçº¿ç¨‹å®‰å…¨çš„è®¿é—®ã€‚
- en: Using `SharedArrayBuffer` is an advanced technique, and it brings the challenges
    of concurrent programming directly into your Node.js application. But understanding
    that it's all built on the same foundation of views (`TypedArray`s) over a shared
    block of memory (`SharedArrayBuffer`) demystifies the magic. It's the same principle
    as `slice` and `subarray`, just extended across the thread boundary.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `SharedArrayBuffer` æ˜¯ä¸€ç§é«˜çº§æŠ€æœ¯ï¼Œå®ƒå°†å¹¶å‘ç¼–ç¨‹çš„æŒ‘æˆ˜ç›´æ¥å¸¦å…¥ä½ çš„ Node.js åº”ç”¨ç¨‹åºã€‚ä½†ç†è§£å®ƒå…¨éƒ¨å»ºç«‹åœ¨ç›¸åŒçš„è§†å›¾ï¼ˆ`TypedArray`sï¼‰å…±äº«å†…å­˜å—ï¼ˆ`SharedArrayBuffer`ï¼‰çš„åŸºç¡€ä¹‹ä¸Šï¼Œå°±èƒ½æ­å¼€é­”æ³•çš„é¢çº±ã€‚è¿™ä¸
    `slice` å’Œ `subarray` çš„åŸç†ç›¸åŒï¼Œåªæ˜¯æ‰©å±•åˆ°äº†çº¿ç¨‹è¾¹ç•Œä¹‹å¤–ã€‚
- en: Memory Retention and Garbage Collection
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®°å¿†ä¿ç•™ä¸åƒåœ¾å›æ”¶
- en: We've talked a lot about memory retention, but let's formalize it. This is the
    mechanism behind our 10GB (hypothetical) log parser leak. In a garbage-collected
    language like JavaScript, an object is kept in memory as long as there is a reachable
    reference to it from the "root" set (e.g., the global object, the current call
    stack).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»è®¨è®ºäº†å¾ˆå¤šå…³äºè®°å¿†ä¿ç•™çš„å†…å®¹ï¼Œä½†è®©æˆ‘ä»¬æ­£å¼åŒ–å®ƒã€‚è¿™æ˜¯æˆ‘ä»¬10GBï¼ˆå‡è®¾ï¼‰æ—¥å¿—è§£æå™¨æ³„éœ²èƒŒåçš„æœºåˆ¶ã€‚åœ¨åƒ JavaScript è¿™æ ·çš„åƒåœ¾å›æ”¶è¯­è¨€ä¸­ï¼Œåªè¦ä»â€œæ ¹â€é›†åˆï¼ˆä¾‹å¦‚ï¼Œå…¨å±€å¯¹è±¡ï¼Œå½“å‰è°ƒç”¨æ ˆï¼‰æœ‰å¯åˆ°è¾¾çš„å¼•ç”¨ï¼Œå¯¹è±¡å°±ä¼šä¿ç•™åœ¨å†…å­˜ä¸­ã€‚
- en: When you create a `Buffer` view with `slice()` or `subarray()`, you create two
    objects with a relationship.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ ä½¿ç”¨ `slice()` æˆ– `subarray()` åˆ›å»º `Buffer` è§†å›¾æ—¶ï¼Œä½ åˆ›å»ºäº†ä¸¤ä¸ªå…·æœ‰å…³ç³»çš„å¯¹è±¡ã€‚
- en: The **View Object** - The new `Buffer` instance (`userIdSlice`). It's a small
    object on the V8 heap.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è§†å›¾å¯¹è±¡** - æ–°çš„ `Buffer` å®ä¾‹ï¼ˆ`userIdSlice`ï¼‰ã€‚å®ƒæ˜¯åœ¨ V8 å †ä¸Šçš„ä¸€ä¸ªå°å¯¹è±¡ã€‚'
- en: The **Parent Buffer Object** - The original `Buffer` (`massiveBuffer`), which
    holds the reference to the large external `ArrayBuffer`.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**çˆ¶ç¼“å†²å¯¹è±¡** - åŸå§‹çš„ `Buffer` (`massiveBuffer`)ï¼Œå®ƒæŒæœ‰å¯¹å¤§å‹å¤–éƒ¨ `ArrayBuffer` çš„å¼•ç”¨ã€‚'
- en: The view object maintains an internal reference to its parent buffer. According
    to V8's memory model, as long as the view object is reachable, its parent buffer
    is also considered reachable. The garbage collector sees the reference from `userIdSlice`
    to `massiveBuffer` and says, "Nope, can't collect `massiveBuffer` yet, someone
    still needs it." It has no idea you only care about 16 bytes out of the 50 megabytes.
    It just sees a valid reference and honors it.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: è§†å›¾å¯¹è±¡ç»´æŠ¤å¯¹å…¶çˆ¶ç¼“å†²åŒºçš„å†…éƒ¨å¼•ç”¨ã€‚æ ¹æ® V8 çš„å†…å­˜æ¨¡å‹ï¼Œåªè¦è§†å›¾å¯¹è±¡æ˜¯å¯åˆ°è¾¾çš„ï¼Œå®ƒçš„çˆ¶ç¼“å†²åŒºä¹Ÿè¢«è®¤ä¸ºæ˜¯å¯åˆ°è¾¾çš„ã€‚åƒåœ¾å›æ”¶å™¨çœ‹åˆ°ä» `userIdSlice`
    åˆ° `massiveBuffer` çš„å¼•ç”¨ï¼Œä¼šè¯´ï¼šâ€œä¸è¡Œï¼Œè¿˜ä¸èƒ½æ”¶é›† `massiveBuffer`ï¼Œè¿˜æœ‰äººéœ€è¦å®ƒã€‚â€å®ƒä¸çŸ¥é“ä½ åªå…³å¿ƒ50MBä¸­çš„16å­—èŠ‚ã€‚å®ƒåªçœ‹åˆ°æœ‰æ•ˆçš„å¼•ç”¨å¹¶å°Šé‡å®ƒã€‚
- en: This is why the heap snapshot was so confusing. The profiler correctly identified
    that the `userIdSlice` objects were small. But it also has a concept of "retained
    size" vs. "shallow size."
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå †å¿«ç…§å¦‚æ­¤ä»¤äººå›°æƒ‘ã€‚åˆ†æå™¨æ­£ç¡®åœ°è¯†åˆ«å‡º `userIdSlice` å¯¹è±¡å¾ˆå°ã€‚ä½†å®ƒè¿˜æœ‰ä¸€ä¸ªâ€œä¿ç•™å¤§å°â€ä¸â€œæµ…å±‚å¤§å°â€çš„æ¦‚å¿µã€‚
- en: '**Shallow Size** is thhe size of the object itself. For our slices, this was
    tiny, just a few dozen bytes for the JavaScript object wrapper.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æµ…å±‚å¤§å°** æ˜¯å¯¹è±¡æœ¬èº«çš„å¤§å°ã€‚å¯¹äºæˆ‘ä»¬çš„åˆ‡ç‰‡ï¼Œè¿™éå¸¸å°ï¼Œåªæ˜¯ JavaScript å¯¹è±¡åŒ…è£…å™¨å‡ åå­—èŠ‚ã€‚'
- en: '**Retained Size** is the size of all memory that is being kept alive *solely*
    because this object exists. For our slices, the retained size was enormous, because
    they were the only thing keeping the 50MB parent buffers from being garbage collected.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¿ç•™å¤§å°** æ˜¯æ‰€æœ‰ä»…å› ä¸ºè¯¥å¯¹è±¡å­˜åœ¨è€Œè¢«ä¿ç•™çš„å†…å­˜çš„å¤§å°ã€‚å¯¹äºæˆ‘ä»¬çš„åˆ‡ç‰‡ï¼Œä¿ç•™å¤§å°éå¸¸å¤§ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å”¯ä¸€é˜»æ­¢50MBçˆ¶ç¼“å†²åŒºè¢«åƒåœ¾å›æ”¶çš„ä¸œè¥¿ã€‚'
- en: 'The heap snapshot showed 890MB retained by 10KB of slices. It looked like an
    accounting error, but it was the brutal truth of view semantics. Once we understood
    this, the fix was obvious: we had to sever the link between the small piece of
    data we needed and its giant parent. The only way to do that is with a copy.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: å †å¿«ç…§æ˜¾ç¤º 890MB çš„åˆ‡ç‰‡ä¿ç•™äº† 10KBã€‚å®ƒçœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªä¼šè®¡é”™è¯¯ï¼Œä½†è¿™æ˜¯è§†å›¾è¯­ä¹‰çš„æ®‹é…·çœŸç›¸ã€‚ä¸€æ—¦æˆ‘ä»¬ç†è§£äº†è¿™ä¸€ç‚¹ï¼Œä¿®å¤å°±æ˜¾è€Œæ˜“è§äº†ï¼šæˆ‘ä»¬å¿…é¡»åˆ‡æ–­æˆ‘ä»¬æ‰€éœ€çš„å°æ•°æ®ç‰‡æ®µä¸å…¶åºå¤§çš„çˆ¶å¯¹è±¡ä¹‹é—´çš„è”ç³»ã€‚å”¯ä¸€çš„æ–¹æ³•æ˜¯è¿›è¡Œå¤åˆ¶ã€‚
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This function returns a view that maintains a strong reference to `parent`'s
    ArrayBuffer. If `parent` is 10MB, your 10-byte view keeps all 10MB alive. The
    V8 garbage collector traces the reference chain and marks the entire parent as
    reachable. This pattern is responsible for the majority of Buffer-related memory
    leaks in production Node.js applications.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å‡½æ•°è¿”å›ä¸€ä¸ªè§†å›¾ï¼Œå®ƒä¿æŒå¯¹ `parent` çš„ `ArrayBuffer` çš„å¼ºå¼•ç”¨ã€‚å¦‚æœ `parent` æ˜¯ 10MBï¼Œä½ çš„ 10 å­—èŠ‚è§†å›¾å°†ä¿æŒæ‰€æœ‰
    10MB çš„æ´»è·ƒçŠ¶æ€ã€‚V8 åƒåœ¾æ”¶é›†å™¨è¿½è¸ªå¼•ç”¨é“¾ï¼Œå¹¶å°†æ•´ä¸ªçˆ¶å¯¹è±¡æ ‡è®°ä¸ºå¯åˆ°è¾¾ã€‚è¿™ç§æ¨¡å¼æ˜¯ç”Ÿäº§ç¯å¢ƒä¸­ Node.js åº”ç”¨ç¨‹åºä¸­ä¸ Buffer ç›¸å…³çš„å¤§å¤šæ•°å†…å­˜æ³„æ¼çš„åŸå› ã€‚
- en: '[PRE35]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `Buffer.from(buffer)` constructor call is the key. It takes the 10-byte
    view created by `slice()`, allocates a *new* 10-byte `ArrayBuffer`, copies the
    data into it, and returns a new `Buffer` object that points to this new, small
    allocation. The original parent buffer is no longer referenced by the returned
    object, and the temporary view created by `slice()` can be immediately collected.
    This pattern, `Buffer.from(buf.slice(...))`, is a common and effective way to
    create a "trimmed" copy of a small section of a large buffer. It's the antidote
    to view-based memory retention. After enough production incidents, you learn to
    spot a missing copy like a hawk.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.from(buffer)` æ„é€ å‡½æ•°è°ƒç”¨æ˜¯å…³é”®ã€‚å®ƒæ¥å—ç”± `slice()` åˆ›å»ºçš„ 10 å­—èŠ‚è§†å›¾ï¼Œåˆ†é…ä¸€ä¸ªæ–°çš„ 10 å­—èŠ‚ `ArrayBuffer`ï¼Œå°†æ•°æ®å¤åˆ¶åˆ°å…¶ä¸­ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ–°çš„
    `Buffer` å¯¹è±¡ï¼Œè¯¥å¯¹è±¡æŒ‡å‘è¿™ä¸ªæ–°çš„ã€å°çš„åˆ†é…ã€‚åŸå§‹çˆ¶ç¼“å†²åŒºä¸å†è¢«è¿”å›çš„å¯¹è±¡å¼•ç”¨ï¼Œç”± `slice()` åˆ›å»ºçš„ä¸´æ—¶è§†å›¾å¯ä»¥ç«‹å³å›æ”¶ã€‚è¿™ç§æ¨¡å¼ï¼Œ`Buffer.from(buf.slice(...))`ï¼Œæ˜¯åˆ›å»ºå¤§å‹ç¼“å†²åŒºä¸€å°éƒ¨åˆ†â€œä¿®å‰ªâ€å‰¯æœ¬çš„å¸¸è§ä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚å®ƒæ˜¯åŸºäºè§†å›¾çš„å†…å­˜ä¿ç•™çš„è§£è¯ã€‚åœ¨ç»å†äº†è¶³å¤Ÿçš„ç”Ÿäº§äº‹æ•…åï¼Œä½ å°±èƒ½åƒé¹°ä¸€æ ·è¿…é€Ÿå‘ç°ç¼ºå¤±çš„å‰¯æœ¬ã€‚'
- en: Binary Protocol Parsing with Views
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºäºè§†å›¾çš„äºŒè¿›åˆ¶åè®®è§£æ
- en: 'Now let''s apply these concepts to a real-world scenario: parsing a custom
    binary protocol. This is common in high-performance systems, IoT, and gaming,
    where the overhead of JSON or XML is unacceptable. A binary protocol defines a
    strict layout of data in a sequence of bytes.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†è¿™äº›æ¦‚å¿µåº”ç”¨åˆ°å®é™…åœºæ™¯ä¸­ï¼šè§£æè‡ªå®šä¹‰äºŒè¿›åˆ¶åè®®ã€‚è¿™åœ¨é«˜æ€§èƒ½ç³»ç»Ÿã€ç‰©è”ç½‘å’Œæ¸¸æˆä¸­å¾ˆå¸¸è§ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼ŒJSON æˆ– XML çš„å¼€é”€æ˜¯ä¸å¯æ¥å—çš„ã€‚äºŒè¿›åˆ¶åè®®å®šä¹‰äº†å­—èŠ‚åºåˆ—ä¸­çš„æ•°æ®ä¸¥æ ¼å¸ƒå±€ã€‚
- en: 'For example, a message might be structured like this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä¸€æ¡æ¶ˆæ¯å¯èƒ½çš„ç»“æ„å¦‚ä¸‹ï¼š
- en: 'Bytes 0-1: Message Type (Uint16)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­—èŠ‚ 0-1ï¼šæ¶ˆæ¯ç±»å‹ï¼ˆUint16ï¼‰
- en: 'Bytes 2-3: Message Length (Uint16)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­—èŠ‚ 2-3ï¼šæ¶ˆæ¯é•¿åº¦ï¼ˆUint16ï¼‰
- en: 'Byte 4: Flags (Uint8)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­—èŠ‚ 4ï¼šæ ‡å¿—ï¼ˆUint8ï¼‰
- en: 'Bytes 5-20: Session ID (16-byte UUID string)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­—èŠ‚ 5-20ï¼šä¼šè¯ IDï¼ˆ16 å­—èŠ‚ UUID å­—ç¬¦ä¸²ï¼‰
- en: 'Bytes 21-end: Payload (raw bytes)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­—èŠ‚ 21-ç»“æŸï¼šæœ‰æ•ˆè½½è·ï¼ˆåŸå§‹å­—èŠ‚ï¼‰
- en: A naive approach to parsing this would involve a lot of slicing and copying.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ææ­¤å†…å®¹çš„ä¸€ä¸ªç®€å•æ–¹æ³•æ¶‰åŠå¤§é‡çš„åˆ‡ç‰‡å’Œå¤åˆ¶ã€‚
- en: '[PRE36]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Each of these lines creates a temporary view just to read a primitive value.
    The `slice(0, 2)` creates a Buffer object (72 bytes on heap), then `readUInt16BE()`
    reads two bytes and converts them from big-endian to native endianness. The view
    is immediately discarded but not before V8 allocates it, tracks it, and eventually
    garbage collects it. With thousands of messages per second, you're creating massive
    GC pressure for no reason. These intermediate views serve no purpose - you could
    read directly from the original buffer.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯è¡Œä»£ç éƒ½åˆ›å»ºä¸€ä¸ªä¸´æ—¶è§†å›¾ï¼Œä»…ç”¨äºè¯»å–åŸå§‹å€¼ã€‚`slice(0, 2)` åˆ›å»ºä¸€ä¸ªç¼“å†²åŒºå¯¹è±¡ï¼ˆå †ä¸Š 72 å­—èŠ‚ï¼‰ï¼Œç„¶å `readUInt16BE()`
    è¯»å–ä¸¤ä¸ªå­—èŠ‚å¹¶å°†å®ƒä»¬ä»å¤§ç«¯è½¬æ¢ä¸ºæœ¬åœ°ç«¯åºã€‚è§†å›¾ç«‹å³è¢«ä¸¢å¼ƒï¼Œä½†åœ¨ V8 åˆ†é…ã€è·Ÿè¸ªå¹¶æœ€ç»ˆåƒåœ¾å›æ”¶å®ƒä¹‹å‰ã€‚æ¯ç§’æ•°åƒæ¡æ¶ˆæ¯ï¼Œä½ æ­£åœ¨æ— ç«¯åœ°åˆ›å»ºå¤§é‡çš„ GC å‹åŠ›ã€‚è¿™äº›ä¸­é—´è§†å›¾æ²¡æœ‰ä»»ä½•ä½œç”¨â€”â€”ä½ å¯ä»¥ç›´æ¥ä»åŸå§‹ç¼“å†²åŒºè¯»å–ã€‚
- en: '[PRE37]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: âš ï¸Warning
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸è­¦å‘Š
- en: The above pattern creates 5 buffer views per message. Processing 1000 messages/sec
    with 1MB payloads would retain 1GB of memory even if you only need the 16-byte
    session IDs!
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°æ¨¡å¼ä¸ºæ¯æ¡æ¶ˆæ¯åˆ›å»º 5 ä¸ªç¼“å†²åŒºè§†å›¾ã€‚ä»¥æ¯ç§’ 1MB çš„è´Ÿè½½å¤„ç† 1000 æ¡æ¶ˆæ¯å°†ä¿ç•™ 1GB çš„å†…å­˜ï¼Œå³ä½¿ä½ åªéœ€è¦ 16 å­—èŠ‚çš„ä¼šè¯ IDï¼
- en: This code *works*, but it's creating five new `Buffer` objects for every single
    message. If you're processing thousands of messages per second, those allocations
    add up, putting pressure on the garbage collector and slowing down your application.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç *å·¥ä½œ*ï¼Œä½†å®ƒä¸ºæ¯æ¡æ¶ˆæ¯åˆ›å»ºäº†äº”ä¸ªæ–°çš„ `Buffer` å¯¹è±¡ã€‚å¦‚æœä½ æ¯ç§’å¤„ç†æ•°åƒæ¡æ¶ˆæ¯ï¼Œè¿™äº›åˆ†é…ä¼šç´¯åŠ èµ·æ¥ï¼Œç»™åƒåœ¾æ”¶é›†å™¨æ–½åŠ å‹åŠ›ï¼Œå¹¶å‡æ…¢ä½ çš„åº”ç”¨ç¨‹åºã€‚
- en: A zero-copy approach, on the other hand, leverages views to read the data without
    creating copies of the data itself. We can use the offset-based read methods directly
    on the main buffer, or create `TypedArray` views for more complex data types.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œé›¶æ‹·è´æ–¹æ³•åˆ©ç”¨è§†å›¾è¯»å–æ•°æ®ï¼Œè€Œä¸åˆ›å»ºæ•°æ®çš„å‰¯æœ¬ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä¸»ç¼“å†²åŒºä¸Šç›´æ¥ä½¿ç”¨åŸºäºåç§»é‡çš„è¯»å–æ–¹æ³•ï¼Œæˆ–è€…ä¸ºæ›´å¤æ‚çš„æ•°æ®ç±»å‹åˆ›å»º`TypedArray`è§†å›¾ã€‚
- en: '[PRE38]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: These direct reads are lightning fast. No intermediate objects, no allocations,
    no GC pressure. The `readUInt16BE()` method calculates the memory address (buffer
    base + offset), reads two bytes, and performs the endianness conversion in optimized
    C++ code. The entire operation stays in CPU cache. For high-frequency parsing,
    this difference between creating a view then reading versus reading directly can
    mean the difference between 10,000 and 100,000 messages per second.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç›´æ¥è¯»å–æ“ä½œéå¸¸å¿«é€Ÿã€‚æ²¡æœ‰ä¸­é—´å¯¹è±¡ï¼Œæ²¡æœ‰åˆ†é…ï¼Œæ²¡æœ‰GCå‹åŠ›ã€‚`readUInt16BE()` æ–¹æ³•è®¡ç®—å†…å­˜åœ°å€ï¼ˆç¼“å†²åŒºåŸºç¡€åœ°å€ + åç§»é‡ï¼‰ï¼Œè¯»å–ä¸¤ä¸ªå­—èŠ‚ï¼Œå¹¶åœ¨ä¼˜åŒ–çš„C++ä»£ç ä¸­æ‰§è¡Œå­—èŠ‚åºè½¬æ¢ã€‚æ•´ä¸ªæ“ä½œéƒ½ä¿æŒåœ¨CPUç¼“å­˜ä¸­ã€‚å¯¹äºé«˜é¢‘è§£æï¼Œåˆ›å»ºè§†å›¾ç„¶åè¯»å–ä¸ç›´æ¥è¯»å–ä¹‹é—´çš„å·®å¼‚å¯èƒ½æ„å‘³ç€æ¯ç§’10,000æ¡å’Œ100,000æ¡æ¶ˆæ¯ä¹‹é—´çš„å·®å¼‚ã€‚
- en: '[PRE39]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ğŸ“ŒImportant
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“Œé‡è¦
- en: 'This zero-copy version is 10x faster but returns views that retain the entire
    parent buffer. Document this clearly: callers MUST copy the data if they need
    to store it beyond the immediate processing scope.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé›¶æ‹·è´ç‰ˆæœ¬é€Ÿåº¦å¿«10å€ï¼Œä½†è¿”å›ä¿ç•™æ•´ä¸ªçˆ¶ç¼“å†²åŒºçš„è§†å›¾ã€‚æ˜ç¡®è®°å½•ï¼šå¦‚æœè°ƒç”¨è€…éœ€è¦å°†æ•°æ®å­˜å‚¨åœ¨ç«‹å³å¤„ç†ä½œç”¨åŸŸä¹‹å¤–ï¼Œè°ƒç”¨è€…å¿…é¡»å¤åˆ¶æ•°æ®ã€‚
- en: This version is significantly more efficient. It creates no intermediate copies
    for the primitive number types. It creates two views for the session ID and payload,
    but no data is duplicated. The `sessionIdView` and `payloadView` are lightweight
    pointers back into the original message buffer.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç‰ˆæœ¬æ•ˆç‡æ˜¾è‘—æ›´é«˜ã€‚å®ƒä¸ºåŸå§‹æ•°å­—ç±»å‹ä¸åˆ›å»ºä»»ä½•ä¸­é—´å‰¯æœ¬ã€‚å®ƒä¸ºä¼šè¯IDå’Œæœ‰æ•ˆè½½è·åˆ›å»ºäº†ä¸¤ä¸ªè§†å›¾ï¼Œä½†æ²¡æœ‰æ•°æ®è¢«é‡å¤ã€‚`sessionIdView`å’Œ`payloadView`æ˜¯è½»é‡çº§æŒ‡é’ˆï¼ŒæŒ‡å‘åŸå§‹æ¶ˆæ¯ç¼“å†²åŒºã€‚
- en: 'This is the pattern that finally saved us 8GB of RAM in our TCP service. We
    use a view because the processing is temporary. If we needed to store the `sessionIdView`
    or `payloadView` long-term (e.g., in a cache or a request map), we would be right
    back in the memory retention trap. The contract of a function like this should
    be clear: it returns views that are only valid for the immediate scope of processing.
    If a caller needs to persist that data, it is the *caller''s responsibility* to
    perform the copy.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬TCPæœåŠ¡ä¸­æœ€ç»ˆèŠ‚çœäº†8GB RAMçš„æ¨¡å¼ã€‚æˆ‘ä»¬ä½¿ç”¨è§†å›¾æ˜¯å› ä¸ºå¤„ç†æ˜¯ä¸´æ—¶çš„ã€‚å¦‚æœæˆ‘ä»¬éœ€è¦é•¿æœŸå­˜å‚¨`sessionIdView`æˆ–`payloadView`ï¼ˆä¾‹å¦‚ï¼Œåœ¨ç¼“å­˜æˆ–è¯·æ±‚æ˜ å°„ä¸­ï¼‰ï¼Œæˆ‘ä»¬å°±ä¼šå›åˆ°å†…å­˜ä¿ç•™é™·é˜±ã€‚æ­¤ç±»å‡½æ•°çš„å¥‘çº¦åº”è¯¥æ˜¯æ¸…æ™°çš„ï¼šå®ƒè¿”å›ä»…åœ¨å¤„ç†ç«‹å³ä½œç”¨åŸŸå†…æœ‰æ•ˆçš„è§†å›¾ã€‚å¦‚æœè°ƒç”¨è€…éœ€è¦æŒä¹…åŒ–è¯¥æ•°æ®ï¼Œå®ƒå°±æ˜¯è°ƒç”¨è€…çš„*è´£ä»»*æ¥æ‰§è¡Œå¤åˆ¶ã€‚
- en: This is a critical design pattern for high-performance libraries. A parsing
    function should perform zero-copy operations and return views. The consumer of
    the function then decides whether the data is short-lived (use the view directly)
    or long-lived (create a copy). This separates concerns and puts the memory management
    decision in the hands of the code that has the most context.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¯¹é«˜æ€§èƒ½åº“è‡³å…³é‡è¦çš„è®¾è®¡æ¨¡å¼ã€‚è§£æå‡½æ•°åº”æ‰§è¡Œé›¶æ‹·è´æ“ä½œå¹¶è¿”å›è§†å›¾ã€‚å‡½æ•°çš„æ¶ˆè´¹è€…ç„¶åå†³å®šæ•°æ®æ˜¯çŸ­æœŸçš„ï¼ˆç›´æ¥ä½¿ç”¨è§†å›¾ï¼‰è¿˜æ˜¯é•¿æœŸçš„ï¼ˆåˆ›å»ºå‰¯æœ¬ï¼‰ã€‚è¿™åˆ†ç¦»äº†å…³æ³¨ç‚¹ï¼Œå¹¶å°†å†…å­˜ç®¡ç†å†³ç­–äº¤ç»™äº†å…·æœ‰æœ€å¤šä¸Šä¸‹æ–‡çš„ä»£ç ã€‚
- en: Platform Endianness and TypedArray Views
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¹³å°å­—èŠ‚åºå’ŒTypedArrayè§†å›¾
- en: When you're working with binary data that comes from the network or a file,
    you can't escape the concept of endianness. It refers to the order in which a
    multi-byte number (like a 16-bit or 32-bit integer) is stored in memory.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ å¤„ç†æ¥è‡ªç½‘ç»œæˆ–æ–‡ä»¶çš„äºŒè¿›åˆ¶æ•°æ®æ—¶ï¼Œä½ æ— æ³•é¿å¼€å­—èŠ‚åºçš„æ¦‚å¿µã€‚å®ƒæŒ‡çš„æ˜¯å¤šå­—èŠ‚æ•°å­—ï¼ˆå¦‚16ä½æˆ–32ä½æ•´æ•°ï¼‰åœ¨å†…å­˜ä¸­å­˜å‚¨çš„é¡ºåºã€‚
- en: â„¹ï¸Note
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: â„¹ï¸æ³¨æ„
- en: Don't sweat it if bit manipulation and bit masks are still a bit fuzzy; we'll
    do a deep dive on them in a dedicated chapter later on. For now, just hang tight.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½æ“ä½œå’Œä½æ©ç ä»ç„¶æœ‰ç‚¹æ¨¡ç³Šï¼Œä¸è¦æ‹…å¿ƒï¼›æˆ‘ä»¬å°†åœ¨åé¢çš„ä¸“é—¨ç« èŠ‚ä¸­æ·±å…¥æ¢è®¨å®ƒä»¬ã€‚ç°åœ¨ï¼Œåªéœ€è€å¿ƒç­‰å¾…ã€‚
- en: '**Big-Endian (BE) -** The most significant byte comes first. This is common
    in network protocols (often called "network byte order"). The number `0x12345678`
    would be stored as `12 34 56 78`.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤§ç«¯ï¼ˆBEï¼‰** - æœ€æ˜¾è‘—çš„å­—èŠ‚é¦–å…ˆã€‚è¿™åœ¨ç½‘ç»œåè®®ä¸­å¾ˆå¸¸è§ï¼ˆé€šå¸¸ç§°ä¸ºâ€œç½‘ç»œå­—èŠ‚åºâ€ï¼‰ã€‚æ•°å­—`0x12345678`å°†å­˜å‚¨ä¸º`12 34 56 78`ã€‚'
- en: '**Little-Endian (LE) -** The least significant byte comes first. This is the
    native format for most modern CPUs, including Intel and AMD x86-64\. The same
    number would be stored as `78 56 34 12`.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°ç«¯ï¼ˆLEï¼‰** - æœ€ä¸æ˜¾è‘—çš„å­—èŠ‚é¦–å…ˆã€‚è¿™æ˜¯å¤§å¤šæ•°ç°ä»£CPUçš„æœ¬åœ°æ ¼å¼ï¼ŒåŒ…æ‹¬Intelå’ŒAMD x86-64ã€‚ç›¸åŒçš„æ•°å­—å°†å­˜å‚¨ä¸º`78 56 34
    12`ã€‚'
- en: 'Forgetting about endianness will lead to completely garbled data when reading
    binary protocols. Node.js `Buffer`s provide explicit methods for this: `readUInt16BE`,
    `readUInt16LE`, `writeInt32BE`, etc. These are your safest bet when you know the
    exact endianness of the data you''re parsing.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å¿½ç•¥å­—èŠ‚åºé—®é¢˜åœ¨è¯»å–äºŒè¿›åˆ¶åè®®æ—¶ä¼šå¯¼è‡´æ•°æ®å®Œå…¨æ··ä¹±ã€‚Node.js çš„ `Buffer` æä¾›äº†æ˜ç¡®çš„æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼š`readUInt16BE`ã€`readUInt16LE`ã€`writeInt32BE`
    ç­‰ã€‚å½“ä½ çŸ¥é“ä½ æ­£åœ¨è§£æçš„æ•°æ®çš„ç¡®åˆ‡å­—èŠ‚åºæ—¶ï¼Œè¿™äº›æ–¹æ³•æ˜¯ä½ çš„æœ€ä½³é€‰æ‹©ã€‚
- en: But what if you're using `TypedArray` views directly on an underlying `ArrayBuffer`?
    This is where it gets tricky. `TypedArray`s (like `Int16Array`, `Float64Array`)
    read and write data using the host system's native endianness. On my x86 laptop,
    that's little-endian. If I create an `Int16Array` view over a buffer that contains
    big-endian network data, I will read garbage.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ ç›´æ¥åœ¨åº•å±‚çš„ `ArrayBuffer` ä¸Šä½¿ç”¨ `TypedArray` è§†å›¾å‘¢ï¼Ÿè¿™å°±å˜å¾—å¤æ‚äº†ã€‚`TypedArray`ï¼ˆå¦‚ `Int16Array`ã€`Float64Array`ï¼‰ä½¿ç”¨å®¿ä¸»ç³»ç»Ÿçš„æœ¬åœ°å­—èŠ‚åºè¯»å–å’Œå†™å…¥æ•°æ®ã€‚åœ¨æˆ‘çš„
    x86 ç¬”è®°æœ¬ç”µè„‘ä¸Šï¼Œè¿™æ˜¯å°ç«¯å­—èŠ‚åºã€‚å¦‚æœæˆ‘åœ¨åŒ…å«å¤§ç«¯ç½‘ç»œæ•°æ®çš„ç¼“å†²åŒºä¸Šåˆ›å»ºä¸€ä¸ª `Int16Array` è§†å›¾ï¼Œæˆ‘å°†è¯»å–åƒåœ¾æ•°æ®ã€‚
- en: '[PRE40]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `readUInt16BE()` method explicitly handles endianness conversion. It reads
    bytes at positions 0 and 1, then combines them as `(buffer[0] << 8) | buffer[1]`,
    which correctly interprets big-endian data regardless of platform endianness.
    This happens in Node's C++ layer with optimized byte-swapping instructions like
    `bswap` on x86 or `rev` on ARM when needed.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`readUInt16BE()` æ–¹æ³•æ˜ç¡®å¤„ç†å­—èŠ‚åºè½¬æ¢ã€‚å®ƒè¯»å–ä½ç½® 0 å’Œ 1 çš„å­—èŠ‚ï¼Œç„¶åå°†å®ƒä»¬ç»„åˆä¸º `(buffer[0] << 8) | buffer[1]`ï¼Œè¿™å¯ä»¥æ­£ç¡®åœ°è§£é‡Šå¤§ç«¯æ•°æ®ï¼Œæ— è®ºå¹³å°å­—èŠ‚åºå¦‚ä½•ã€‚å½“éœ€è¦æ—¶ï¼Œè¿™å‘ç”Ÿåœ¨
    Node çš„ C++ å±‚ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å­—èŠ‚äº¤æ¢æŒ‡ä»¤ï¼Œå¦‚ x86 ä¸Šçš„ `bswap` æˆ– ARM ä¸Šçš„ `rev`ã€‚'
- en: '[PRE41]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ğŸš¨Caution
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš¨ è­¦å‘Š
- en: TypedArray views use platform endianness (usually little-endian on x86/ARM).
    Network protocols typically use big-endian. NEVER use raw TypedArray views for
    network data - always use Buffer's BE/LE methods or DataView with explicit endianness.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`TypedArray` è§†å›¾ä½¿ç”¨å¹³å°å­—èŠ‚åºï¼ˆé€šå¸¸åœ¨ x86/ARM ä¸Šæ˜¯å°ç«¯å­—èŠ‚åºï¼‰ã€‚ç½‘ç»œåè®®é€šå¸¸ä½¿ç”¨å¤§ç«¯å­—èŠ‚åºã€‚æ°¸è¿œä¸è¦ä½¿ç”¨åŸå§‹çš„ `TypedArray`
    è§†å›¾æ¥å¤„ç†ç½‘ç»œæ•°æ® - æ€»æ˜¯ä½¿ç”¨ `Buffer` çš„ BE/LE æ–¹æ³•æˆ–å¸¦æœ‰æ˜ç¡®å­—èŠ‚åºçš„ `DataView`ã€‚'
- en: This is a disaster waiting to happen. How do we control endianness when using
    generic `TypedArray` views? The answer is the `DataView` object. A `DataView`
    is a low-level interface for reading and writing data to an `ArrayBuffer` that
    lets you explicitly specify the endianness for each operation. It's more verbose
    than using a `TypedArray`, but it gives you absolute control.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå³å°†å‘ç”Ÿçš„ç¾éš¾ã€‚æˆ‘ä»¬å¦‚ä½•åœ¨ä½¿ç”¨é€šç”¨ `TypedArray` è§†å›¾æ—¶æ§åˆ¶å­—èŠ‚åºï¼Ÿç­”æ¡ˆæ˜¯ `DataView` å¯¹è±¡ã€‚`DataView` æ˜¯ä¸€ä¸ªä½çº§æ¥å£ï¼Œç”¨äºè¯»å–å’Œå†™å…¥
    `ArrayBuffer` ä¸­çš„æ•°æ®ï¼Œå…è®¸ä½ ä¸ºæ¯ä¸ªæ“ä½œæ˜ç¡®æŒ‡å®šå­—èŠ‚åºã€‚å®ƒæ¯”ä½¿ç”¨ `TypedArray` æ›´å†—é•¿ï¼Œä½†å®ƒæä¾›äº†ç»å¯¹çš„æ§åˆ¶ã€‚
- en: '[PRE42]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `setInt32()` with `false` writes the bytes as [0x07, 0x5B, 0xCD, 0x15] -
    most significant byte first. DataView internally handles the byte ordering regardless
    of platform endianness. On a little-endian system, it reverses the bytes before
    writing. On a big-endian system, it writes them directly. This abstraction layer
    costs a few CPU cycles but guarantees correct behavior across all platforms.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`setInt32()` æ–¹æ³•ä½¿ç”¨ `false` å°†å­—èŠ‚å†™å…¥ä¸º [0x07, 0x5B, 0xCD, 0x15] - æœ€é«˜æœ‰æ•ˆå­—èŠ‚é¦–å…ˆå†™å…¥ã€‚`DataView`
    å†…éƒ¨å¤„ç†å­—èŠ‚åºï¼Œæ— è®ºå¹³å°å­—èŠ‚åºå¦‚ä½•ã€‚åœ¨å°ç«¯ç³»ç»Ÿä¸Šï¼Œå®ƒåœ¨å†™å…¥ä¹‹å‰åè½¬å­—èŠ‚ã€‚åœ¨å¤§ç«¯ç³»ç»Ÿä¸Šï¼Œå®ƒç›´æ¥å†™å…¥ã€‚è¿™ä¸ªæŠ½è±¡å±‚ä¼šæ¶ˆè€—ä¸€äº› CPU å‘¨æœŸï¼Œä½†ç¡®ä¿äº†æ‰€æœ‰å¹³å°ä¸Šçš„æ­£ç¡®è¡Œä¸ºã€‚'
- en: '[PRE43]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This `DataView` cast seemed fine until it corrupted everything. In one of our
    services, a developer had used a `Float32Array` to quickly parse a list of floating-point
    numbers from a network stream, assuming the host endianness matched the network
    endianness. It worked fine on their development machine. But when deployed to
    a different cloud architecture with a different endianness (a rarity these days,
    but it happens), the service started reading completely nonsensical data. The
    fix was to replace the direct `Float32Array` view with a loop that used a `DataView`
    to read each float with the correct, explicitly-stated endianness. It was a painful
    reminder that hidden assumptions about the execution environment are a recipe
    for production failures. When in doubt, be explicit. Use `Buffer`'s `BE`/`LE`
    methods or use a `DataView`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª `DataView` ç±»å‹è½¬æ¢çœ‹èµ·æ¥æ²¡é—®é¢˜ï¼Œç›´åˆ°å®ƒç ´åäº†ä¸€åˆ‡ã€‚åœ¨æˆ‘ä»¬çš„ä¸€ä¸ªæœåŠ¡ä¸­ï¼Œä¸€ä½å¼€å‘è€…ä½¿ç”¨ `Float32Array` ä»ç½‘ç»œæµä¸­å¿«é€Ÿè§£æä¸€ç³»åˆ—æµ®ç‚¹æ•°ï¼Œå‡è®¾ä¸»æœºå­—èŠ‚åºä¸ç½‘ç»œå­—èŠ‚åºåŒ¹é…ã€‚åœ¨ä»–ä»¬çš„å¼€å‘æœºå™¨ä¸Šè¿è¡Œæ­£å¸¸ã€‚ä½†å½“éƒ¨ç½²åˆ°å…·æœ‰ä¸åŒå­—èŠ‚åºçš„ä¸åŒäº‘æ¶æ„ï¼ˆè¿™äº›å¤©å¾ˆç½•è§ï¼Œä½†ç¡®å®ä¼šå‘ç”Ÿï¼‰æ—¶ï¼ŒæœåŠ¡å¼€å§‹è¯»å–å®Œå…¨æ— æ„ä¹‰çš„éšæœºæ•°æ®ã€‚ä¿®å¤æ–¹æ³•æ˜¯æ›¿æ¢ç›´æ¥çš„
    `Float32Array` è§†å›¾ï¼Œä½¿ç”¨å¾ªç¯å’Œ `DataView` æ¥ä»¥æ­£ç¡®ã€æ˜ç¡®å£°æ˜çš„å­—èŠ‚åºè¯»å–æ¯ä¸ªæµ®ç‚¹æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªç—›è‹¦çš„æé†’ï¼Œéšè—çš„æ‰§è¡Œç¯å¢ƒå‡è®¾æ˜¯ç”Ÿäº§å¤±è´¥çš„é…æ–¹ã€‚å½“æœ‰ç–‘é—®æ—¶ï¼Œè¦æ˜ç¡®ã€‚ä½¿ç”¨
    `Buffer` çš„ `BE`/`LE` æ–¹æ³•æˆ–ä½¿ç”¨ `DataView`ã€‚
- en: Production Patterns for Zero-Copy
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›¶æ‹·è´çš„ç”Ÿäº§æ¨¡å¼
- en: After experiencing these production issues, my team developed a set of thoroughly
    validated patterns for working with buffers. These aren't just theoretical best
    practices; they are essential patterns for production systems.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ç»å†äº†è¿™äº›ç”Ÿäº§é—®é¢˜åï¼Œæˆ‘çš„å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—ç»è¿‡å……åˆ†éªŒè¯çš„æ¨¡å¼ï¼Œç”¨äºå¤„ç†ç¼“å†²åŒºã€‚è¿™äº›ä¸ä»…ä»…æ˜¯ç†è®ºä¸Šçš„æœ€ä½³å®è·µï¼›å®ƒä»¬æ˜¯ç”Ÿäº§ç³»ç»Ÿä¸­å¿…ä¸å¯å°‘çš„æ¨¡å¼ã€‚
- en: '**Pattern 1: The Temporary View for Synchronous Processing**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å¼ 1ï¼šç”¨äºåŒæ­¥å¤„ç†çš„ä¸´æ—¶è§†å›¾**'
- en: This is the most common and safest use of zero-copy. When you need to process
    a chunk of a larger buffer within a single function scope, a view is perfect.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é›¶æ‹·è´æœ€å¸¸è§ä¸”æœ€å®‰å…¨çš„ä½¿ç”¨æ–¹å¼ã€‚å½“ä½ éœ€è¦åœ¨å•ä¸ªå‡½æ•°ä½œç”¨åŸŸå†…å¤„ç†æ›´å¤§ç¼“å†²åŒºçš„ä¸€ä¸ªå—æ—¶ï¼Œè§†å›¾æ˜¯å®Œç¾çš„ã€‚
- en: '[PRE44]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This pattern is safe because the view''s lifetime is scoped to the function
    execution. The view is created, used, and becomes unreachable when the function
    returns. V8''s escape analysis can often optimize this further - if the view doesn''t
    escape the function, it might not even allocate the Buffer object on the heap,
    keeping everything in registers. The key insight: synchronous, function-scoped
    views are nearly always safe from retention issues.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å¼æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºè§†å›¾çš„å¯¿å‘½é™åˆ¶åœ¨å‡½æ•°æ‰§è¡ŒèŒƒå›´å†…ã€‚è§†å›¾è¢«åˆ›å»ºã€ä½¿ç”¨ï¼Œå¹¶åœ¨å‡½æ•°è¿”å›æ—¶å˜å¾—ä¸å¯è¾¾ã€‚V8çš„é€ƒé€¸åˆ†æé€šå¸¸å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–è¿™ä¸€ç‚¹â€”â€”å¦‚æœè§†å›¾æ²¡æœ‰é€ƒé€¸å‡½æ•°ï¼Œå®ƒç”šè‡³å¯èƒ½ä¸åœ¨å †ä¸Šåˆ†é…Bufferå¯¹è±¡ï¼Œè€Œå°†æ‰€æœ‰å†…å®¹ä¿æŒåœ¨å¯„å­˜å™¨ä¸­ã€‚å…³é”®æ´å¯Ÿï¼šåŒæ­¥ã€å‡½æ•°ä½œç”¨åŸŸçš„è§†å›¾å‡ ä¹æ€»æ˜¯å®‰å…¨äºä¿ç•™é—®é¢˜ã€‚
- en: '[PRE45]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We use a view here because the processing is temporary and synchronous. The
    view doesn't escape the function's scope. If we needed to store this chunk long-term
    or use it in an asynchronous callback, we'd copy instead. Here's why that decision
    matters...
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨è§†å›¾æ˜¯å› ä¸ºå¤„ç†æ˜¯ä¸´æ—¶çš„ä¸”åŒæ­¥çš„ã€‚è§†å›¾ä¸ä¼šè¶…å‡ºå‡½æ•°çš„ä½œç”¨åŸŸã€‚å¦‚æœæˆ‘ä»¬éœ€è¦é•¿æœŸå­˜å‚¨è¿™ä¸ªå—æˆ–ç”¨äºå¼‚æ­¥å›è°ƒï¼Œæˆ‘ä»¬ä¼šé€‰æ‹©å¤åˆ¶ã€‚è¿™é‡Œå°±æ˜¯ä¸ºä»€ä¹ˆè¿™ä¸ªå†³å®šå¾ˆé‡è¦çš„åŸå› ...
- en: '**Pattern 2: The Defensive Copy for Asynchronous Operations and Storage**'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å¼ 2ï¼šç”¨äºå¼‚æ­¥æ“ä½œå’Œå­˜å‚¨çš„é˜²å¾¡æ€§å¤åˆ¶**'
- en: Any time buffer data needs to cross an asynchronous boundary or be stored in
    a collection, you must assume it needs to be copied. The original buffer might
    be reused or garbage collected by the time your callback executes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ä»»ä½•éœ€è¦è·¨å¼‚æ­¥è¾¹ç•Œæˆ–å­˜å‚¨åœ¨é›†åˆä¸­çš„ç¼“å†²åŒºæ•°æ®ï¼Œä½ å¿…é¡»å‡è®¾å®ƒéœ€è¦è¢«å¤åˆ¶ã€‚åŸå§‹ç¼“å†²åŒºå¯èƒ½åœ¨ä½ å›è°ƒæ‰§è¡Œæ—¶è¢«é‡ç”¨æˆ–åƒåœ¾å›æ”¶ã€‚
- en: '[PRE46]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: These views are created for immediate processing. They're lightweight - just
    72 bytes each on the heap - but they hold references to `dataBuffer`'s entire
    ArrayBuffer. If we stored these views directly in our cache, we'd create a memory
    leak. The entire `dataBuffer` would be retained for as long as the cache entry
    exists, which could be hours or days in a production system.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è§†å›¾æ˜¯ä¸ºäº†ç«‹å³å¤„ç†è€Œåˆ›å»ºçš„ã€‚å®ƒä»¬å¾ˆè½»é‡çº§â€”â€”åœ¨å †ä¸Šæ¯ä¸ªè§†å›¾åªæœ‰72å­—èŠ‚â€”â€”ä½†å®ƒä»¬æŒæœ‰`dataBuffer`æ•´ä¸ªArrayBufferçš„å¼•ç”¨ã€‚å¦‚æœæˆ‘ä»¬ç›´æ¥å°†è¿™äº›è§†å›¾å­˜å‚¨åœ¨æˆ‘ä»¬çš„ç¼“å­˜ä¸­ï¼Œå°±ä¼šåˆ›å»ºå†…å­˜æ³„æ¼ã€‚æ•´ä¸ª`dataBuffer`å°†ä¿ç•™ï¼Œç›´åˆ°ç¼“å­˜æ¡ç›®å­˜åœ¨ï¼Œè¿™åœ¨ç”Ÿäº§ç³»ç»Ÿä¸­å¯èƒ½æ˜¯å‡ ä¸ªå°æ—¶æˆ–å‡ å¤©ã€‚
- en: '[PRE47]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Here, we create views to initially parse the buffer. But the moment we decide
    to put the `value` into our `longLivedCache`, we immediately create a copy. This
    ensures our cache entry is self-contained and doesn't unexpectedly hold a reference
    to a much larger `dataBuffer`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºè§†å›¾ä»¥æœ€åˆè§£æç¼“å†²åŒºã€‚ä½†å½“æˆ‘ä»¬å†³å®šå°†`value`æ”¾å…¥æˆ‘ä»¬çš„`longLivedCache`æ—¶ï¼Œæˆ‘ä»¬ç«‹å³åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ã€‚è¿™ç¡®ä¿æˆ‘ä»¬çš„ç¼“å­˜æ¡ç›®æ˜¯è‡ªåŒ…å«çš„ï¼Œå¹¶ä¸”ä¸ä¼šæ„å¤–åœ°æŒæœ‰å¯¹æ›´å¤§çš„`dataBuffer`çš„å¼•ç”¨ã€‚
- en: '**Pattern 3: The Parser Protocol (Views out, Copies in)**'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å¼ 3ï¼šè§£æå™¨åè®®ï¼ˆè¾“å‡ºè§†å›¾ï¼Œè¾“å…¥å‰¯æœ¬**ï¼‰'
- en: This is the library author's pattern. Write parsing functions that are purely
    zero-copy and return views. Document clearly that the returned values are views
    and may be invalidated if the original buffer changes.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åº“ä½œè€…çš„æ¨¡å¼ã€‚ç¼–å†™çº¯é›¶æ‹·è´çš„è§£æå‡½æ•°ï¼Œå¹¶è¿”å›è§†å›¾ã€‚æ˜ç¡®è®°å½•è¿”å›çš„å€¼æ˜¯è§†å›¾ï¼Œå¦‚æœåŸå§‹ç¼“å†²åŒºå‘ç”Ÿå˜åŒ–ï¼Œå®ƒä»¬å¯èƒ½ä¼šè¢«æ— æ•ˆåŒ–ã€‚
- en: '[PRE48]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This function contract is critical. The JSDoc explicitly warns that returned
    values are views. This shifts the memory management decision to the caller, who
    has more context about data lifetime. The function itself is pure and fast - no
    allocations beyond the two small Buffer objects for the views. This pattern scales
    to millions of operations per second because it does the minimum necessary work.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‡½æ•°åˆçº¦è‡³å…³é‡è¦ã€‚JSDocæ˜ç¡®è­¦å‘Šè¿”å›çš„å€¼æ˜¯è§†å›¾ã€‚è¿™æŠŠå†…å­˜ç®¡ç†å†³ç­–æƒäº¤ç»™äº†è°ƒç”¨è€…ï¼Œè°ƒç”¨è€…å¯¹æ•°æ®å¯¿å‘½æœ‰æ›´å¤šçš„ä¸Šä¸‹æ–‡ã€‚è¿™ä¸ªå‡½æ•°æœ¬èº«æ˜¯çº¯çš„ä¸”å¿«é€Ÿçš„â€”â€”é™¤äº†ä¸ºè§†å›¾çš„ä¸¤ä¸ªå°Bufferå¯¹è±¡è¿›è¡Œåˆ†é…ä¹‹å¤–ï¼Œæ²¡æœ‰å…¶ä»–åˆ†é…ã€‚å› ä¸ºè¿™ä¸ªæ¨¡å¼å°†æœ€å°å¿…è¦çš„å·¥ä½œæ‰©å±•åˆ°æ¯ç§’æ•°ç™¾ä¸‡æ¬¡æ“ä½œã€‚
- en: '[PRE49]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This pattern provides maximum performance for consumers who can handle the data
    immediately and maximum safety for those who need to store it, by forcing them
    to be explicit about their intentions.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å¼é€šè¿‡å¼ºåˆ¶ä»–ä»¬æ˜ç¡®è‡ªå·±çš„æ„å›¾ï¼Œä¸ºå¯ä»¥ç«‹å³å¤„ç†æ•°æ®çš„æ¶ˆè´¹è€…æä¾›æœ€å¤§æ€§èƒ½ï¼Œä¸ºéœ€è¦å­˜å‚¨æ•°æ®çš„æ¶ˆè´¹è€…æä¾›æœ€å¤§å®‰å…¨æ€§ã€‚
- en: Debugging Memory Issues with Views
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è§†å›¾è°ƒè¯•å†…å­˜é—®é¢˜
- en: When you suspect a view-related memory leak, your primary tool is the heap snapshot.
    You can generate these using the Chrome DevTools for Node.js or programmatically
    with modules like `heapdump`.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ æ€€ç–‘ä¸è§†å›¾ç›¸å…³çš„å†…å­˜æ³„æ¼æ—¶ï¼Œä½ çš„ä¸»è¦å·¥å…·æ˜¯å †å¿«ç…§ã€‚ä½ å¯ä»¥ä½¿ç”¨Node.jsçš„Chrome DevToolsæˆ–ä½¿ç”¨åƒ`heapdump`è¿™æ ·çš„æ¨¡å—æ¥ç”Ÿæˆè¿™äº›å¿«ç…§ã€‚
- en: 'The process is usually:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¿‡ç¨‹é€šå¸¸å¦‚ä¸‹ï¼š
- en: Take a heap snapshot when your application is in a stable, low-memory state.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“ä½ çš„åº”ç”¨ç¨‹åºå¤„äºç¨³å®šã€ä½å†…å­˜çŠ¶æ€æ—¶ï¼Œè¿›è¡Œå †å¿«ç…§ã€‚
- en: Apply a load to your application that you suspect triggers the leak.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‘ä½ çš„åº”ç”¨ç¨‹åºæ–½åŠ ä½ è®¤ä¸ºå¯èƒ½è§¦å‘æ³„æ¼çš„è´Ÿè½½ã€‚
- en: Take a second heap snapshot.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å†æ¬¡è¿›è¡Œå †å¿«ç…§ã€‚
- en: Take a third snapshot after some more time to confirm the growth trend.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ä¸€æ®µæ—¶é—´åå†æ¬¡è¿›è¡Œç¬¬ä¸‰æ¬¡å¿«ç…§ä»¥ç¡®è®¤å¢é•¿è¶‹åŠ¿ã€‚
- en: In the snapshot viewer, you'll want to use the "Comparison" view to see what
    objects were allocated between snapshots. When debugging our log parser, we saw
    a massive increase in the number of `Buffer` objects.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¿«ç…§æŸ¥çœ‹å™¨ä¸­ï¼Œä½ å°†æƒ³è¦ä½¿ç”¨â€œæ¯”è¾ƒâ€è§†å›¾æ¥æŸ¥çœ‹å¿«ç…§ä¹‹é—´åˆ†é…äº†å“ªäº›å¯¹è±¡ã€‚å½“è°ƒè¯•æˆ‘ä»¬çš„æ—¥å¿—è§£æå™¨æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†`Buffer`å¯¹è±¡æ•°é‡çš„å·¨å¤§å¢åŠ ã€‚
- en: ğŸ’¡Tip
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡æç¤º
- en: Use Chrome DevTools with `node --inspect-brk` for memory profiling. The "Retained
    Size" column is key - it shows memory kept alive by each object. Look for small
    Buffers with huge retained sizes - that's the signature of view-based leaks.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`node --inspect-brk`ä¸Chrome DevToolsè¿›è¡Œå†…å­˜åˆ†æã€‚å…³é”®çš„æ˜¯â€œä¿ç•™å¤§å°â€åˆ—â€”â€”å®ƒæ˜¾ç¤ºäº†æ¯ä¸ªå¯¹è±¡ä¿ç•™çš„å†…å­˜ã€‚å¯»æ‰¾å…·æœ‰å·¨å¤§ä¿ç•™å¤§å°çš„å¾®å°ç¼“å†²åŒºâ€”â€”è¿™æ˜¯åŸºäºè§†å›¾çš„æ³„æ¼çš„æ ‡å¿—ã€‚
- en: When you click on one of these `Buffer` objects, the profiler will show you
    its properties. The key is to look for the internal reference to the parent buffer.
    In Chrome DevTools, this is often shown under a property like `[[backing_store]]`
    or by inspecting the object's retainers. You'll see your tiny 16-byte `Buffer`
    slice, and in its retainer chain, you will find the massive multi-megabyte parent
    `Buffer` it's keeping alive.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ ç‚¹å‡»è¿™äº›`Buffer`å¯¹è±¡ä¹‹ä¸€æ—¶ï¼Œåˆ†æå™¨å°†æ˜¾ç¤ºå…¶å±æ€§ã€‚å…³é”®æ˜¯å¯»æ‰¾å¯¹çˆ¶ç¼“å†²åŒºçš„å†…éƒ¨å¼•ç”¨ã€‚åœ¨Chrome DevToolsä¸­ï¼Œè¿™é€šå¸¸åœ¨åƒ`[[backing_store]]`è¿™æ ·çš„å±æ€§ä¸‹æ˜¾ç¤ºï¼Œæˆ–è€…é€šè¿‡æ£€æŸ¥å¯¹è±¡ä¿ç•™è€…ã€‚ä½ ä¼šçœ‹åˆ°ä½ é‚£å¾®å°çš„16å­—èŠ‚`Buffer`åˆ‡ç‰‡ï¼Œåœ¨å…¶ä¿ç•™é“¾ä¸­ï¼Œä½ ä¼šæ‰¾åˆ°å®ƒæ‰€ä¿æŒæ´»çš„å·¨å¤§å¤šå…†å­—èŠ‚çˆ¶`Buffer`ã€‚
- en: Another powerful technique is to use `process.memoryUsage()` that we've gone
    through a lot of times already.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯æ˜¯ä½¿ç”¨æˆ‘ä»¬å·²å¤šæ¬¡è®¨è®ºè¿‡çš„`process.memoryUsage()`ã€‚
- en: ğŸ’¡Tip
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡æç¤º
- en: In Node.js 13.9.0+, use `process.memoryUsage().arrayBuffers` to specifically
    track Buffer memory. This is more accurate than `external` which includes other
    C++ allocations.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Node.js 13.9.0+ä¸­ï¼Œä½¿ç”¨`process.memoryUsage().arrayBuffers`æ¥ä¸“é—¨è·Ÿè¸ªç¼“å†²åŒºå†…å­˜ã€‚è¿™æ¯”åŒ…æ‹¬å…¶ä»–C++åˆ†é…çš„`external`æ›´å‡†ç¡®ã€‚
- en: In our leak, `heapUsed` was growing slowly, but `external` and `rss` were exploding.
    This told us the leak wasn't in standard JavaScript objects but in the external
    memory managed by Node.js - a classic signature of a `Buffer` retention problem.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ³„æ¼ä¸­ï¼Œ`heapUsed`ç¼“æ…¢å¢é•¿ï¼Œä½†`external`å’Œ`rss`å´åœ¨çˆ†ç‚¸æ€§å¢é•¿ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬æ³„æ¼ä¸åœ¨æ ‡å‡†JavaScriptå¯¹è±¡ä¸­ï¼Œè€Œæ˜¯åœ¨Node.jsç®¡ç†çš„å¤–éƒ¨å†…å­˜ä¸­â€”â€”è¿™æ˜¯`Buffer`ä¿ç•™é—®é¢˜çš„ç»å…¸ç‰¹å¾ã€‚
- en: After profiling, we discovered our views were aliasing each other in another
    service. We had a circular buffer implementation where we would wrap around by
    creating a view. A bug in our offset logic caused a new view to overlap slightly
    with an old view, inadvertently keeping the old view (and thus the entire buffer)
    alive far longer than intended. The heap snapshot was the only way to visualize
    that chain of references.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†æåï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„è§†å›¾åœ¨å¦ä¸€ä¸ªæœåŠ¡ä¸­ç›¸äº’åˆ«åã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå¾ªç¯ç¼“å†²åŒºå®ç°ï¼Œæˆ‘ä»¬ä¼šé€šè¿‡åˆ›å»ºè§†å›¾æ¥ç¯ç»•ã€‚æˆ‘ä»¬åç§»é€»è¾‘ä¸­çš„é”™è¯¯å¯¼è‡´æ–°è§†å›¾ä¸æ—§è§†å›¾ç•¥æœ‰é‡å ï¼Œæ— æ„ä¸­ä½¿æ—§è§†å›¾ï¼ˆä»¥åŠæ•´ä¸ªç¼“å†²åŒºï¼‰æ¯”é¢„æœŸå­˜æ´»å¾—æ›´ä¹…ã€‚å †å¿«ç…§æ˜¯å”¯ä¸€å¯è§†åŒ–è¿™äº›å¼•ç”¨é“¾çš„æ–¹æ³•ã€‚
- en: Best Practices for Buffer Manipulation
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºæ“ä½œçš„æœ€ä½³å®è·µ
- en: If I could distill all this pain and suffering down into a set of guiding principles,
    it would be these.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘èƒ½å°†æ‰€æœ‰è¿™äº›ç—›è‹¦å’Œè‹¦éš¾æµ“ç¼©æˆä¸€å¥—æŒ‡å¯¼åŸåˆ™ï¼Œé‚£å°†æ˜¯è¿™äº›ã€‚
- en: After enough production incidents, you learn to **profile memory retention before
    deploying** any new code that heavily manipulates buffers. You don't just test
    for correctness; you test for memory behavior under load. You start using **views
    for temporary, synchronous processing** but reach for **explicit copies for any
    data that is long-lived, asynchronous, or stored in a collection**. You internalize
    the parent-child relationship between a view and its underlying buffer because
    you've debugged the alternative at 3 AM. You **test with memory profilers** because
    you've been burned by assumptions one too many times.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»å†äº†è¶³å¤Ÿçš„ç”Ÿäº§äº‹æ•…åï¼Œä½ å­¦ä¼šäº†åœ¨éƒ¨ç½²ä»»ä½•å¤§é‡æ“ä½œç¼“å†²åŒºçš„æ–°ä»£ç ä¹‹å‰**åˆ†æå†…å­˜ä¿ç•™æƒ…å†µ**ã€‚ä½ ä¸ä»…æµ‹è¯•æ­£ç¡®æ€§ï¼Œè¿˜æµ‹è¯•è´Ÿè½½ä¸‹çš„å†…å­˜è¡Œä¸ºã€‚ä½ å¼€å§‹ä½¿ç”¨**è§†å›¾è¿›è¡Œä¸´æ—¶ã€åŒæ­¥å¤„ç†**ï¼Œä½†å¯¹äºä»»ä½•é•¿æœŸå­˜åœ¨ã€å¼‚æ­¥æˆ–å­˜å‚¨åœ¨é›†åˆä¸­çš„æ•°æ®ï¼Œä½ éƒ½ä¼šå¯»æ±‚**æ˜¾å¼å¤åˆ¶**ã€‚ä½ å†…åŒ–äº†è§†å›¾ä¸å…¶åº•å±‚ç¼“å†²åŒºä¹‹é—´çš„çˆ¶å­å…³ç³»ï¼Œå› ä¸ºä½ å·²ç»åœ¨å‡Œæ™¨3ç‚¹è°ƒè¯•äº†æ›¿ä»£æ–¹æ¡ˆã€‚ä½ **ä½¿ç”¨å†…å­˜åˆ†æå™¨è¿›è¡Œæµ‹è¯•**ï¼Œå› ä¸ºä½ å·²ç»å› ä¸ºå‡è®¾è€Œå¤šæ¬¡è¢«çƒ§ä¼¤ã€‚
- en: You **document your function signatures relentlessly**. If a function returns
    a view, you scream it from the rooftops in the JSDoc comments. You make it impossible
    for the next developer to accidentally misuse your API and create a leak. You
    learn to recognize the code smell of a `slice()` or `subarray()` whose result
    is being assigned to a variable with a wider scope, like an object property or
    a module-level variable. You see that and you immediately ask, "Shouldn't that
    be a copy?"
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ **ä¸æ‡ˆåœ°è®°å½•å‡½æ•°ç­¾å**ã€‚å¦‚æœä¸€ä¸ªå‡½æ•°è¿”å›ä¸€ä¸ªè§†å›¾ï¼Œä½ ä¼šåœ¨JSDocæ³¨é‡Šä¸­å¤§å£°ç–¾å‘¼ã€‚ä½ ä½¿ä¸‹ä¸€ä¸ªå¼€å‘è€…ä¸å¯èƒ½æ„å¤–åœ°è¯¯ç”¨ä½ çš„APIå¹¶åˆ›å»ºæ³„æ¼ã€‚ä½ å­¦ä¼šäº†è¯†åˆ«
    `slice()` æˆ– `subarray()` çš„ä»£ç å¼‚å‘³ï¼Œå…¶ç»“æœè¢«åˆ†é…ç»™å…·æœ‰æ›´å¹¿ä½œç”¨åŸŸçš„å˜é‡ï¼Œå¦‚å¯¹è±¡å±æ€§æˆ–æ¨¡å—çº§å˜é‡ã€‚ä½ çœ‹åˆ°è¿™ä¸€ç‚¹ï¼Œå°±ä¼šç«‹åˆ»é—®ï¼Œâ€œé‚£ä¸åº”è¯¥æ˜¯ä¸€ä¸ªå¤åˆ¶å—ï¼Ÿâ€
- en: And most importantly, you **treat every zero-copy operation with suspicion**.
    You don't see it as a free performance boost; you see it as a powerful tool with
    significant risks. You ask yourself, "What is the lifetime of the data I'm creating?
    And what is the lifetime of the data I'm referencing?" If those two lifetimes
    are different, a copy is almost always the right answer.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€é‡è¦çš„æ˜¯ï¼Œä½ **å¯¹æ¯ä¸ªé›¶æ‹·è´æ“ä½œéƒ½æŒæ€€ç–‘æ€åº¦**ã€‚ä½ ä¸å°†å…¶è§†ä¸ºå…è´¹æ€§èƒ½æå‡ï¼›ä½ å°†å…¶è§†ä¸ºä¸€ä¸ªå…·æœ‰é‡å¤§é£é™©çš„å¼ºå¤§å·¥å…·ã€‚ä½ é—®è‡ªå·±ï¼Œâ€œæˆ‘åˆ›å»ºçš„æ•°æ®çš„ç”Ÿå‘½å‘¨æœŸæ˜¯ä»€ä¹ˆï¼Ÿæˆ‘å¼•ç”¨çš„æ•°æ®çš„ç”Ÿå‘½å‘¨æœŸæ˜¯ä»€ä¹ˆï¼Ÿâ€å¦‚æœè¿™ä¸¤ä¸ªç”Ÿå‘½å‘¨æœŸä¸åŒï¼Œå¤åˆ¶å‡ ä¹æ€»æ˜¯æ­£ç¡®çš„ç­”æ¡ˆã€‚
- en: Memory Profiling Data
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…å­˜åˆ†ææ•°æ®
- en: Here is a sample of the kind of data we collected during our investigation.
    The test creates 100,000 small objects derived from a single 50MB buffer.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æˆ‘ä»¬è°ƒæŸ¥æœŸé—´æ”¶é›†åˆ°çš„æ•°æ®æ ·æœ¬ã€‚æµ‹è¯•åˆ›å»ºäº†ä¸€ä¸ªç”±å•ä¸ª50MBç¼“å†²åŒºæ´¾ç”Ÿå‡ºçš„100,000ä¸ªå°å¯¹è±¡ã€‚
- en: '**Test Scenario 1: Using `slice()` (creating views)**'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**æµ‹è¯•åœºæ™¯1ï¼šä½¿ç”¨ `slice()`ï¼ˆåˆ›å»ºè§†å›¾**ï¼‰'
- en: '[PRE50]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: â„¹ï¸Note
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: â„¹ï¸æ³¨æ„
- en: With Node.js 22+'s native TypeScript support, you can run TypeScript buffer
    code directly without transpilation. Use `node --experimental-strip-types` for
    .ts files with buffer operations.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Node.js 22+çš„æœ¬åœ°TypeScriptæ”¯æŒä¸‹ï¼Œä½ å¯ä»¥ç›´æ¥è¿è¡ŒTypeScriptç¼“å†²åŒºä»£ç ï¼Œæ— éœ€è½¬è¯‘ã€‚å¯¹äºè¿›è¡Œç¼“å†²åŒºæ“ä½œçš„.tsæ–‡ä»¶ï¼Œä½¿ç”¨
    `node --experimental-strip-types`ã€‚
- en: '**`process.memoryUsage()` Output:**'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`process.memoryUsage()` è¾“å‡ºï¼š**'
- en: '`rss`: ~78 MB'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rss`: ~78 MB'
- en: '`heapUsed`: ~8 MB'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`heapUsed`: ~8 MB'
- en: '`external`: ~50.5 MB'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`external`: ~50.5 MB'
- en: '**Heap Snapshot Analysis:**'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å †å¿«ç…§åˆ†æï¼š**'
- en: 'Shallow size of all `Buffer` objects in the `views` array: ~800 KB (100,000
    * ~8 bytes/object)'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`views` æ•°ç»„ä¸­æ‰€æœ‰ `Buffer` å¯¹è±¡çš„æµ…å±‚å¤§å°ï¼š~800 KBï¼ˆ100,000 * ~8å­—èŠ‚/å¯¹è±¡ï¼‰'
- en: 'Retained size: **~50 MB**. The entire `largeBuffer` is retained by the views.'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿ç•™å¤§å°ï¼š**~50 MB**ã€‚æ•´ä¸ª `largeBuffer` éƒ½è¢«è§†å›¾ä¿ç•™ã€‚
- en: '**Test Scenario 2: Using a strategic copy**'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**æµ‹è¯•åœºæ™¯2ï¼šä½¿ç”¨æˆ˜ç•¥å¤åˆ¶**'
- en: '[PRE51]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Each iteration creates a temporary view with `slice(0, 10)`, then immediately
    copies it with `Buffer.from()`. The temporary view is eligible for collection
    as soon as `Buffer.from()` completes. The copy has its own 10-byte ArrayBuffer
    with no reference to `largeBuffer`. After the loop, we have 100,000 independent
    10-byte buffers totaling ~1MB of memory, and `largeBuffer` can be garbage collected,
    freeing 50MB.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡è¿­ä»£éƒ½ä¼šä½¿ç”¨ `slice(0, 10)` åˆ›å»ºä¸€ä¸ªä¸´æ—¶è§†å›¾ï¼Œç„¶åç«‹å³ä½¿ç”¨ `Buffer.from()` è¿›è¡Œå¤åˆ¶ã€‚ä¸´æ—¶è§†å›¾åœ¨ `Buffer.from()`
    å®Œæˆåå³å¯è¿›è¡Œå›æ”¶ã€‚å¤åˆ¶æ‹¥æœ‰è‡ªå·±çš„10å­—èŠ‚ ArrayBufferï¼Œä¸ `largeBuffer` æ— å…³ã€‚åœ¨å¾ªç¯ç»“æŸåï¼Œæˆ‘ä»¬æ‹¥æœ‰100,000ä¸ªç‹¬ç«‹çš„10å­—èŠ‚ç¼“å†²åŒºï¼Œæ€»å…±çº¦1MBçš„å†…å­˜ï¼Œè€Œ
    `largeBuffer` å¯ä»¥è¢«åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾50MBã€‚
- en: '[PRE52]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '**`process.memoryUsage()` Output (after GC is triggered):**'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`process.memoryUsage()` è¾“å‡ºï¼ˆè§¦å‘åƒåœ¾å›æ”¶åï¼‰ï¼š**'
- en: '`rss`: ~32 MB'
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rss`: ~32 MB'
- en: '`heapUsed`: ~9 MB'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`heapUsed`: ~9 MB'
- en: '`external`: ~1.5 MB'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`external`: ~1.5 MB'
- en: '**Heap Snapshot Analysis:**'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å †å¿«ç…§åˆ†æï¼š**'
- en: The 50MB `largeBuffer` is gone.
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 50MBçš„ `largeBuffer` å·²æ¶ˆå¤±ã€‚
- en: The `copies` array holds 100,000 small `Buffer` objects, each with its own 10-byte
    backing store. Total external memory is approximately 1MB (100,000 * 10 bytes)
    plus some overhead.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`copies`æ•°ç»„åŒ…å«100,000ä¸ªå°çš„`Buffer`å¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡éƒ½æœ‰è‡ªå·±çš„10å­—èŠ‚åç«¯å­˜å‚¨ã€‚æ€»å¤–éƒ¨å†…å­˜å¤§çº¦ä¸º1MBï¼ˆ100,000 * 10å­—èŠ‚ï¼‰åŠ ä¸Šä¸€äº›å¼€é”€ã€‚'
- en: These measurements clearly quantify the trade-off. The view-based approach used
    less CPU upfront but retained 50MB of memory it didn't need. The copy-based approach
    used slightly more CPU in the loop but resulted in a vastly smaller memory footprint.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æµ‹é‡æ¸…æ¥šåœ°é‡åŒ–äº†æƒè¡¡ã€‚åŸºäºè§†å›¾çš„æ–¹æ³•åœ¨åˆæœŸä½¿ç”¨äº†æ›´å°‘çš„CPUï¼Œä½†ä¿ç•™äº†å®ƒä¸éœ€è¦çš„50MBå†…å­˜ã€‚åŸºäºå‰¯æœ¬çš„æ–¹æ³•åœ¨å¾ªç¯ä¸­ä½¿ç”¨äº†ç¨å¾®æ›´å¤šçš„CPUï¼Œä½†ç»“æœå†…å­˜å ç”¨å¤§å¤§å‡å°ã€‚
- en: Closing
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æŸè¯­
- en: â„¹ï¸Note
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: â„¹ï¸æ³¨æ„
- en: With Node.js 22+'s native TypeScript support, you can write type-safe buffer
    operations without a build step. TypeScript's type system can help catch buffer
    misuse at compile time, preventing many of the runtime issues discussed in this
    chapter.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Node.js 22+çš„æœ¬åœ°TypeScriptæ”¯æŒä¸‹ï¼Œä½ å¯ä»¥ç¼–å†™æ— éœ€æ„å»ºæ­¥éª¤çš„ç±»å‹å®‰å…¨ç¼“å†²åŒºæ“ä½œã€‚TypeScriptçš„ç±»å‹ç³»ç»Ÿå¯ä»¥åœ¨ç¼–è¯‘æ—¶å¸®åŠ©æ•è·ç¼“å†²åŒºè¯¯ç”¨ï¼Œä»è€Œé˜²æ­¢æœ¬ç« è®¨è®ºçš„è®¸å¤šè¿è¡Œæ—¶é—®é¢˜ã€‚
- en: I still remember one of my mentee who, asked with genuine curiosity, "So why
    don't we just use copies everywhere? It seems safer." It's a fair question. The
    answer is that real engineering is about making informed trade-offs. We could
    copy everything, and our applications would be simpler to reason about but also
    slower and less efficient. We could have services that use twice the CPU and memory
    they need to, and in a large-scale system, that's a cost you can't afford.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»ç„¶è®°å¾—æˆ‘çš„ä¸€ä¸ªå¾’å¼Ÿï¼Œä»–å¸¦ç€çœŸæ­£çš„å¥½å¥‡å¿ƒé—®é“ï¼šâ€œé‚£ä¹ˆæˆ‘ä»¬ä¸ºä»€ä¹ˆä¸åœ¨æ¯ä¸ªåœ°æ–¹éƒ½ä½¿ç”¨å‰¯æœ¬å‘¢ï¼Ÿè¿™ä¼¼ä¹æ›´å®‰å…¨ã€‚â€è¿™æ˜¯ä¸€ä¸ªåˆç†çš„é—®é¢˜ã€‚ç­”æ¡ˆæ˜¯ï¼ŒçœŸæ­£çš„å·¥ç¨‹æ˜¯å…³äºåšå‡ºæ˜æ™ºçš„æƒè¡¡ã€‚æˆ‘ä»¬å¯ä»¥å¤åˆ¶ä¸€åˆ‡ï¼Œæˆ‘ä»¬çš„åº”ç”¨ç¨‹åºå°†æ›´å®¹æ˜“æ¨ç†ï¼Œä½†ä¹Ÿä¼šæ›´æ…¢ã€æ•ˆç‡æ›´ä½ã€‚æˆ‘ä»¬å¯ä»¥æ‹¥æœ‰ä½¿ç”¨ä¸¤å€CPUå’Œå†…å­˜çš„æœåŠ¡ï¼Œåœ¨ä¸€ä¸ªå¤§è§„æ¨¡ç³»ç»Ÿä¸­ï¼Œè¿™æ˜¯ä½ è´Ÿæ‹…ä¸èµ·çš„æˆæœ¬ã€‚
- en: The goal isn't to fear zero-copy operations instead we should respect them.
    It's to understand that shared memory is a mechanism with both powerful advantages
    and serious risks. When you create a view, you are making a promise to the runtime
    - a promise that you understand the lifecycle of both the view and its parent.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡ä¸æ˜¯å®³æ€•é›¶æ‹·è´æ“ä½œï¼Œç›¸åï¼Œæˆ‘ä»¬åº”è¯¥å°Šé‡å®ƒä»¬ã€‚è¿™æ˜¯è¦ç†è§£å…±äº«å†…å­˜æ˜¯ä¸€ç§æ—¢æœ‰å¼ºå¤§ä¼˜åŠ¿åˆæœ‰ä¸¥é‡é£é™©çš„æœºåˆ¶ã€‚å½“ä½ åˆ›å»ºä¸€ä¸ªè§†å›¾æ—¶ï¼Œä½ æ˜¯åœ¨å‘è¿è¡Œæ—¶åšå‡ºæ‰¿è¯ºâ€”â€”ä¸€ä¸ªæ‰¿è¯ºï¼Œå³ä½ ç†è§£è§†å›¾åŠå…¶çˆ¶è§†å›¾çš„ç”Ÿå‘½å‘¨æœŸã€‚
- en: Mastery is about understanding the consequences of each call. It's about looking
    at `const view = buf.slice(0, 10)` and not just seeing a line of code, but seeing
    the internal reference it creates back to the parent buffer and asking, "Is that
    a reference I'm prepared to manage?" When you can answer that question instinctively,
    you'll never look at memory the same way again.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ç²¾é€šæ˜¯å…³äºç†è§£æ¯ä¸ªè°ƒç”¨çš„åæœã€‚è¿™æ˜¯å…³äºæŸ¥çœ‹`const view = buf.slice(0, 10)`ï¼Œè€Œä¸ä»…ä»…æ˜¯çœ‹åˆ°ä¸€è¡Œä»£ç ï¼Œè€Œæ˜¯çœ‹åˆ°å®ƒåˆ›å»ºçš„æŒ‡å‘çˆ¶ç¼“å†²åŒºçš„å†…éƒ¨å¼•ç”¨ï¼Œå¹¶é—®è‡ªå·±ï¼šâ€œè¿™æ˜¯å¦æ˜¯æˆ‘å‡†å¤‡ç®¡ç†çš„å¼•ç”¨ï¼Ÿâ€å½“ä½ æœ¬èƒ½åœ°å›ç­”è¿™ä¸ªé—®é¢˜æ—¶ï¼Œä½ å°†æ°¸è¿œä¸ä¼šä»¥åŒæ ·çš„æ–¹å¼çœ‹å¾…å†…å­˜ã€‚
