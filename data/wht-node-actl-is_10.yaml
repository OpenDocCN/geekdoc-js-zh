- en: Foundation of Streams
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流的基石
- en: 原文：[https://www.thenodebook.com/streams/foundation-of-streams](https://www.thenodebook.com/streams/foundation-of-streams)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.thenodebook.com/streams/foundation-of-streams](https://www.thenodebook.com/streams/foundation-of-streams)
- en: 'Before we write a single line of code that uses Node.js streams, we need to
    understand the fundamental problem they solve. This is not a problem unique to
    Node.js, or even to JavaScript. It is a problem as old as computing itself: **how
    do we process data that is larger than the memory available to hold it?**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用 Node.js 流编写任何一行代码之前，我们需要理解它们解决的基本问题。这不是 Node.js 或 JavaScript 独有的问题，甚至也不是计算机科学独有的问题。这是一个与计算机一样古老的问题：**我们如何处理大于可用内存的数据？**
- en: This question may seem simple, but its answer has shaped the architecture of
    operating systems, databases, network protocols, and nearly every system that
    handles real-world data at scale. Node.js streams are not an arbitrary API design
    choice. They are a direct, inevitable response to the constraints of physical
    memory and the realities of I/O operations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可能看起来很简单，但它的答案塑造了操作系统的架构、数据库、网络协议，以及几乎处理大规模真实世界数据的每个系统的架构。Node.js 流不是一个任意的
    API 设计选择。它们是对物理内存限制和 I/O 操作现实的直接、必然的回应。
- en: The Problem with Large Data
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据问题
- en: Let us start with a realistic scenario. You are building a web service that
    needs to process uploaded files. Users can upload images, videos, documents -
    any file type. The service must read these files, perhaps transform them in some
    way (compress an image, extract metadata from a video, scan for viruses), and
    then store them or send them elsewhere.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一种现实场景开始。你正在构建一个需要处理上传文件的网络服务。用户可以上传图片、视频、文档——任何类型的文件。该服务必须读取这些文件，可能以某种方式对其进行转换（压缩图片、从视频中提取元数据、扫描病毒），然后存储它们或发送到其他地方。
- en: 'The most straightforward approach - the one that immediately comes to mind
    - is this: read the entire file into memory as a single Buffer, perform your operations
    on that Buffer, and then write the result. In code, this looks simple:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的方法——即我们首先想到的方法——是这样的：将整个文件作为一个单一的 Buffer 读入内存，在该 Buffer 上执行操作，然后写入结果。在代码中，这看起来很简单：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Three lines. Clean. Easy to reason about. And for small files, this works perfectly.
    But what happens when a user uploads a 2GB video file? Or a 10GB database dump?
    Suddenly, your simple program must allocate 2GB of memory just to hold that one
    file. If ten users upload files simultaneously, you need 20GB of memory. This
    approach does not scale.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 三行。干净。易于推理。对于小文件，这种方法完美适用。但当一个用户上传一个 2GB 的视频文件，或者一个 10GB 的数据库转储文件时，会发生什么？突然之间，你的简单程序必须分配
    2GB 的内存来仅保存那个文件。如果有十个用户同时上传文件，你需要 20GB 的内存。这种方法无法扩展。
- en: 'But the problem runs deeper than just memory capacity. Even if your server
    has 128GB of RAM, loading an entire 2GB file into memory means you must wait for
    the entire file to be read from disk (or received over the network) before you
    can begin processing it. If reading that file takes 5 seconds, your program sits
    idle for 5 seconds before the first byte is processed. Then, after processing
    is complete, you must write the entire 2GB back to disk or over the network, waiting
    again for the entire write operation to complete. The program is fundamentally
    synchronous in its data flow: read everything, then process everything, then write
    everything.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但问题不仅仅在于内存容量。即使你的服务器有 128GB 的 RAM，将整个 2GB 文件加载到内存中意味着你必须等待整个文件从磁盘（或通过网络）读取完毕，你才能开始处理它。如果读取该文件需要
    5 秒，那么你的程序在处理第一个字节之前将闲置 5 秒。然后，在处理完成后，你必须将整个 2GB 写回磁盘或通过网络，再次等待整个写入操作完成。程序在数据流上本质上是同步的：先读取所有内容，然后处理所有内容，最后写入所有内容。
- en: This is inefficient. While you are waiting for the disk to deliver the last
    megabyte of the file, you could already be processing the first megabyte. While
    you are processing the middle of the file, you could already be writing the processed
    beginning to the output. The operations - reading, processing, writing - could
    happen concurrently, overlapping in time. But the "read everything into memory"
    approach makes that concurrency impossible.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法效率低下。当你等待磁盘提供文件的最后一个兆字节时，你本可以处理第一个兆字节。当你正在处理文件的中间部分时，你本可以将处理好的开头写入输出。读取、处理、写入这些操作可以并发发生，在时间上重叠。但“将所有内容读入内存”的方法使得这种并发变得不可能。
- en: Sequential Processing
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 顺序处理
- en: Operations happen one after another - each must complete before the next begins
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 操作一个接一个发生——每个操作必须完成才能开始下一个
- en: READ5 secondsPROCESS3 secondsWRITE4 secondsTOTAL TIME12 seconds
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 读取5秒处理3秒写入4秒总时间12秒
- en: Why chunking?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么分块？
- en: 'The fundamental insight is this: **we do not need to hold the entire dataset
    in memory at once to process it**. We only need to hold the portion we are currently
    working on.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基本洞察是这样的：**我们不需要一次性将整个数据集保存在内存中以便处理**。我们只需要保存我们当前正在工作的部分。
- en: Consider a different approach. Instead of reading the entire file, what if we
    read just a small portion of it - say, 64 kilobytes - into memory? We process
    those 64 kilobytes. We write the result. Then we read the next 64 kilobytes, process
    them, write the result, and so on, until the entire file has been processed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑另一种方法。如果我们只读取文件的一小部分——比如说，64千字节——到内存中会怎样？我们处理这64千字节。我们写入结果。然后我们读取下一个64千字节，处理它们，写入结果，以此类推，直到整个文件被处理。
- en: This chunked processing solves both problems. First, our memory usage is now
    bounded by the chunk size, not the file size. Processing a 2GB file requires only
    64KB of memory at any given moment. Second, the operations can now overlap. While
    we are processing chunk N, the operating system can be reading chunk N+1 from
    disk in the background. While we are writing the processed chunk N to the output,
    we can simultaneously be processing chunk N+1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分块处理解决了这两个问题。首先，我们的内存使用现在由分块大小限制，而不是文件大小。处理2GB文件在任何给定时刻只需要64KB的内存。其次，操作现在可以重叠。当我们正在处理分块N时，操作系统可以在后台从磁盘读取分块N+1。当我们正在将处理后的分块N写入输出时，我们可以同时处理分块N+1。
- en: Memory Usage Comparison
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存使用比较
- en: 'Processing a 2GB file: entire vs. chunked approach'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 处理2GB文件：完整方式与分块方式
- en: Entire File in Memory
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将整个文件加载到内存中
- en: Load everything at once
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性加载所有内容
- en: MEMORY USAGE1200MBPeak Memory:2000 MBStatus:DANGEROUS
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 内存使用1200MB峰值内存：2000 MB状态：危险
- en: Chunked Processing
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分块处理
- en: Process in small pieces
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分块处理
- en: MEMORY USAGE64MBPeak Memory:64 MBStatus:SAFEEFFICIENCY GAIN31xLess memory required
    with chunked processing
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 内存使用64MB峰值内存：64 MB状态：安全效率提升31x分块处理需要的内存更少
- en: But this chunked approach introduces new complexity. We must manage the flow
    of chunks. We must decide when to read the next chunk, when to process it, and
    when to write it. We must handle the case where the producer of chunks (the file
    system, the network) is faster than the consumer (our processing logic), or vice
    versa. We must ensure that if an error occurs in the middle of processing, we
    clean up resources properly. We must signal when the data stream has ended.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这种分块方法引入了新的复杂性。我们必须管理分块流。我们必须决定何时读取下一个分块，何时处理它，何时写入它。我们必须处理这种情况，即分块的生产者（文件系统、网络）比消费者（我们的处理逻辑）快，或者反之亦然。我们必须确保如果在处理过程中发生错误，我们正确地清理资源。我们必须在数据流结束时发出信号。
- en: This is where the **stream paradigm** enters. A stream is an abstraction for
    managing the flow of chunked data. It handles the mechanics of reading chunks,
    buffering them when necessary, and delivering them to your processing logic. It
    provides a structured way to think about and implement chunked, asynchronous data
    processing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**流范式**介入的地方。流是管理分块数据流的抽象。它处理读取分块、在必要时缓冲它们，并将它们交付给您的处理逻辑的机制。它提供了一种结构化的方式来思考和实现分块、异步数据处理。
- en: The Two Fundamental Models of Streaming
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流的两种基本模型
- en: There are two fundamentally different ways to organize the flow of data in a
    streaming system. These models are not specific to Node.js. They represent two
    opposing philosophies about who controls the flow of data, and they appear in
    many different programming environments and paradigms.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在流系统中组织数据流有两种根本不同的方式。这些模型并不特定于Node.js。它们代表了两种关于谁控制数据流的相反哲学，并且出现在许多不同的编程环境和范例中。
- en: The first model is **push-based streaming**. In this model, the producer of
    data actively pushes chunks to the consumer. The producer decides when to send
    data. The consumer receives data whenever the producer chooses to send it. This
    is the model of event-driven systems. The producer emits events, and the consumer
    reacts to those events.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模型是基于**推送的流**。在这个模型中，数据的生产者主动将分块推送到消费者。生产者决定何时发送数据。消费者在生产者选择发送数据时接收数据。这是事件驱动系统的模型。生产者发出事件，消费者对这些事件做出反应。
- en: The second model is **pull-based streaming**. In this model, the consumer actively
    requests chunks from the producer. The consumer decides when it is ready for more
    data and explicitly asks for it. The producer responds to these requests. This
    is the model of iterator-based systems. The consumer iterates over the data source,
    pulling values one at a time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种模型是 **基于拉取的流**。在这个模型中，消费者主动从生产者请求数据块。消费者决定何时准备好更多数据，并明确请求它。生产者响应这些请求。这是基于迭代器的系统模型。消费者遍历数据源，一次拉取一个值。
- en: 'PUSH: Producer Controls'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**PUSH**: 生产者控制'
- en: Producer decides WHEN to send data
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者决定何时发送数据
- en: IN CONTROLPRODUCERsetInterval(() => emit())Waiting...REACTSCONSUMERon('data',
    handle)KEY TRAITS✓Multiple consumers can listen✓Event-driven architecture✗Consumer
    can be overwhelmed
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **CONTROL** PRODUCER 中设置定时器 `setInterval(() => emit())` 等待... **REACT** SCONSUMER
    监听 ('data', handle) 关键特性✓ 多个消费者可以监听✗ 消费者可能会过载
- en: 'PULL: Consumer Controls'
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**PULL**: 消费者控制'
- en: Consumer decides WHEN to request data
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者决定何时请求数据
- en: WAITSPRODUCERfunction* generate()Idle...IN CONTROLCONSUMERiterator.next()KEY
    TRAITS✓Natural backpressure control✓Lazy evaluation possible✗Typically single
    consumer
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**WAIT** SPRODUCER 函数 *generate* 空闲... **IN CONTROL** CONSUMER 迭代器.next() 关键特性✓
    自然背压控制✓ 可实现懒加载✗ 通常只有一个消费者'
- en: 'These two models have different characteristics, different trade-offs, and
    different use cases. Node.js streams, as we will see, attempt to combine both
    models into a single, flexible abstraction. But before we can understand that
    hybrid model, we must first understand the pure forms: push and pull.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种模型具有不同的特性，不同的权衡，以及不同的用例。正如我们将看到的，Node.js 流尝试将这两种模型结合成一个单一、灵活的抽象。但在我们理解这种混合模型之前，我们必须首先理解其纯形式：推送和拉取。
- en: Push Architecture
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推送架构
- en: 'The push model has deep roots in software design. It is formalized in the **Observer
    pattern**, one of the classic design patterns documented in the 1994 "Gang of
    Four" book. The Observer pattern describes a one-to-many dependency between objects:
    when the subject (the observable) changes state, all of its observers (the subscribers)
    are notified automatically.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 推送模型在软件设计中有着深厚的根源。它在 1994 年的 "Gang of Four" 书中记录的经典设计模式 **观察者模式** 中得到了形式化。观察者模式描述了对象之间的一对多依赖关系：当主题（可观察的）状态改变时，所有它的观察者（订阅者）都会自动收到通知。
- en: In the context of streaming, the subject is the data source, and the observers
    are the consumers of that data. When the data source has new data available, it
    notifies all registered consumers by pushing that data to them.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在流处理的上下文中，主题是数据源，观察者是该数据消费者。当数据源有新数据可用时，它会通过将数据推送到所有已注册的消费者来通知所有注册的消费者。
- en: In Node.js, the fundamental building block for push-based systems is the `EventEmitter`
    class. This class, which you have already encountered in your study of the event
    loop and asynchronous primitives, provides a simple but powerful mechanism for
    implementing the Observer pattern.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Node.js 中，基于推送系统的基本构建块是 `EventEmitter` 类。这个类，你在学习事件循环和异步原语时已经遇到过，提供了一个简单但强大的机制来实现观察者模式。
- en: Observer Pattern in Push Streams
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推送流中的观察者模式
- en: One EventEmitter broadcasts to multiple listeners simultaneously
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `EventEmitter` 同时向多个监听器广播
- en: AUTO-PLAYING - Next emit in 2sEVENT EMITTER
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**AUTO-PLAYING** - 下一个触发将在 2 秒后 **EVENT EMITTER**'
- en: Subject
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主题
- en: stream.emit('data', chunk)IDLE-waiting...LISTENER 1Observer⏳ Waiting...LISTENER
    2Observer⏳ Waiting...LISTENER 3Observer⏳ Waiting...CODE EXAMPLE
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`stream.emit(''data'', chunk)` 空闲 - 等待... **LISTENER 1** 观察者⏳ 等待... **LISTENER
    2** 观察者⏳ 等待... **LISTENER 3** 观察者⏳ 等待... 代码示例'
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let us build a simple push-based stream from scratch using `EventEmitter`. This
    will not be a production-ready stream implementation - Node.js already provides
    that - but building it ourselves will clarify the mechanics of the push model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从零开始构建一个简单的基于推送的流，使用 `EventEmitter`。这不会是一个生产就绪的流实现 - Node.js 已经提供了这样的实现 -
    但自己构建它将阐明推送模型的机制。
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This simple class extends `EventEmitter` and implements a push stream. It takes
    an array of data chunks in its constructor. When `start()` is called, it begins
    pushing chunks to any listeners by emitting `data` events. When all chunks have
    been pushed, it emits an `end` event to signal completion.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的类扩展了 `EventEmitter` 并实现了推送流。它在构造函数中接受一个数据块数组。当调用 `start()` 时，它通过触发 `data`
    事件开始向任何监听器推送块。当所有块都已推送后，它触发一个 `end` 事件以表示完成。
- en: 'The consumer uses this stream by registering event listeners:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者通过注册事件监听器来使用此流：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is the essence of the push model. The stream actively pushes data to the
    consumer. The consumer does not request data; it simply reacts to data when it
    arrives.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是推送模型的核心。数据流主动将数据推送给消费者。消费者不需要请求数据；它只是在数据到达时做出反应。
- en: Now, you might be wondering about the use of `setImmediate()` in the `_pushNext()`
    method. This is not strictly necessary for the logic to work, but it is important
    for the behavior. Without `setImmediate()`, all the data would be pushed synchronously
    in a tight loop during the call to `start()`. By using `setImmediate()`, we ensure
    that each chunk is pushed in a separate event loop tick. This gives the event
    loop a chance to process other events and prevents our stream from monopolizing
    the CPU. This is a simple form of yielding, a pattern you will see repeatedly
    in Node.js's asynchronous architecture.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道在`_pushNext()`方法中使用`setImmediate()`的原因。这对逻辑来说并非绝对必要，但对行为很重要。如果没有`setImmediate()`，所有数据都会在调用`start()`时同步地在一个紧密的循环中推送。通过使用`setImmediate()`，我们确保每个数据块都在单独的事件循环中推送。这给了事件循环处理其他事件的机会，并防止我们的流独占CPU。这是一种简单的让步形式，你将在Node.js的异步架构中反复看到这种模式。
- en: Push Model's Advantages and Limitations
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推送模型的优点和局限性
- en: The push model has several advantages. First, it is conceptually simple. The
    producer decides when to produce data, and the consumer simply reacts. This maps
    naturally to event-driven architectures, which are pervasive in Node.js.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 推送模型有几个优点。首先，它在概念上很简单。生产者决定何时生产数据，消费者只是做出反应。这自然映射到事件驱动架构，这在Node.js中很普遍。
- en: Second, the push model can be very efficient when the producer and consumer
    operate at similar speeds. If the producer can generate data as fast as the consumer
    can process it, the data flows smoothly with minimal buffering.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，当生产者和消费者以相似的速度运行时，推送模型可以非常高效。如果生产者可以像消费者一样快地生成数据，数据就会以最小的缓冲顺畅流动。
- en: Third, the push model allows for multiple consumers. Because the producer emits
    events, any number of listeners can subscribe to those events and receive the
    same data stream. This fan-out pattern is natural in the Observer pattern.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，推送模型允许有多个消费者。因为生产者发射事件，任何数量的监听器都可以订阅这些事件并接收相同的数据流。这种扇出模式在观察者模式中很自然。
- en: 'However, the push model has a fundamental problem: **backpressure**. What happens
    if the producer is faster than the consumer? In our simple implementation above,
    the producer pushes data as fast as it can, regardless of whether the consumer
    is ready for it. If the consumer takes time to process each chunk - perhaps it
    is writing to a slow disk or making a network request - the producer will keep
    pushing more data. These chunks must be buffered somewhere, waiting for the consumer
    to process them. The buffer grows unbounded, consuming memory, until eventually
    the program runs out of memory and crashes.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，推送模型有一个基本问题：**背压**。如果生产者比消费者快会发生什么？在我们上面的简单实现中，生产者尽可能快地推送数据，不管消费者是否准备好。如果消费者需要时间来处理每个数据块——也许它正在向慢速磁盘写入或进行网络请求——生产者会继续推送更多数据。这些数据块必须在某处缓冲，等待消费者处理。缓冲区会无限增长，消耗内存，直到最终程序因内存耗尽而崩溃。
- en: 'In a production push-based system, we need a mechanism for the consumer to
    signal to the producer: "I am not ready for more data yet. Slow down." This is
    backpressure. The consumer pushes back against the producer to regulate the flow.
    Implementing backpressure in a push-based system is non-trivial. The consumer
    must have a way to tell the producer to pause, and the producer must respect that
    signal. This requires a more sophisticated contract between producer and consumer
    than simply emitting events.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于生产的推送系统中，我们需要一个机制，让消费者向生产者发出信号：“我还没有准备好接收更多数据。请慢一点。”这就是背压。消费者通过向生产者施加反向压力来调节数据流。在基于推送的系统中实现背压并非易事。消费者必须有一种方式告诉生产者暂停，而生产者必须尊重这个信号。这需要生产者和消费者之间比仅仅发射事件更复杂的合同。
- en: Node.js streams implement backpressure, as we will see in later chapters. But
    the point here is that backpressure does not naturally fall out of the pure push
    model. It must be added as an additional layer of complexity.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js流实现了背压，正如我们将在后面的章节中看到的。但这里的重点是，背压并不是从纯推送模型自然产生的。它必须作为额外的复杂层添加。
- en: Backpressure Problem
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 背压问题
- en: Fast producer overwhelms slow consumer - buffer grows unbounded
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 快速生产者压倒慢速消费者——缓冲区无限增长
- en: FAST PRODUCEREmitting chunks rapidlySLOW CONSUMERProcessing slowlyBUFFER STATE0
    / 8 chunksBuffer OKBuffer within safe limits
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 快速产生 发射块快速 慢速消费者 慢慢处理 缓冲状态 0 / 8 块 缓冲在安全范围内
- en: Pull Architecture
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拉取架构
- en: The pull model inverts the control flow. Instead of the producer pushing data
    to the consumer, the consumer pulls data from the producer. The consumer decides
    when it is ready for the next chunk and explicitly requests it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 拉取模型反转了控制流。不是生产者将数据推送到消费者，而是消费者从生产者那里拉取数据。消费者决定何时准备好下一个块，并显式请求它。
- en: In JavaScript, the pull model is formalized in the **Iterator** and **Iterable**
    protocols. These protocols define a standard way for objects to produce a sequence
    of values on demand. You have likely used iterators without thinking deeply about
    them. When you write a `for...of` loop over an array, you are using the array's
    built-in iterator.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在 JavaScript 中，拉取模型在 **迭代器** 和 **可迭代** 协议中得到正式化。这些协议定义了对象按需产生值的标准方式。你可能已经使用了迭代器，但没有深入思考。当你在一个数组上写
    `for...of` 循环时，你正在使用数组的内置迭代器。
- en: 'Let us examine the Iterator protocol. An iterator is an object with a `next()`
    method. Each call to `next()` returns an object with two properties: `value` (the
    next item in the sequence) and `done` (a boolean indicating whether the sequence
    is complete).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查迭代器协议。迭代器是一个具有 `next()` 方法的对象。每次调用 `next()` 都会返回一个具有两个属性的对象：`value`（序列中的下一个项）和
    `done`（一个布尔值，指示序列是否完成）。
- en: 'Here is a simple pull-based stream implemented as an iterator:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个简单的基于拉取的流，实现为一个迭代器：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The consumer uses this stream by explicitly calling `next()` to pull each chunk:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者通过显式调用 `next()` 来使用此流，以拉取每个块：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is the essence of the pull model. The consumer is in control. It pulls
    data when it is ready. The producer simply responds to those pull requests.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是拉取模型的核心。消费者处于控制地位。它在准备好时拉取数据。生产者只需响应这些拉取请求。
- en: Generators and Iterable Protocol
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成器和可迭代协议
- en: 'JavaScript provides syntactic sugar for implementing iterators: **generator
    functions**. A generator function is a special kind of function that can pause
    its execution and resume later, yielding values one at a time. Generator functions
    are marked with an asterisk (`function*`) and use the `yield` keyword to produce
    values.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript 为实现迭代器提供了语法糖：**生成器函数**。生成器函数是一种特殊的函数，它可以暂停其执行并在稍后恢复，一次产生一个值。生成器函数用星号（`function*`）标记，并使用
    `yield` 关键字来产生值。
- en: 'Here is our pull stream reimplemented as a generator:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是将我们的拉取流重新实现为生成器：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This generator produces the same sequence of values as our manual iterator,
    but the syntax is much more concise. Under the hood, the generator function automatically
    implements the Iterator protocol. When you call a generator function, it returns
    an iterator object. Each call to the iterator's `next()` method resumes the generator
    function's execution until the next `yield` statement.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此生成器产生的值序列与我们的手动迭代器相同，但语法要简洁得多。在底层，生成器函数自动实现了迭代器协议。当你调用生成器函数时，它返回一个迭代器对象。每次调用迭代器的
    `next()` 方法都会恢复生成器函数的执行，直到下一个 `yield` 语句。
- en: Generators also implement the **Iterable** protocol. An iterable is an object
    that has a method with the key `Symbol.iterator`, which returns an iterator. Arrays
    are iterable. Strings are iterable. Generator functions return iterables.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器还实现了 **可迭代** 协议。可迭代对象是一个具有键 `Symbol.iterator` 的方法的对象，该方法返回一个迭代器。数组是可迭代的。字符串是可迭代的。生成器函数返回可迭代对象。
- en: 'Because generators are iterable, we can use them with `for...of` loops:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因为生成器是可迭代的，所以我们可以用 `for...of` 循环来使用它们：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `for...of` loop automatically calls the iterator's `next()` method behind
    the scenes, pulling values until `done` is `true`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`for...of` 循环在幕后自动调用迭代器的 `next()` 方法，拉取值，直到 `done` 为 `true`。'
- en: Generator Call-Yield-Resume Cycle
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成器调用-产生-恢复周期
- en: Interactive visualization of how generators pause and resume
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器暂停和恢复的交互式可视化
- en: STEP 1 OF 8INITIAL CALLGenerator created`const gen = myGenerator()`GENERATOR
    FUNCTION
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 步/8 步 初始调用 生成器创建 `const gen = myGenerator()` 生成器函数
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'CURRENT STATEPausedValues yielded: 0'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当前状态 暂停 已产生的值：0
- en: Async Iterators and `for await...of`
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步迭代器和 `for await...of`
- en: Generators solve the problem of synchronous sequences, but real-world data streams
    are asynchronous. Reading from a file, fetching from a network, querying a database
    - all of these operations are inherently asynchronous in Node.js. We need a way
    to pull data asynchronously.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器解决了同步序列的问题，但现实世界中的数据流是异步的。从文件中读取、从网络中获取、查询数据库——所有这些操作在 Node.js 中都是固有的异步操作。我们需要一种异步拉取数据的方法。
- en: JavaScript provides **async iterators** for this purpose. An async iterator
    is like a regular iterator, but its `next()` method returns a Promise that resolves
    to the next value. Async generator functions are marked with `async function*`
    and can use `await` inside them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript 提供了 **异步迭代器** 来实现这一目的。异步迭代器类似于常规迭代器，但其 `next()` 方法返回一个解析为下一个值的 Promise。异步生成器函数用
    `async function*` 标记，并且可以在其中使用 `await`。
- en: 'Here is an async pull stream that simulates asynchronous data production:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个模拟异步数据生产的异步拉取流：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The consumer uses `for await...of` to pull from an async iterator:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者使用 `for await...of` 从异步迭代器中提取数据：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `for await...of` loop automatically handles the Promises returned by the
    async iterator's `next()` method. Each iteration waits for the next Promise to
    resolve before proceeding. This makes asynchronous pull-based streaming feel as
    natural as synchronous iteration.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`for await...of` 循环自动处理异步迭代器 `next()` 方法返回的 Promises。每次迭代都会在继续之前等待下一个 Promise
    解析。这使得基于异步拉取的流感觉就像同步迭代一样自然。'
- en: Async iterators are a relatively recent addition to JavaScript (standardized
    in ES2018), but they are extremely powerful. They provide a clean, composable
    way to work with asynchronous sequences of data. Node.js streams support async
    iteration, as we will see.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 异步迭代器是 JavaScript 中相对较新的功能（在 ES2018 中标准化），但它们非常强大。它们提供了一种干净、可组合的方式来处理异步数据序列。Node.js
    流支持异步迭代，正如我们将看到的。
- en: Pull Model's Advantages and Limitations
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拉模型的优势和局限性
- en: The pull model has its own set of advantages. First and foremost, **backpressure
    is implicit**. Because the consumer explicitly pulls each chunk, the producer
    cannot overwhelm the consumer. The producer only produces data when requested.
    If the consumer is slow, it simply pulls less frequently, and the producer idles,
    waiting for the next pull. There is no need for complex signaling between producer
    and consumer to regulate flow. The pull mechanism itself provides the regulation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 拉模型有其自身的优势。首先，**背压是隐式的**。因为消费者明确地拉取每个数据块，生产者不能压倒消费者。生产者仅在请求时才产生数据。如果消费者速度慢，它只需减少拉取频率，生产者就会空闲，等待下一次拉取。无需在生产者和消费者之间进行复杂的信号传递来调节流量。拉取机制本身提供了调节。
- en: Second, the pull model maps naturally to lazy evaluation. The producer can avoid
    doing work until the consumer actually requests data. If the consumer only pulls
    the first few items from a potentially infinite sequence, the producer never generates
    the rest. This can be a significant efficiency win.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，拉模型自然映射到懒计算。生产者可以避免在消费者实际请求数据之前进行工作。如果消费者只从可能无限长的序列中拉取前几个项目，生产者就不会生成其余部分。这可以是一个显著的效率提升。
- en: Third, the pull model composes elegantly. You can chain multiple pull-based
    transformations together, and each stage will only pull from the previous stage
    when it needs data. The entire pipeline is driven by the final consumer's pull
    requests, propagating backward through the chain.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，拉模型具有优雅的合成性。你可以将多个基于拉取的转换链式连接起来，并且每个阶段只有在需要数据时才会从上一个阶段拉取。整个管道由最终消费者的拉取请求驱动，反向传播到链中。
- en: However, the pull model has limitations. It is less natural for event-driven
    systems. If data arrives unpredictably (for example, messages over a WebSocket
    connection), it does not fit cleanly into the pull model. You cannot pull data
    that has not yet arrived. The pull model works best when the producer can generate
    data on demand, not when the producer is itself reacting to external events.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拉模型也有局限性。对于事件驱动系统来说，它不太自然。如果数据到达不可预测（例如，通过 WebSocket 连接的消息），它无法干净地适应拉模型。你不能拉取尚未到达的数据。当生产者可以按需生成数据时，拉模型效果最佳，而不是当生产者本身对外部事件做出反应时。
- en: Additionally, the pull model does not naturally support fan-out. An iterator
    produces a sequence of values, and that sequence is consumed by pulling. Once
    a value is pulled, it is consumed. If you want multiple consumers to receive the
    same data, you need to implement a separate mechanism to broadcast or tee the
    stream.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，拉取模型本身不支持扇出。迭代器生成一系列值，并通过拉取来消费这些值。一旦值被拉取，它就被消费了。如果你想让多个消费者接收相同的数据，你需要实现一个单独的机制来广播或
    tee 流。
- en: Node.js Streams's Hybrid Approach
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Node.js 流的混合方法
- en: Node.js streams are neither purely push nor purely pull. They are a hybrid model
    that combines the advantages of both approaches while mitigating their limitations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js 的流既不是纯粹的推送模式也不是纯粹的拉取模式。它们是一种混合模型，结合了两种方法的优点，同时缓解了它们的局限性。
- en: At their core, Node.js streams are push-based. They extend `EventEmitter`, and
    data flows through them via `data` events. This makes them a natural fit for Node.js's
    event-driven architecture. However, Node.js streams implement backpressure explicitly.
    Consumers can signal to producers that they are not ready for more data, and producers
    must respect this signal. This backpressure mechanism adds pull-like control to
    the push-based architecture.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心上，Node.js 的流是基于推送的。它们扩展了 `EventEmitter`，数据通过 `data` 事件流经它们。这使得它们非常适合 Node.js
    的事件驱动架构。然而，Node.js 的流明确实现了背压。消费者可以向生产者发出信号，表明他们尚未准备好接收更多数据，而生产者必须尊重这一信号。这种背压机制为基于推送的架构添加了类似拉取的控制。
- en: 'Furthermore, Node.js streams support async iteration. You can consume a Readable
    stream using `for await...of`, treating it as a pull-based async iterator. Under
    the hood, the async iterator pulls from the stream''s internal buffer, and the
    stream manages the flow from the underlying data source. This allows you to use
    whichever consumption model best fits your use case: event-based (push) or iterator-based
    (pull).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Node.js 的流支持异步迭代。你可以使用 `for await...of` 来消费可读流，将其视为基于拉取的异步迭代器。在底层，异步迭代器从流的内部缓冲区拉取数据，而流管理从底层数据源的数据流。这允许你使用最适合你用例的消费模型：基于事件的（推送）或基于迭代器的（拉取）。
- en: This hybrid approach is not without complexity. Node.js streams have gone through
    several iterations in their design, and the API has evolved over time to add new
    features and address discovered issues. We will explore this history briefly,
    because understanding how streams evolved helps us understand why they work the
    way they do today.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种混合方法并非没有复杂性。Node.js 的流在设计上已经经历了多次迭代，API 随着时间的推移而演变，以添加新特性和解决发现的问题。我们将简要探讨这一历史，因为了解流是如何演变的有助于我们理解它们为什么今天会以这种方式工作。
- en: Let's go back... in time
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们回到...过去
- en: 'Streams have been part of Node.js since the very beginning. The initial implementation
    was simple: streams emitted `data` events, and consumers listened for those events.
    There was no concept of pausing or backpressure. If the consumer could not keep
    up, data would accumulate in memory.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 流一直是 Node.js 的一部分。最初的实现很简单：流发出 `data` 事件，消费者监听这些事件。没有暂停或背压的概念。如果消费者跟不上了，数据就会积累在内存中。
- en: 'Node.js version **0.10** introduced Streams2, a major redesign that added explicit
    support for backpressure. Readable streams gained two modes of operation: "paused"
    and "flowing." In paused mode, the consumer explicitly calls `read()` to pull
    data from the stream''s internal buffer. In flowing mode, the stream pushes data
    to the consumer via `data` events, but the consumer can pause the stream to signal
    backpressure. Writable streams gained a mechanism where the `write()` method returns
    a boolean indicating whether the internal buffer is full, signaling to the producer
    to stop writing until a `drain` event is emitted.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js **版本 0.10** 引入了 Streams2，这是一次重大的重新设计，增加了对背压的显式支持。可读流获得了两种操作模式：“暂停”和“流动”。在暂停模式下，消费者明确调用
    `read()` 从流的内部缓冲区拉取数据。在流动模式下，流通过 `data` 事件将数据推送到消费者，但消费者可以暂停流以发出背压信号。可写流获得了一种机制，其中
    `write()` 方法返回一个布尔值，指示内部缓冲区是否已满，向生产者发出停止写入的信号，直到发出 `drain` 事件。
- en: Node.js **v10.0** and beyond refined this model further, adding features like
    `stream.pipeline()` for robust error handling, `stream.finished()` for detecting
    stream completion, and async iterator support. These additions made streams more
    ergonomic and reliable for production use.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js **v10.0** 及以后的版本进一步精炼了这一模型，增加了诸如 `stream.pipeline()` 用于健壮的错误处理、`stream.finished()`
    用于检测流完成以及异步迭代器支持等特性。这些新增功能使得流在生产和使用上更加人性化且可靠。
- en: Today, Node.js streams are a mature, battle-tested abstraction. They are used
    throughout the Node.js ecosystem - by the `fs` module for file I/O, by the `http`
    module for request and response bodies, by `zlib` for compression, by `crypto`
    for encryption, and by countless third-party libraries.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Node.js 流是一个成熟、经过实战检验的抽象。它们被广泛应用于 Node.js 生态系统 - 由 `fs` 模块用于文件 I/O，由 `http`
    模块用于请求和响应体，由 `zlib` 用于压缩，由 `crypto` 用于加密，以及无数第三方库。
- en: The Four Stream Types
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 四种流类型
- en: Node.js defines four fundamental types of streams. Each type represents a different
    role in the data flow.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js 定义了四种基本类型的流。每种类型代表数据流中的不同角色。
- en: '**Readable streams** are sources of data. They produce data that can be consumed.
    Examples include `fs.createReadStream()` for reading files, `http.IncomingMessage`
    for HTTP request bodies on the server side or response bodies on the client side,
    and `process.stdin` for reading from standard input.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**可读流**是数据源。它们产生可以被消费的数据。例如，`fs.createReadStream()` 用于读取文件，`http.IncomingMessage`
    用于服务器端的 HTTP 请求体或客户端的响应体，以及 `process.stdin` 用于读取标准输入。'
- en: '**Writable streams** are sinks for data. They consume data that can be written
    to them. Examples include `fs.createWriteStream()` for writing files, `http.ServerResponse`
    for HTTP response bodies, and `process.stdout` for writing to standard output.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**可写流**是数据的目的地。它们消费可以被写入的数据。例如，`fs.createWriteStream()` 用于写入文件，`http.ServerResponse`
    用于 HTTP 响应体，以及 `process.stdout` 用于写入标准输出。'
- en: '**Transform streams** are both readable and writable. They consume data, transform
    it in some way, and produce new data. They sit in the middle of a pipeline, accepting
    input on their writable side and emitting output on their readable side. Examples
    include `zlib.createGzip()` for compression and `crypto.createCipheriv()` for
    encryption. Transform streams are subclasses of Duplex streams with a simplified
    interface for the common case where the readable output is directly derived from
    the writable input.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换流**也是可读和可写的。它们消费数据，以某种方式转换它，并产生新的数据。它们位于管道的中间，在其可写端接受输入，在其可读端发出输出。例如，`zlib.createGzip()`
    用于压缩和 `crypto.createCipheriv()` 用于加密。转换流是双工流的子类，具有简化的接口，适用于可读输出直接从可写输入派生的常见情况。'
- en: '**Duplex streams** are also both readable and writable, but unlike Transform
    streams, their readable and writable sides are independent. They represent two-way
    communication channels. The most common example is `net.Socket`, which represents
    a TCP connection. Data written to a socket is sent over the network, and data
    received from the network can be read from the socket. The two directions of data
    flow are separate; writing to the socket does not directly affect what can be
    read from it.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**双工流**也是可读和可写的，但与转换流不同，它们的可读和可写部分是独立的。它们代表双向通信通道。最常见的一个例子是 `net.Socket`，它代表一个
    TCP 连接。写入套接字的数据通过网络发送，从网络接收到的数据可以从套接字中读取。数据流的两个方向是分开的；写入套接字不会直接影响到可以从它读取的内容。'
- en: These four types form the vocabulary of streaming in Node.js. By combining them,
    you can construct complex data processing pipelines. A Readable stream can be
    piped to a Transform stream, which can be piped to another Transform stream, which
    can be piped to a Writable stream. Each stage processes data incrementally, in
    chunks, with backpressure propagating backward through the pipeline to ensure
    memory usage remains bounded.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这四种类型构成了 Node.js 中流的语言。通过组合它们，你可以构建复杂的数据处理管道。一个可读流可以被连接到一个转换流，这个转换流可以连接到另一个转换流，然后连接到一个可写流。每个阶段增量地、分块地处理数据，通过反向传播的背压来确保内存使用保持在有限范围内。
- en: Conceptualizing Data Flow
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据流的概念化
- en: 'Let us visualize how data flows through a stream pipeline. Imagine a simple
    pipeline with three stages:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化数据如何通过流管道流动。想象一个有三个阶段的简单管道：
- en: A Readable stream (the source) reads chunks from a file.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个可读流（源）从文件中读取数据块。
- en: A Transform stream (the processor) converts each chunk to uppercase.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个转换流（处理器）将每个数据块转换为大写。
- en: A Writable stream (the sink) writes each chunk to another file.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个可写流（目的地）将每个数据块写入另一个文件。
- en: 'Data flows forward through the pipeline: from the Readable stream to the Transform
    stream to the Writable stream. Each chunk moves through the stages in sequence.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通过管道向前流动：从可读流到转换流再到可写流。每个数据块按顺序通过各个阶段。
- en: 'But control signals flow backward. If the Writable stream''s internal buffer
    fills up (perhaps because disk writes are slow), it signals backpressure. The
    Transform stream, seeing that the Writable stream cannot accept more data, pauses
    its own consumption from the Readable stream. The Readable stream, seeing that
    no one is pulling data from it, stops reading from the file. The entire pipeline
    pauses until the Writable stream''s buffer drains and it emits a signal that it
    is ready for more data. At that point, the pipeline resumes: the Readable stream
    reads more data, the Transform stream processes it, and the Writable stream writes
    it.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 但控制信号会反向流动。如果可写流的内部缓冲区满了（可能是因为磁盘写入速度慢），它会发出反压信号。转换流看到可写流不能接受更多数据，会暂停从可读流中读取。可读流看到没有人从它那里拉取数据，就会停止从文件中读取。整个管道会暂停，直到可写流的缓冲区排空并发出它准备好接收更多数据的信号。在那个时刻，管道会继续：可读流读取更多数据，转换流处理它，可写流写入它。
- en: This bidirectional flow - data forward, backpressure backward - is the key to
    bounded memory usage in stream pipelines. Without backpressure, the fast stages
    would produce data faster than the slow stages could consume it, and buffers would
    grow without limit. With backpressure, the pipeline self-regulates, ensuring that
    every stage operates at the speed of the slowest stage.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种双向流动——数据向前，反压向后——是流管道中有限内存使用的关键。没有反压，快速阶段会产生比慢速阶段能消费的数据更快的数据，缓冲区会无限制地增长。有了反压，管道会自我调节，确保每个阶段都以最慢阶段的速度运行。
- en: Bidirectional Pipeline Flow
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双向管道流
- en: Data flows forward ➜ Backpressure signals flow backward ⬅
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据向前流动 ➜ 反压信号向后流动 ⬅
- en: STAGE 1
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段 1
- en: Readable Stream
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可读流
- en: fs.createReadStream()ACTIVE - Reading dataSTAGE 2
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`fs.createReadStream()` - 活跃 - 读取数据 - 阶段 2'
- en: Transform Stream
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转换流
- en: zlib.createGzip()STAGE 3
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`zlib.createGzip()` - 阶段 3'
- en: Writable Stream
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可写流
- en: fs.createWriteStream()FORWARD DATA FLOW
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`fs.createWriteStream()` - 前向数据流'
- en: Chunks move from source → transform → destination
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 数据块从源 → 转换 → 目的地移动
- en: BACKPRESSURE SIGNAL
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 反压信号
- en: Pause signal travels backward when buffer is full
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当缓冲区满时，暂停信号会反向传播
- en: When to Use Streams
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用流
- en: Streams are not always the right tool. For small amounts of data that easily
    fit in memory, reading the entire dataset into a Buffer or string is simpler and
    often faster. Streams add overhead - the event loop must schedule callbacks, data
    must be chunked, and backpressure must be managed. If you are processing a 10KB
    JSON file, streams are overkill. Just use `fs.readFile()` or `fs.readFileSync()`,
    parse the JSON, and be done.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 流并不总是正确的工具。对于小量的数据，这些数据容易放入内存，将整个数据集读入一个Buffer或字符串会更简单，通常也更快。流增加了开销 - 事件循环必须安排回调，数据必须分块，并且必须管理反压。如果你正在处理一个10KB大小的JSON文件，流就过度了。只需使用`fs.readFile()`或`fs.readFileSync()`，解析JSON，然后完成。
- en: Streams shine when working with large datasets or unbounded data. If you are
    processing a multi-gigabyte log file, streams are essential. If you are handling
    an incoming HTTP request body of unknown size, streams are the correct abstraction.
    If you are implementing a network protocol where messages arrive continuously,
    streams provide the structure you need.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大型数据集或无界数据时，流特别有用。如果你正在处理一个多吉字节大小的日志文件，流是必不可少的。如果你正在处理一个大小未知的传入HTTP请求体，流是正确的抽象。如果你正在实现一个消息连续到达的网络协议，流提供了你需要的结构。
- en: Streams are also valuable when you want to start processing data before all
    of it is available. Consider an HTTP server responding to a file download request.
    Without streams, the server would have to read the entire file into memory before
    starting to send the response. With streams, the server can start sending the
    first chunks of the file as soon as they are read from disk, significantly reducing
    the time to first byte for the client.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当你希望在所有数据都可用之前就开始处理数据时，流也是非常有价值的。考虑一个HTTP服务器响应文件下载请求的情况。如果没有流，服务器必须在开始发送响应之前将整个文件读入内存。有了流，服务器可以在从磁盘读取文件的第一部分后立即开始发送文件的第一部分，这显著减少了客户端的首次字节时间。
- en: Finally, streams are useful for composing pipelines of transformations. If you
    need to read a file, decompress it, parse it, transform the parsed data, and write
    the result to another file, streams allow you to express this as a clean, linear
    pipeline where each stage is a separate, focused transformation. This composability
    is a major advantage of the stream abstraction.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，流对于组合转换的管道非常有用。如果你需要读取一个文件，解压缩它，解析它，转换解析后的数据，并将结果写入另一个文件，流允许你将这个过程表达为一个干净、线性的管道，其中每个阶段都是一个独立的、专注的转换。这种可组合性是流抽象的主要优势。
- en: Common Use Cases
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**常见用例**'
- en: Several patterns appear repeatedly when working with streams in Node.js.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Node.js流工作时，会出现一些重复出现的模式。
- en: '**File I/O** is the most common use case. Reading and writing large files should
    almost always be done with streams. This avoids loading the entire file into memory
    and allows processing to begin immediately.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**文件输入/输出**是最常见的用例。读取和写入大文件几乎总是应该使用流来完成。这样可以避免将整个文件加载到内存中，并允许立即开始处理。'
- en: '**Network communication** is inherently streaming. HTTP request and response
    bodies are streams. TCP sockets are duplex streams. When you send data over a
    network, you do not have all the data up front; it is generated or received incrementally.
    Streams are the natural abstraction for network protocols.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络通信**本质上是流式的。HTTP请求和响应体是流。TCP套接字是双向流。当你通过网络发送数据时，你不会一开始就拥有所有数据；它是逐步生成或接收的。流是网络协议的自然抽象。'
- en: '**Data transformation pipelines** are a perfect fit for streams. Any time you
    have a series of transformations to apply to data - parsing, filtering, mapping,
    aggregating - streams allow you to express each transformation as a separate,
    composable stage. This is common in ETL (Extract, Transform, Load) workflows,
    log processing, and data analytics.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据转换管道**非常适合流。任何需要应用一系列转换到数据上的情况——解析、过滤、映射、聚合——流允许你将每个转换表达为一个独立的、可组合的阶段。这在ETL（提取、转换、加载）工作流程、日志处理和数据分析中很常见。'
- en: '**Real-time data processing** often uses streams. If you are processing events
    from a message queue, sensor data from IoT devices, or user interactions in a
    web application, streams provide a way to handle each event as it arrives without
    accumulating events in memory.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**实时数据处理**通常使用流。如果你正在处理来自消息队列的事件、物联网设备的传感器数据，或者在Web应用中的用户交互，流提供了一种处理每个到达的事件的方法，而不会在内存中累积事件。'
- en: '**Proxying and multiplexing** leverage the duplex nature of sockets. When building
    a proxy server or a load balancer, you pipe data between sockets, forwarding requests
    and responses without buffering the entire message. This allows the proxy to handle
    very large requests and responses efficiently.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**代理和复用**利用套接字的复用特性。当构建代理服务器或负载均衡器时，你可以在套接字之间传输数据，转发请求和响应而不需要缓冲整个消息。这使得代理能够高效地处理非常大的请求和响应。'
- en: Setting the Stage
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**设置场景**'
- en: 'We have now established the conceptual foundation for streams. We understand
    the problem they solve: processing large or unbounded data without exhausting
    memory. We understand the two fundamental models: push and pull, and we have seen
    how Node.js streams combine both. We understand the four stream types and the
    roles they play in data flow.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经建立了流的原理基础。我们理解了它们解决的问题：在不耗尽内存的情况下处理大量或无界数据。我们理解了两种基本模型：推送和拉取，并看到了Node.js流如何结合两者。我们理解了四种流类型以及它们在数据流中的作用。
- en: What we have not yet done is implement or use real Node.js streams. We have
    built simple examples to illustrate concepts, but we have not explored the actual
    `stream.Readable`, `stream.Writable`, `stream.Transform`, and `stream.Duplex`
    classes. We have not examined how to implement custom streams, how to configure
    their behavior, or how to construct robust pipelines with error handling.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有做的是实现或使用真正的Node.js流。我们构建了一些简单的示例来阐述概念，但我们还没有探索实际的`stream.Readable`、`stream.Writable`、`stream.Transform`和`stream.Duplex`类。我们还没有研究如何实现自定义流，如何配置它们的行为，或者如何构建具有错误处理的健壮管道。
- en: 'That is the work of the next chapters, where we will dive deep into Readable
    streams: how they are implemented, how they manage internal buffers, how their
    two modes of operation work, and how to create custom Readable streams from various
    data sources.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 那是下一章的工作内容，我们将深入探讨可读流：它们是如何实现的，它们如何管理内部缓冲区，它们两种操作模式是如何工作的，以及如何从各种数据源创建自定义的可读流。
- en: But all of that rests on the foundation we have built here. Streams are not
    magic. They are a systematic response to the constraints of memory and the realities
    of asynchronous I/O. They implement well-established patterns - Observer, Iterator
    - adapted to the specific needs of Node.js's event-driven architecture. By understanding
    streams from first principles, you will be able to reason about their behavior,
    debug problems when they arise, and design your own streaming systems with confidence.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 但所有这些都建立在我们在这里建立的基础之上。流不是魔法。它们是对内存限制和异步I/O现实的一种系统性的响应。它们实现了经过验证的模式 - 观察者、迭代器
    - 这些模式被调整以适应Node.js事件驱动架构的具体需求。通过从第一原理理解流，你将能够对其行为进行推理，在出现问题时进行调试，并自信地设计你自己的流系统。
- en: It is going to get interesting from now onwards!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始将会变得有趣！
