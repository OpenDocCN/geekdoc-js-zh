- en: Zero-Copy, Scatter/Gather I/O & Advanced Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é›¶æ‹·è´ã€åˆ†æ•£/æ”¶é›†I/Oä¸é«˜çº§æŠ€æœ¯
- en: åŸæ–‡ï¼š[https://www.thenodebook.com/streams/zero-copy-scatter-gather](https://www.thenodebook.com/streams/zero-copy-scatter-gather)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://www.thenodebook.com/streams/zero-copy-scatter-gather](https://www.thenodebook.com/streams/zero-copy-scatter-gather)
- en: ğŸš¨Caution
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš¨æ³¨æ„
- en: This chapter covers advanced performance optimization techniques. If you feel
    uncomfortable while reading, feel free to skip this and come back later. The techniques
    here matter most when you're processing large volumes of data and have already
    identified I/O as your bottleneck through profiling.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« æ¶µç›–äº†é«˜çº§æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ã€‚å¦‚æœæ‚¨åœ¨é˜…è¯»æ—¶æ„Ÿåˆ°ä¸èˆ’æœï¼Œè¯·éšæ—¶è·³è¿‡æ­¤éƒ¨åˆ†ï¼Œç¨åå†å›æ¥ã€‚è¿™é‡Œçš„æŠ€æœ¯åœ¨æ‚¨å¤„ç†å¤§é‡æ•°æ®å¹¶ä¸”å·²ç»é€šè¿‡åˆ†æç¡®å®šI/Oæ˜¯ç“¶é¢ˆæ—¶æœ€ä¸ºé‡è¦ã€‚
- en: Every time you copy a file in Node.js, you're likely copying the same data four
    times. First from disk into kernel memory. Then from kernel memory into your Node.js
    process memory. Then from your process memory back into kernel memory for the
    destination. Finally from kernel memory to the destination disk. Four copies for
    what should conceptually be one operation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡æ‚¨åœ¨Node.jsä¸­å¤åˆ¶æ–‡ä»¶æ—¶ï¼Œæ‚¨å¾ˆå¯èƒ½ä¼šå¤åˆ¶ç›¸åŒçš„æ•°æ®å››æ¬¡ã€‚é¦–å…ˆä»ç£ç›˜åˆ°å†…æ ¸å†…å­˜ã€‚ç„¶åä»å†…æ ¸å†…å­˜åˆ°æ‚¨çš„Node.jsè¿›ç¨‹å†…å­˜ã€‚ç„¶åä»æ‚¨çš„è¿›ç¨‹å†…å­˜å›åˆ°å†…æ ¸å†…å­˜ï¼Œä¸ºç›®çš„åœ°ã€‚æœ€åä»å†…æ ¸å†…å­˜åˆ°ç›®æ ‡ç£ç›˜ã€‚å¯¹äºæœ¬åº”æ¦‚å¿µä¸Šæ˜¯ä¸€ä¸ªæ“ä½œçš„äº‹æƒ…ï¼Œéœ€è¦å››æ¬¡å¤åˆ¶ã€‚
- en: This matters because copying is expensive. Every copy involves CPU time, memory
    bandwidth, and cache pollution. When you're moving gigabytes of data through a
    stream pipeline, unnecessary copies become the bottleneck. Your disk might be
    capable of 2GB/sec, but you're getting 500MB/sec because you're spending CPU cycles
    shuffling bytes around in memory instead of actually doing useful work.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå¤åˆ¶æ˜¯æ˜‚è´µçš„ã€‚æ¯æ¬¡å¤åˆ¶éƒ½æ¶‰åŠCPUæ—¶é—´ã€å†…å­˜å¸¦å®½å’Œç¼“å­˜æ±¡æŸ“ã€‚å½“æ‚¨é€šè¿‡æµç®¡é“ç§»åŠ¨æ•°GBçš„æ•°æ®æ—¶ï¼Œä¸å¿…è¦çš„å¤åˆ¶æˆä¸ºç“¶é¢ˆã€‚æ‚¨çš„ç£ç›˜å¯èƒ½èƒ½å¤Ÿè¾¾åˆ°2GB/secçš„é€Ÿåº¦ï¼Œä½†æ‚¨åªèƒ½è·å¾—500MB/secï¼Œå› ä¸ºæ‚¨æ­£åœ¨èŠ±è´¹CPUå‘¨æœŸåœ¨å†…å­˜ä¸­ç§»åŠ¨å­—èŠ‚ï¼Œè€Œä¸æ˜¯çœŸæ­£åšæœ‰ç”¨çš„å·¥ä½œã€‚
- en: The performance techniques we're covering in this chapter - zero-copy, scatter/gather
    I/O, buffer pooling - aren't academic curiosities. They're the difference between
    a stream pipeline that maxes out your hardware and one that wastes resources on
    bookkeeping. We're going to look at how data actually moves through a system,
    where the copies happen, and how to eliminate them. Then we'll explore techniques
    for batching I/O operations to reduce syscall overhead, and strategies for managing
    buffers to minimize garbage collection pressure.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä¸­æ¶µç›–çš„æ€§èƒ½æŠ€æœ¯â€”â€”é›¶æ‹·è´ã€åˆ†æ•£/æ”¶é›†I/Oã€ç¼“å†²åŒºæ± åŒ–â€”â€”ä¸æ˜¯å­¦æœ¯ä¸Šçš„å¥½å¥‡å¿ƒã€‚å®ƒä»¬æ˜¯æµç®¡é“è¾¾åˆ°ç¡¬ä»¶æé™ä¸æµªè´¹èµ„æºåœ¨ç°¿è®°ä¸Šçš„åŒºåˆ«ã€‚æˆ‘ä»¬å°†æ¢è®¨æ•°æ®å®é™…ä¸Šæ˜¯å¦‚ä½•é€šè¿‡ç³»ç»Ÿç§»åŠ¨çš„ï¼Œå¤åˆ¶å‘ç”Ÿåœ¨å“ªé‡Œï¼Œä»¥åŠå¦‚ä½•æ¶ˆé™¤å®ƒä»¬ã€‚ç„¶åæˆ‘ä»¬å°†æ¢è®¨æ‰¹é‡I/Oæ“ä½œçš„æŠ€æœ¯ä»¥å‡å°‘ç³»ç»Ÿè°ƒç”¨å¼€é”€ï¼Œä»¥åŠç®¡ç†ç¼“å†²åŒºä»¥æœ€å°åŒ–åƒåœ¾æ”¶é›†å‹åŠ›çš„ç­–ç•¥ã€‚
- en: By the end, you'll understand how to make streams faster, why they're slow in
    the first place, and which optimizations actually matter for your workload.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨å°†äº†è§£å¦‚ä½•ä½¿æµæ›´å¿«ï¼Œä¸ºä»€ä¹ˆå®ƒä»¬ä¸€å¼€å§‹å°±æ…¢ï¼Œä»¥åŠå“ªäº›ä¼˜åŒ–å¯¹æ‚¨çš„è´Ÿè½½çœŸæ­£é‡è¦ã€‚
- en: What is Zero-Copy?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯é›¶æ‹·è´ï¼Ÿ
- en: The term "zero-copy" gets thrown around loosely, and it's not as magical as
    it sounds. We need to be precise about what it actually means.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: â€œé›¶æ‹·è´â€è¿™ä¸ªæœ¯è¯­è¢«éšæ„ä½¿ç”¨ï¼Œå®ƒå¹¶ä¸åƒå¬èµ·æ¥é‚£ä¹ˆç¥å¥‡ã€‚æˆ‘ä»¬éœ€è¦ç²¾ç¡®åœ°äº†è§£å®ƒå®é™…ä¸Šæ„å‘³ç€ä»€ä¹ˆã€‚
- en: Traditional I/O involves copying data between different regions of memory. Your
    operating system maintains a strict separation between kernel space (where the
    OS kernel runs) and user space (where your application runs). This separation
    matters for system stability - it prevents applications from corrupting kernel
    memory or interfering with each other.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿçš„I/Oæ¶‰åŠåœ¨ä¸åŒå†…å­˜åŒºåŸŸä¹‹é—´å¤åˆ¶æ•°æ®ã€‚æ‚¨çš„æ“ä½œç³»ç»Ÿåœ¨å†…æ ¸ç©ºé—´ï¼ˆæ“ä½œç³»ç»Ÿå†…æ ¸è¿è¡Œçš„åœ°æ–¹ï¼‰å’Œç”¨æˆ·ç©ºé—´ï¼ˆæ‚¨çš„åº”ç”¨ç¨‹åºè¿è¡Œçš„åœ°æ–¹ï¼‰ä¹‹é—´ä¿æŒä¸¥æ ¼çš„åˆ†ç¦»ã€‚è¿™ç§åˆ†ç¦»å¯¹ç³»ç»Ÿç¨³å®šæ€§å¾ˆé‡è¦â€”â€”å®ƒé˜²æ­¢åº”ç”¨ç¨‹åºæŸåå†…æ ¸å†…å­˜æˆ–ç›¸äº’å¹²æ‰°ã€‚
- en: 'When you read a file in Node.js, here''s what typically happens. The operating
    system reads data from disk into a kernel buffer. This is the first copy - from
    disk to kernel memory. Then, because your Node.js process can''t directly access
    kernel memory, the OS copies that data from the kernel buffer into your process''s
    memory space. That''s the second copy. When you write that data to another file,
    the process reverses: your process writes to a buffer in user space, the OS copies
    it into a kernel buffer (third copy), and then writes it to disk (fourth copy).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ åœ¨Node.jsä¸­è¯»å–æ–‡ä»¶æ—¶ï¼Œé€šå¸¸ä¼šå‘ç”Ÿä»¥ä¸‹æƒ…å†µã€‚æ“ä½œç³»ç»Ÿä»ç£ç›˜è¯»å–æ•°æ®åˆ°å†…æ ¸ç¼“å†²åŒºã€‚è¿™æ˜¯ç¬¬ä¸€æ¬¡å¤åˆ¶â€”â€”ä»ç£ç›˜åˆ°å†…æ ¸å†…å­˜ã€‚ç„¶åï¼Œå› ä¸ºä½ çš„Node.jsè¿›ç¨‹ä¸èƒ½ç›´æ¥è®¿é—®å†…æ ¸å†…å­˜ï¼Œæ“ä½œç³»ç»Ÿå°†é‚£äº›æ•°æ®ä»å†…æ ¸ç¼“å†²åŒºå¤åˆ¶åˆ°ä½ çš„è¿›ç¨‹å†…å­˜ç©ºé—´ã€‚è¿™æ˜¯ç¬¬äºŒæ¬¡å¤åˆ¶ã€‚å½“ä½ å°†é‚£äº›æ•°æ®å†™å…¥å¦ä¸€ä¸ªæ–‡ä»¶æ—¶ï¼Œè¿‡ç¨‹ç›¸åï¼šä½ çš„è¿›ç¨‹å°†æ•°æ®å†™å…¥ç”¨æˆ·ç©ºé—´çš„ä¸€ä¸ªç¼“å†²åŒºï¼Œæ“ä½œç³»ç»Ÿå°†å…¶å¤åˆ¶åˆ°å†…æ ¸ç¼“å†²åŒºï¼ˆç¬¬ä¸‰æ¬¡å¤åˆ¶ï¼‰ï¼Œç„¶åå†™å…¥ç£ç›˜ï¼ˆç¬¬å››æ¬¡å¤åˆ¶ï¼‰ã€‚
- en: Four copies. For a simple file copy operation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å››æ¬¡å¤åˆ¶ã€‚å¯¹äºä¸€ä¸ªç®€å•çš„æ–‡ä»¶å¤åˆ¶æ“ä½œã€‚
- en: Each copy operation involves several expensive steps. The CPU must execute instructions
    to read from one memory location and write to another. This consumes CPU cycles
    that could be spent on actual computation. It also pollutes the CPU cache - when
    you copy megabytes of data through the cache, you evict useful data that other
    parts of your program need, causing cache misses later.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡å¤åˆ¶æ“ä½œéƒ½æ¶‰åŠå‡ ä¸ªæ˜‚è´µçš„æ­¥éª¤ã€‚CPUå¿…é¡»æ‰§è¡ŒæŒ‡ä»¤ä»å†…å­˜çš„ä¸€ä¸ªä½ç½®è¯»å–å¹¶å†™å…¥å¦ä¸€ä¸ªä½ç½®ã€‚è¿™æ¶ˆè€—äº†æœ¬å¯ä»¥ç”¨äºå®é™…è®¡ç®—çš„CPUå‘¨æœŸã€‚å®ƒè¿˜æ±¡æŸ“äº†CPUç¼“å­˜â€”â€”å½“ä½ é€šè¿‡ç¼“å­˜å¤åˆ¶å…†å­—èŠ‚çš„æ•°æ®æ—¶ï¼Œä½ ä¼šé©±é€ç¨‹åºå…¶ä»–éƒ¨åˆ†éœ€è¦çš„æœ‰æ•ˆæ•°æ®ï¼Œå¯¼è‡´åç»­çš„ç¼“å­˜æœªå‘½ä¸­ã€‚
- en: Memory bandwidth is also finite. Modern systems have very fast memory, but there's
    still a limit to how many bytes per second can move between CPU and RAM. When
    you're copying the same data multiple times, you're consuming that bandwidth repeatedly
    for the same bytes. This becomes the bottleneck when moving large amounts of data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜å¸¦å®½ä¹Ÿæ˜¯æœ‰é™çš„ã€‚ç°ä»£ç³»ç»Ÿæœ‰éå¸¸å¿«çš„å†…å­˜ï¼Œä½†CPUå’ŒRAMä¹‹é—´æ¯ç§’å¯ä»¥ç§»åŠ¨çš„å­—èŠ‚æ•°ä»ç„¶æœ‰é™ã€‚å½“ä½ å¤šæ¬¡å¤åˆ¶ç›¸åŒçš„æ•°æ®æ—¶ï¼Œä½ ä¼šåœ¨ç›¸åŒçš„å­—èŠ‚ä¸Šé‡å¤æ¶ˆè€—è¯¥å¸¦å®½ã€‚å½“ç§»åŠ¨å¤§é‡æ•°æ®æ—¶ï¼Œè¿™æˆä¸ºç“¶é¢ˆã€‚
- en: 'Consider a web server serving a 1GB video file. With traditional I/O, that
    1GB gets copied four times, but each CPU-mediated copy involves both a read and
    a write on the memory bus. The actual bandwidth breakdown:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªæœåŠ¡å™¨æ­£åœ¨æä¾›1GBçš„è§†é¢‘æ–‡ä»¶ã€‚ä½¿ç”¨ä¼ ç»Ÿçš„I/Oï¼Œè¿™1GBçš„æ•°æ®éœ€è¦å¤åˆ¶å››æ¬¡ï¼Œä½†æ¯æ¬¡é€šè¿‡CPUä»‹å¯¼çš„å¤åˆ¶éƒ½æ¶‰åŠå†…å­˜æ€»çº¿ä¸Šçš„è¯»å†™æ“ä½œã€‚å®é™…çš„å¸¦å®½åˆ†é…å¦‚ä¸‹ï¼š
- en: 'Disk to kernel buffer: DMA write (1GB)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç£ç›˜åˆ°å†…æ ¸ç¼“å†²åŒºï¼šDMAå†™å…¥ï¼ˆ1GBï¼‰
- en: 'Kernel to user space: CPU read + write (2GB)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†…æ ¸åˆ°ç”¨æˆ·ç©ºé—´ï¼šCPUè¯»å– + å†™å…¥ï¼ˆ2GBï¼‰
- en: 'User space to kernel: CPU read + write (2GB)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨æˆ·ç©ºé—´åˆ°å†…æ ¸ï¼šCPUè¯»å– + å†™å…¥ï¼ˆ2GBï¼‰
- en: 'Kernel to network: DMA read (1GB)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†…æ ¸åˆ°ç½‘ç»œï¼šDMAè¯»å–ï¼ˆ1GBï¼‰
- en: That's 6GB of memory bandwidth consumed to serve 1GB of actual data. On a system
    capable of 50GB/sec memory bandwidth, that single file transfer consumes 120ms
    just in memory operations, before even accounting for disk or network I/O latency.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¶ˆè€—äº†6GBçš„å†…å­˜å¸¦å®½æ¥æœåŠ¡1GBçš„å®é™…æ•°æ®ã€‚åœ¨ä¸€ä¸ªèƒ½å¤Ÿè¾¾åˆ°50GB/secå†…å­˜å¸¦å®½çš„ç³»ç»Ÿä¸Šï¼Œè¿™ä¸ªå•ç‹¬çš„æ–‡ä»¶ä¼ è¾“ä»…å†…å­˜æ“ä½œå°±æ¶ˆè€—äº†120msï¼Œè¿˜ä¸åŒ…æ‹¬ç£ç›˜æˆ–ç½‘ç»œI/Oå»¶è¿Ÿã€‚
- en: Zero-copy is the technique of eliminating some or all of these intermediate
    copies. The "zero" is aspirational - you can't truly have zero copies because
    data has to move from one place to another - but you can avoid copies between
    kernel and user space, which is where most of the CPU overhead occurs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: é›¶å¤åˆ¶æ˜¯ä¸€ç§æ¶ˆé™¤éƒ¨åˆ†æˆ–å…¨éƒ¨è¿™äº›ä¸­é—´å¤åˆ¶çš„æŠ€æœ¯ã€‚â€œé›¶â€æ˜¯ç†æƒ³åŒ–çš„â€”â€”ä½ æ— æ³•çœŸæ­£å®ç°é›¶å¤åˆ¶ï¼Œå› ä¸ºæ•°æ®å¿…é¡»ä»ä¸€ä¸ªåœ°æ–¹ç§»åŠ¨åˆ°å¦ä¸€ä¸ªåœ°æ–¹â€”â€”ä½†ä½ å¯ä»¥åœ¨å†…æ ¸å’Œç”¨æˆ·ç©ºé—´ä¹‹é—´é¿å…å¤åˆ¶ï¼Œè¿™æ˜¯å¤§å¤šæ•°CPUå¼€é”€å‘ç”Ÿçš„åœ°æ–¹ã€‚
- en: If you're just moving data from one file to another without examining or modifying
    it, why copy it into your process's memory at all? The kernel already has the
    data. It could just move it directly from the source file's kernel buffer to the
    destination file's kernel buffer, bypassing your process entirely.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åªæ˜¯å°†æ•°æ®ä»ä¸€ä¸ªæ–‡ä»¶ç§»åŠ¨åˆ°å¦ä¸€ä¸ªæ–‡ä»¶ï¼Œè€Œä¸æ£€æŸ¥æˆ–ä¿®æ”¹å®ƒï¼Œä¸ºä»€ä¹ˆè¿˜è¦å°†å…¶å¤åˆ¶åˆ°è¿›ç¨‹çš„å†…å­˜ä¸­å‘¢ï¼Ÿå†…æ ¸å·²ç»æœ‰äº†è¿™äº›æ•°æ®ã€‚å®ƒå¯ä»¥ç›´æ¥ä»æºæ–‡ä»¶çš„å†…æ ¸ç¼“å†²åŒºç§»åŠ¨åˆ°ç›®æ ‡æ–‡ä»¶çš„å†…æ ¸ç¼“å†²åŒºï¼Œå®Œå…¨ç»•è¿‡ä½ çš„è¿›ç¨‹ã€‚
- en: The `sendfile()` syscall on Linux does exactly this. You tell the kernel "copy
    data from file descriptor A to file descriptor B," and it does so entirely within
    kernel space. No copying to user space. No CPU cycles spent in your process moving
    bytes around. The data flows from source to destination with the fewest possible
    copies.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Linuxä¸Šçš„`sendfile()`ç³»ç»Ÿè°ƒç”¨æ­£æ˜¯å¦‚æ­¤ã€‚ä½ å‘Šè¯‰å†…æ ¸â€œä»æ–‡ä»¶æè¿°ç¬¦Aå¤åˆ¶æ•°æ®åˆ°æ–‡ä»¶æè¿°ç¬¦Bâ€ï¼Œå®ƒå®Œå…¨åœ¨å†…æ ¸ç©ºé—´å†…å®Œæˆè¿™ä¸€æ“ä½œã€‚æ— éœ€å¤åˆ¶åˆ°ç”¨æˆ·ç©ºé—´ã€‚æ— éœ€åœ¨è¿›ç¨‹ä¸­è¿›è¡Œå­—èŠ‚ç§»åŠ¨çš„CPUå‘¨æœŸã€‚æ•°æ®ä»¥å°½å¯èƒ½å°‘çš„å¤åˆ¶æ¬¡æ•°ä»æºæµå‘ç›®çš„åœ°ã€‚
- en: The implementation varies by operating system. Linux has `sendfile()` and `splice()`.
    FreeBSD and macOS have `sendfile()` with slightly different semantics. Windows
    has `TransmitFile()`. These are OS-level primitives that applications can use
    to achieve zero-copy transfers in specific scenarios.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°æ–¹å¼å› æ“ä½œç³»ç»Ÿè€Œå¼‚ã€‚Linuxæœ‰`sendfile()`å’Œ`splice()`ã€‚FreeBSDå’ŒmacOSæœ‰ç•¥æœ‰ä¸åŒçš„è¯­ä¹‰çš„`sendfile()`ã€‚Windowsæœ‰`TransmitFile()`ã€‚è¿™äº›éƒ½æ˜¯æ“ä½œç³»ç»Ÿçº§åˆ«çš„åŸè¯­ï¼Œåº”ç”¨ç¨‹åºå¯ä»¥ä½¿ç”¨å®ƒä»¬åœ¨ç‰¹å®šåœºæ™¯ä¸­å®ç°é›¶æ‹·è´ä¼ è¾“ã€‚
- en: On modern systems with DMA (Direct Memory Access), it's even better. DMA controllers
    can transfer data between devices and memory without involving the CPU at all.
    The disk controller reads data from disk and writes it directly to memory. The
    network card reads data from memory and sends it over the wire. The CPU just sets
    up the transfer and then does other work while the DMA controller handles the
    actual data movement.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…·æœ‰DMAï¼ˆç›´æ¥å†…å­˜è®¿é—®ï¼‰çš„ç°ä»£ç³»ç»Ÿä¸­ï¼Œè¿™ç”šè‡³æ›´å¥½ã€‚DMAæ§åˆ¶å™¨å¯ä»¥åœ¨ä¸æ¶‰åŠCPUçš„æƒ…å†µä¸‹åœ¨è®¾å¤‡å’Œå†…å­˜ä¹‹é—´ä¼ è¾“æ•°æ®ã€‚ç£ç›˜æ§åˆ¶å™¨ä»ç£ç›˜è¯»å–æ•°æ®å¹¶å°†å…¶ç›´æ¥å†™å…¥å†…å­˜ã€‚ç½‘ç»œå¡ä»å†…å­˜è¯»å–æ•°æ®å¹¶é€šè¿‡ç”µçº¿å‘é€ã€‚CPUåªéœ€è®¾ç½®ä¼ è¾“ï¼Œç„¶ååœ¨DMAæ§åˆ¶å™¨å¤„ç†å®é™…æ•°æ®ç§»åŠ¨çš„åŒæ—¶åšå…¶ä»–å·¥ä½œã€‚
- en: When zero-copy works perfectly, the CPU's job is reduced to setting up transfers.
    The data moves via DMA and kernel-to-kernel copies, never touching user space,
    never consuming CPU cycles for memcpy operations. The CPU initiates the transfer
    with a syscall, the kernel programs the DMA controllers, and the data flows directly
    from disk to network (or file to file) without ever entering the CPU cache.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é›¶æ‹·è´å®Œç¾å·¥ä½œæ—¶ï¼ŒCPUçš„å·¥ä½œå‡å°‘åˆ°è®¾ç½®ä¼ è¾“ã€‚æ•°æ®é€šè¿‡DMAå’Œå†…æ ¸åˆ°å†…æ ¸çš„å¤åˆ¶ç§»åŠ¨ï¼Œä»æœªè§¦åŠç”¨æˆ·ç©ºé—´ï¼Œä»æœªæ¶ˆè€—CPUå‘¨æœŸè¿›è¡Œmemcpyæ“ä½œã€‚CPUé€šè¿‡ç³»ç»Ÿè°ƒç”¨å¯åŠ¨ä¼ è¾“ï¼Œå†…æ ¸ç¼–ç¨‹DMAæ§åˆ¶å™¨ï¼Œæ•°æ®ç›´æ¥ä»ç£ç›˜åˆ°ç½‘ç»œï¼ˆæˆ–æ–‡ä»¶åˆ°æ–‡ä»¶ï¼‰æµåŠ¨ï¼Œä»æœªè¿›å…¥CPUç¼“å­˜ã€‚
- en: This is the ideal case. In practice, there are still some copies. The disk might
    read into a kernel buffer first, then the kernel copies (or maps) that buffer
    to the network card's DMA region. But compared to four copies (disk â†’ kernel â†’
    user â†’ kernel â†’ destination), two kernel-level copies is a big improvement.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç†æƒ³çš„æƒ…å†µã€‚åœ¨å®è·µä¸­ï¼Œä»ç„¶å­˜åœ¨ä¸€äº›å‰¯æœ¬ã€‚ç£ç›˜å¯èƒ½é¦–å…ˆè¯»å–åˆ°å†…æ ¸ç¼“å†²åŒºï¼Œç„¶åå†…æ ¸å°†ï¼ˆæˆ–æ˜ å°„ï¼‰è¯¥ç¼“å†²åŒºåˆ°ç½‘ç»œå¡çš„DMAåŒºåŸŸã€‚ä½†ä¸å››æ¬¡å¤åˆ¶ï¼ˆç£ç›˜ â†’
    å†…æ ¸ â†’ ç”¨æˆ· â†’ å†…æ ¸ â†’ ç›®çš„åœ°ï¼‰ç›¸æ¯”ï¼Œä¸¤æ¬¡å†…æ ¸çº§åˆ«çš„å¤åˆ¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ”¹è¿›ã€‚
- en: Zero-copy only works when you don't need to examine or modify the data. The
    moment you need to transform data - parse it, compress it, encrypt it - you need
    access to it in user space. You can't ask the kernel to "compress this data before
    writing it" (well, you can in some specialized cases with kernel-level compression,
    but it's not broadly applicable). You need to copy data into your process, transform
    it, and copy it back out.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é›¶æ‹·è´ä»…åœ¨ä¸éœ€è¦æ£€æŸ¥æˆ–ä¿®æ”¹æ•°æ®æ—¶æ‰æœ‰æ•ˆã€‚å½“ä½ éœ€è¦è½¬æ¢æ•°æ®â€”â€”è§£æå®ƒã€å‹ç¼©å®ƒã€åŠ å¯†å®ƒâ€”â€”æ—¶ï¼Œä½ éœ€è¦è®¿é—®å®ƒã€‚ä½ ä¸èƒ½è¦æ±‚å†…æ ¸â€œåœ¨å†™å…¥ä¹‹å‰å‹ç¼©è¿™äº›æ•°æ®â€ï¼ˆå¥½å§ï¼Œåœ¨æŸäº›å…·æœ‰å†…æ ¸çº§å‹ç¼©çš„ä¸“ç”¨æƒ…å†µä¸‹ä½ å¯ä»¥è¿™æ ·åšï¼Œä½†å®ƒå¹¶ä¸å¹¿æ³›é€‚ç”¨ï¼‰ã€‚ä½ éœ€è¦å°†æ•°æ®å¤åˆ¶åˆ°ä½ çš„è¿›ç¨‹ï¼Œè½¬æ¢å®ƒï¼Œç„¶åå†å¤åˆ¶å‡ºæ¥ã€‚
- en: 'Zero-copy gives you maximum throughput when data flows untouched. The second
    you need to process the data, you''re back to traditional I/O with all its copying
    overhead. This is why zero-copy is most applicable to proxy scenarios: an HTTP
    proxy forwarding requests, a file server serving static files, a reverse proxy
    routing traffic. In these cases, you''re just moving data from one socket or file
    to another without modification.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: é›¶æ‹·è´åœ¨æ•°æ®æœªå—è§¦ç¢°æ—¶æä¾›æœ€å¤§çš„ååé‡ã€‚å½“ä½ éœ€è¦å¤„ç†æ•°æ®æ—¶ï¼Œä½ å°†å›åˆ°ä¼ ç»Ÿçš„I/Oï¼Œå¹¶å¸¦æœ‰æ‰€æœ‰å¤åˆ¶å¼€é”€ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé›¶æ‹·è´æœ€é€‚ç”¨äºä»£ç†åœºæ™¯ï¼šä¸€ä¸ªHTTPä»£ç†è½¬å‘è¯·æ±‚ï¼Œä¸€ä¸ªæ–‡ä»¶æœåŠ¡å™¨æä¾›é™æ€æ–‡ä»¶ï¼Œä¸€ä¸ªåå‘ä»£ç†è·¯ç”±æµé‡ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œä½ åªæ˜¯åœ¨ä¿®æ”¹æ•°æ®çš„æƒ…å†µä¸‹ä»ä¸€ä¸ªå¥—æ¥å­—æˆ–æ–‡ä»¶ç§»åŠ¨åˆ°å¦ä¸€ä¸ªã€‚
- en: 'There''s also a practical limitation: zero-copy only works for certain source-destination
    combinations. Linux''s `sendfile()` requires the source to be a regular file (something
    that supports `mmap()`), not a pipe or socket. You can''t use `sendfile()` for
    socket-to-socket transfers at all on Linux - the input must be a file. For socket-to-socket
    zero-copy, you''d need `splice()` with an intermediate pipe, which is more complex
    to implement.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªå®é™…é™åˆ¶ï¼šé›¶æ‹·è´ä»…é€‚ç”¨äºæŸäº›æº-ç›®æ ‡ç»„åˆã€‚Linuxçš„`sendfile()`è¦æ±‚æºæ˜¯ä¸€ä¸ªå¸¸è§„æ–‡ä»¶ï¼ˆæ”¯æŒ`mmap()`çš„ä¸œè¥¿ï¼‰ï¼Œè€Œä¸æ˜¯ç®¡é“æˆ–å¥—æ¥å­—ã€‚åœ¨Linuxä¸Šï¼Œä½ ä¸èƒ½å®Œå…¨ä½¿ç”¨`sendfile()`è¿›è¡Œå¥—æ¥å­—åˆ°å¥—æ¥å­—çš„ä¼ è¾“
    - è¾“å…¥å¿…é¡»æ˜¯ä¸€ä¸ªæ–‡ä»¶ã€‚å¯¹äºå¥—æ¥å­—åˆ°å¥—æ¥å­—çš„é›¶æ‹·è´ï¼Œä½ éœ€è¦ä½¿ç”¨å¸¦æœ‰ä¸­é—´ç®¡é“çš„`splice()`ï¼Œè¿™æ›´å¤æ‚æ¥å®ç°ã€‚
- en: Understanding these constraints helps you reason about when your streams will
    be fast (zero-copy applies) and when they'll be slower (fallback to traditional
    I/O). You can't make every stream zero-copy - that's impossible. But you can structure
    your pipelines so the high-volume data paths use zero-copy when available.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£è¿™äº›é™åˆ¶æœ‰åŠ©äºä½ æ¨æ–­å‡ºä½•æ—¶ä½ çš„æµä¼šå¾ˆå¿«ï¼ˆé›¶æ‹·è´é€‚ç”¨ï¼‰ä»¥åŠä½•æ—¶å®ƒä»¬ä¼šè¾ƒæ…¢ï¼ˆå›é€€åˆ°ä¼ ç»ŸI/Oï¼‰ã€‚ä½ ä¸èƒ½ä½¿æ¯ä¸ªæµéƒ½å®ç°é›¶æ‹·è´â€”â€”è¿™æ˜¯ä¸å¯èƒ½çš„ã€‚ä½†ä½ å¯ä»¥æ„å»ºä½ çš„ç®¡é“ï¼Œä»¥ä¾¿åœ¨å¯ç”¨æ—¶ï¼Œé«˜æµé‡æ•°æ®è·¯å¾„ä½¿ç”¨é›¶æ‹·è´ã€‚
- en: Let's see how this applies to Node.js streams.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è¿™å¦‚ä½•åº”ç”¨äºNode.jsæµã€‚
- en: Zero-Copy in Node.js Streams
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Node.jsæµçš„é›¶æ‹·è´
- en: A common misconception is that Node.js automatically uses zero-copy techniques
    like `sendfile()` when you pipe a file to a socket. It doesn't.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¸¸è§çš„è¯¯è§£æ˜¯ï¼Œå½“ä½ å°†æ–‡ä»¶ç®¡é“åˆ°å¥—æ¥å­—æ—¶ï¼ŒNode.jsä¼šè‡ªåŠ¨ä½¿ç”¨åƒ`sendfile()`è¿™æ ·çš„é›¶æ‹·è´æŠ€æœ¯ã€‚å®ƒä¸ä¼šã€‚
- en: 'When you write code like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ ç¼–å†™å¦‚ä¸‹ä»£ç æ—¶ï¼š
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Node.js does **not** automatically use the `sendfile()` system call. The standard
    stream `pipe()` method reads data through user space buffers using regular `read()`
    and `write()` operations. The data flows from disk into a kernel buffer, then
    into your Node.js process memory, then back to a kernel buffer, and finally to
    the socket. That's the traditional four-copy path we discussed earlier.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js**ä¸**ä¼šè‡ªåŠ¨ä½¿ç”¨`sendfile()`ç³»ç»Ÿè°ƒç”¨ã€‚æ ‡å‡†çš„æµ`pipe()`æ–¹æ³•é€šè¿‡ç”¨æˆ·ç©ºé—´ç¼“å†²åŒºä½¿ç”¨å¸¸è§„çš„`read()`å’Œ`write()`æ“ä½œè¯»å–æ•°æ®ã€‚æ•°æ®ä»ç£ç›˜æµå…¥å†…æ ¸ç¼“å†²åŒºï¼Œç„¶åè¿›å…¥ä½ çš„Node.jsè¿›ç¨‹å†…å­˜ï¼Œç„¶åå›åˆ°å†…æ ¸ç¼“å†²åŒºï¼Œæœ€ååˆ°è¾¾å¥—æ¥å­—ã€‚è¿™å°±æ˜¯æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„ä¼ ç»Ÿå››æ‹·è´è·¯å¾„ã€‚
- en: 'Despite claims you may have read elsewhere, Node.js streams don''t use kernel-level
    zero-copy. Here''s what''s actually true:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä½ å¯èƒ½åœ¨å…¶ä»–åœ°æ–¹çœ‹åˆ°è¿‡è¿™æ ·çš„è¯´æ³•ï¼Œä½†Node.jsæµå¹¶ä¸ä½¿ç”¨å†…æ ¸çº§åˆ«çš„é›¶æ‹·è´ã€‚è¿™é‡Œå®é™…ä¸Šæ˜¯çœŸçš„ï¼š
- en: '**libuv does have `uv_fs_sendfile()`**, but it''s used for file-to-file operations
    like `fs.copyFile()`, not for stream piping. When you call `fs.copyFile()`, libuv
    can use the kernel''s efficient file copying mechanisms. On Linux, this might
    use `sendfile()` or copy-on-write reflinks (with `COPYFILE_FICLONE` flag) if the
    filesystem supports them. But this is file-to-file, not file-to-socket.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**libuvç¡®å®æœ‰`uv_fs_sendfile()`**ï¼Œä½†å®ƒç”¨äºæ–‡ä»¶åˆ°æ–‡ä»¶çš„æ“ä½œï¼Œå¦‚`fs.copyFile()`ï¼Œè€Œä¸æ˜¯ç”¨äºæµç®¡é“ã€‚å½“ä½ è°ƒç”¨`fs.copyFile()`æ—¶ï¼Œlibuvå¯ä»¥ä½¿ç”¨å†…æ ¸çš„é«˜æ•ˆæ–‡ä»¶å¤åˆ¶æœºåˆ¶ã€‚åœ¨Linuxä¸Šï¼Œè¿™å¯èƒ½ä¼šä½¿ç”¨`sendfile()`æˆ–åŸºäºå†™æ—¶å¤åˆ¶çš„reflinksï¼ˆå¸¦æœ‰`COPYFILE_FICLONE`æ ‡å¿—ï¼‰ï¼Œå¦‚æœæ–‡ä»¶ç³»ç»Ÿæ”¯æŒçš„è¯ã€‚ä½†è¿™åªæ˜¯æ–‡ä»¶åˆ°æ–‡ä»¶ï¼Œä¸æ˜¯æ–‡ä»¶åˆ°å¥—æ¥å­—ã€‚'
- en: '**Node.js streams use buffered I/O through JavaScript.** When you `pipe()`,
    Node sets up event listeners. The readable stream emits `data` events with Buffer
    chunks. The writable stream''s `write()` method is called for each chunk. This
    all happens in JavaScript, with data passing through V8''s heap. There''s no magic
    kernel bypass.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**Node.jsæµä½¿ç”¨JavaScriptè¿›è¡Œç¼“å†²I/O**ã€‚å½“ä½ `pipe()`æ—¶ï¼ŒNodeä¼šè®¾ç½®äº‹ä»¶ç›‘å¬å™¨ã€‚å¯è¯»æµä¼šå‘å‡ºå¸¦æœ‰Bufferå—çš„`data`äº‹ä»¶ã€‚å¯å†™æµçš„`write()`æ–¹æ³•ä¼šå¯¹æ¯ä¸ªå—è¿›è¡Œè°ƒç”¨ã€‚è¿™ä¸€åˆ‡éƒ½åœ¨JavaScriptä¸­å‘ç”Ÿï¼Œæ•°æ®é€šè¿‡V8çš„å †ä¼ é€’ã€‚è¿™é‡Œæ²¡æœ‰ç¥å¥‡çš„å†…æ ¸ç»•è¿‡ã€‚'
- en: 'Why doesn''t Node.js use `sendfile()` for file-to-socket streaming? Several
    reasons:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆNode.jsä¸ä½¿ç”¨`sendfile()`è¿›è¡Œæ–‡ä»¶åˆ°å¥—æ¥å­—æµä¼ è¾“ï¼Ÿæœ‰å‡ ä¸ªåŸå› ï¼š
- en: '`sendfile()` behaves differently on Linux, macOS, and FreeBSD. Windows uses
    `TransmitFile()` with different semantics. The abstraction cost adds up.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sendfile()`åœ¨Linuxã€macOSå’ŒFreeBSDä¸Šçš„è¡Œä¸ºä¸åŒã€‚Windowsä½¿ç”¨`TransmitFile()`ï¼Œè¯­ä¹‰ä¸åŒã€‚æŠ½è±¡æˆæœ¬ç´¯åŠ ã€‚'
- en: '**HTTPS complicates things.** Zero-copy requires data to flow untouched through
    the kernel. Traditional TLS requires encrypting every byte in user space. Linux
    4.13+ introduced kernel TLS (kTLS), which can enable zero-copy for TLS traffic
    by handling encryption in the kernel. However, Node.js doesn''t currently use
    kTLS, so HTTPS traffic still requires user-space encryption. Since most production
    traffic is HTTPS, the practical benefit of kernel-level zero-copy in Node.js is
    limited.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**HTTPSä½¿äº‹æƒ…å˜å¾—å¤æ‚**ã€‚é›¶æ‹·è´éœ€è¦æ•°æ®åœ¨å†…æ ¸ä¸­æ— æ”¹åŠ¨åœ°æµåŠ¨ã€‚ä¼ ç»Ÿçš„TLSéœ€è¦åœ¨ç”¨æˆ·ç©ºé—´ä¸­åŠ å¯†æ¯ä¸ªå­—èŠ‚ã€‚Linux 4.13+å¼•å…¥äº†å†…æ ¸TLSï¼ˆkTLSï¼‰ï¼Œå®ƒå¯ä»¥é€šè¿‡åœ¨å†…æ ¸ä¸­å¤„ç†åŠ å¯†æ¥å¯ç”¨TLSæµé‡çš„é›¶æ‹·è´ã€‚ç„¶è€Œï¼ŒNode.jsç›®å‰æ²¡æœ‰ä½¿ç”¨kTLSï¼Œæ‰€ä»¥HTTPSæµé‡ä»ç„¶éœ€è¦ç”¨æˆ·ç©ºé—´åŠ å¯†ã€‚ç”±äºå¤§å¤šæ•°ç”Ÿäº§æµé‡éƒ½æ˜¯HTTPSï¼ŒNode.jsä¸­å†…æ ¸çº§åˆ«é›¶æ‹·è´çš„å®é™…å¥½å¤„æœ‰é™ã€‚'
- en: Node's stream backpressure mechanism relies on JavaScript callbacks and the
    `drain` event. Integrating this with kernel-level `sendfile()` is complex.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nodeçš„æµåå‹æœºåˆ¶ä¾èµ–äºJavaScriptå›è°ƒå’Œ`drain`äº‹ä»¶ã€‚å°†å…¶ä¸å†…æ ¸çº§åˆ«çš„`sendfile()`é›†æˆæ˜¯å¤æ‚çš„ã€‚
- en: Node.js had `sendfile()` support briefly in its early days, but it was removed
    after the libeio to libuv transition due to bugs and cross-platform issues.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Node.jsåœ¨å…¶æ—©æœŸé˜¶æ®µçŸ­æš‚åœ°æ”¯æŒäº†`sendfile()`ï¼Œä½†ç”±äºbugå’Œè·¨å¹³å°é—®é¢˜ï¼Œåœ¨ä»libeioåˆ°libuvçš„è¿‡æ¸¡åè¢«ç§»é™¤ã€‚
- en: 'So when would you actually get zero-copy benefits in Node.js? There are a few
    scenarios:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½ å®é™…ä¸Šåœ¨Node.jsä¸­ä½•æ—¶ä¼šå¾—åˆ°é›¶æ‹·è´çš„å¥½å¤„ï¼Ÿæœ‰å‡ ä¸ªåœºæ™¯ï¼š
- en: '**`fs.copyFile()` for file-to-file copies.** This uses libuv''s `uv_fs_copyfile()`
    which can use `sendfile()` or copy-on-write reflinks:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç”¨äºæ–‡ä»¶åˆ°æ–‡ä»¶æ‹·è´çš„`fs.copyFile()`ã€‚** è¿™ä½¿ç”¨libuvçš„`uv_fs_copyfile()`ï¼Œå®ƒå¯ä»¥ä½¿ç”¨`sendfile()`æˆ–å†™æ—¶å¤åˆ¶reflinksï¼š'
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: On filesystems that support reflinks (like Btrfs, XFS, APFS), this creates an
    instant copy that shares physical blocks until one copy is modified. That's true
    zero-copy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ”¯æŒreflinksçš„æ–‡ä»¶ç³»ç»Ÿï¼ˆå¦‚Btrfsã€XFSã€APFSï¼‰ä¸Šï¼Œè¿™ä¼šåˆ›å»ºä¸€ä¸ªå³æ—¶å‰¯æœ¬ï¼Œç›´åˆ°å…¶ä¸­ä¸€ä¸ªå‰¯æœ¬è¢«ä¿®æ”¹ï¼Œæ‰ä¼šå…±äº«ç‰©ç†å—ã€‚è¿™æ‰æ˜¯çœŸæ­£çš„é›¶æ‹·è´ã€‚
- en: '**Native addons.** If you absolutely need `sendfile()` for file-to-socket,
    you can write a native addon that calls it directly. This requires handling partial
    writes, backpressure, and platform differences yourself. It''s rarely worth it
    for most applications.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸç”Ÿæ’ä»¶ã€‚** å¦‚æœä½ ç»å¯¹éœ€è¦`sendfile()`æ¥è¿›è¡Œæ–‡ä»¶åˆ°å¥—æ¥å­—çš„ä¼ è¾“ï¼Œä½ å¯ä»¥ç¼–å†™ä¸€ä¸ªåŸç”Ÿæ’ä»¶æ¥ç›´æ¥è°ƒç”¨å®ƒã€‚è¿™éœ€è¦ä½ è‡ªå·±å¤„ç†éƒ¨åˆ†å†™å…¥ã€èƒŒå‹å’Œå¹³å°å·®å¼‚ã€‚å¯¹äºå¤§å¤šæ•°åº”ç”¨æ¥è¯´ï¼Œè¿™æ ·åšå¾ˆå°‘å€¼å¾—ã€‚'
- en: '**HTTP/2 with `respondWithFile()`.** The `http2` module has `http2stream.respondWithFile()`
    and `respondWithFD()` methods that are optimized for serving files. These still
    go through user space but are more efficient than manual streaming.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨`respondWithFile()`çš„HTTP/2ã€‚** `http2`æ¨¡å—æœ‰`http2stream.respondWithFile()`å’Œ`respondWithFD()`æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•é’ˆå¯¹æ–‡ä»¶æœåŠ¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚è¿™äº›æ–¹æ³•ä»ç„¶é€šè¿‡ç”¨æˆ·ç©ºé—´è¿›è¡Œï¼Œä½†æ¯”æ‰‹åŠ¨æµå¼ä¼ è¾“æ›´é«˜æ•ˆã€‚'
- en: 'The practical implications: don''t structure your code expecting automatic
    zero-copy magic. Instead, focus on the optimizations you can control - buffer
    sizes, avoiding unnecessary copies in your code, and using `_writev()` for batching.
    These give you real, measurable wins.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…å½±å“ï¼šä¸è¦æœŸæœ›ä»£ç ä¼šè‡ªåŠ¨å®ç°é›¶æ‹·è´é­”æ³•ã€‚ç›¸åï¼Œä¸“æ³¨äºä½ å¯ä»¥æ§åˆ¶çš„ä¼˜åŒ–â€”â€”ç¼“å†²åŒºå¤§å°ã€é¿å…ä»£ç ä¸­çš„ä¸å¿…è¦æ‹·è´ï¼Œä»¥åŠä½¿ç”¨`_writev()`è¿›è¡Œæ‰¹å¤„ç†ã€‚è¿™äº›å¯ä»¥ç»™ä½ å¸¦æ¥çœŸå®ã€å¯è¡¡é‡çš„æ”¶ç›Šã€‚
- en: The stream abstraction prioritizes correctness, flexibility, and cross-platform
    compatibility over maximum raw throughput. For most applications, this is the
    right trade-off. The scenarios where kernel-level zero-copy matters (serving huge
    files over HTTP to thousands of concurrent clients) are better handled by specialized
    software like nginx or a CDN, not Node.js.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æµæŠ½è±¡ä¼˜å…ˆè€ƒè™‘æ­£ç¡®æ€§ã€çµæ´»æ€§å’Œè·¨å¹³å°å…¼å®¹æ€§ï¼Œè€Œä¸æ˜¯æœ€å¤§åŸå§‹ååé‡ã€‚å¯¹äºå¤§å¤šæ•°åº”ç”¨æ¥è¯´ï¼Œè¿™æ˜¯æ­£ç¡®çš„æƒè¡¡ã€‚åœ¨å†…æ ¸çº§åˆ«é›¶æ‹·è´è‡³å…³é‡è¦çš„åœºæ™¯ï¼ˆå¦‚é€šè¿‡HTTPå‘æ•°åƒä¸ªå¹¶å‘å®¢æˆ·ç«¯æä¾›å¤§æ–‡ä»¶ï¼‰ä¸­ï¼Œæœ€å¥½ç”±åƒnginxæˆ–CDNè¿™æ ·çš„ä¸“ç”¨è½¯ä»¶æ¥å¤„ç†ï¼Œè€Œä¸æ˜¯Node.jsã€‚
- en: The concepts of zero-copy are still useful even without kernel bypass. You can
    minimize copies in your own code, and that's what we'll focus on next.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ²¡æœ‰ç»•è¿‡å†…æ ¸ï¼Œé›¶æ‹·è´çš„æ¦‚å¿µä»ç„¶æœ‰ç”¨ã€‚ä½ å¯ä»¥æœ€å°åŒ–è‡ªå·±çš„ä»£ç ä¸­çš„æ‹·è´ï¼Œè¿™æ˜¯æˆ‘ä»¬æ¥ä¸‹æ¥è¦å…³æ³¨çš„ã€‚
- en: Memory Mapping
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…å­˜æ˜ å°„
- en: 'There''s another zero-copy approach worth understanding: memory mapping. Instead
    of reading a file into a buffer, you can map the file directly into your process''s
    address space. The file''s contents appear as a region of memory, and you access
    it by reading from that memory region.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—äº†è§£çš„å¦ä¸€ç§é›¶æ‹·è´æ–¹æ³•æ˜¯å†…å­˜æ˜ å°„ã€‚ä½ ä¸éœ€è¦å°†æ–‡ä»¶è¯»å…¥ç¼“å†²åŒºï¼Œè€Œæ˜¯å¯ä»¥ç›´æ¥å°†æ–‡ä»¶æ˜ å°„åˆ°ä½ çš„è¿›ç¨‹åœ°å€ç©ºé—´ã€‚æ–‡ä»¶çš„å†…å®¹å°†ä½œä¸ºä¸€ä¸ªå†…å­˜åŒºåŸŸå‡ºç°ï¼Œä½ å¯ä»¥é€šè¿‡è¯»å–è¯¥å†…å­˜åŒºåŸŸæ¥è®¿é—®å®ƒã€‚
- en: Memory mapping uses the OS's virtual memory system. The kernel maps the file's
    pages into your process's address space, but it doesn't actually load the file
    into physical memory until you access those pages. When you read from a mapped
    page, the OS loads that page from disk. When you write to it, the OS marks the
    page as dirty and flushes it back to disk later.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜æ˜ å°„ä½¿ç”¨æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜ç³»ç»Ÿã€‚å†…æ ¸å°†æ–‡ä»¶çš„é¡µé¢æ˜ å°„åˆ°ä½ çš„è¿›ç¨‹åœ°å€ç©ºé—´ï¼Œä½†å®é™…ä¸Šç›´åˆ°ä½ è®¿é—®è¿™äº›é¡µé¢ï¼Œå®ƒä¸ä¼šå°†æ–‡ä»¶åŠ è½½åˆ°ç‰©ç†å†…å­˜ä¸­ã€‚å½“ä½ ä»æ˜ å°„çš„é¡µé¢è¯»å–æ—¶ï¼Œæ“ä½œç³»ç»Ÿä¼šä»ç£ç›˜åŠ è½½è¯¥é¡µé¢ã€‚å½“ä½ å†™å…¥å®ƒæ—¶ï¼Œæ“ä½œç³»ç»Ÿå°†è¯¥é¡µé¢æ ‡è®°ä¸ºè„ï¼Œå¹¶åœ¨ç¨åå°†å…¶åˆ·æ–°å›ç£ç›˜ã€‚
- en: This is zero-copy in the sense that you're not explicitly copying the file's
    data into a separate buffer. You're accessing the file's data directly via the
    memory map. Changes you make to the mapped memory are reflected in the file.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨é›¶æ‹·è´çš„æ„ä¹‰ä¸Šï¼Œæ˜¯å› ä¸ºä½ æ²¡æœ‰å°†æ–‡ä»¶æ•°æ®æ˜¾å¼åœ°æ‹·è´åˆ°ä¸€ä¸ªå•ç‹¬çš„ç¼“å†²åŒºã€‚ä½ æ˜¯é€šè¿‡å†…å­˜æ˜ å°„ç›´æ¥è®¿é—®æ–‡ä»¶æ•°æ®çš„ã€‚ä½ å¯¹æ˜ å°„å†…å­˜æ‰€åšçš„æ›´æ”¹ä¼šåæ˜ åœ¨æ–‡ä»¶ä¸­ã€‚
- en: Node.js core doesn't expose memory mapping. There's no built-in JavaScript API
    for `mmap()`. To use memory mapping in Node.js, you need third-party native addons.
    The original `node-mmap` and `mmap-io` packages are largely unmaintained and may
    not work with modern Node.js versions. If you need mmap functionality, look for
    actively maintained forks or consider whether the use case truly requires memory
    mapping - often `fs.read()` with appropriate offsets works well enough.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Node.jsæ ¸å¿ƒæ²¡æœ‰æš´éœ²å†…å­˜æ˜ å°„ã€‚æ²¡æœ‰å†…ç½®çš„JavaScript APIç”¨äº`mmap()`ã€‚è¦åœ¨Node.jsä¸­ä½¿ç”¨å†…å­˜æ˜ å°„ï¼Œä½ éœ€è¦ç¬¬ä¸‰æ–¹åŸç”Ÿæ’ä»¶ã€‚åŸå§‹çš„`node-mmap`å’Œ`mmap-io`åŒ…å¤§éƒ¨åˆ†å·²ç»ä¸å†ç»´æŠ¤ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•ä¸ç°ä»£Node.jsç‰ˆæœ¬å…¼å®¹ã€‚å¦‚æœä½ éœ€è¦mmapåŠŸèƒ½ï¼Œè¯·å¯»æ‰¾æ´»è·ƒç»´æŠ¤çš„åˆ†æ”¯ï¼Œæˆ–è€…è€ƒè™‘æ˜¯å¦çœŸçš„éœ€è¦å†…å­˜æ˜ å°„â€”â€”é€šå¸¸ä½¿ç”¨å¸¦æœ‰é€‚å½“åç§»é‡çš„`fs.read()`å°±è¶³å¤Ÿå¥½äº†ã€‚
- en: Memory mapping is useful for random access to large files. If you need to read
    specific offsets within a multi-gigabyte file, mapping it into memory lets you
    treat it like a huge byte array. No need to seek and read chunks - just index
    into the mapped region.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜æ˜ å°„å¯¹äºå¯¹å¤§æ–‡ä»¶çš„éšæœºè®¿é—®å¾ˆæœ‰ç”¨ã€‚å¦‚æœä½ éœ€è¦è¯»å–å¤šå‰å­—èŠ‚æ–‡ä»¶ä¸­çš„ç‰¹å®šåç§»é‡ï¼Œå°†å…¶æ˜ å°„åˆ°å†…å­˜ä¸­å¯ä»¥è®©ä½ åƒå¤„ç†å·¨å¤§çš„å­—èŠ‚æ•°ç»„ä¸€æ ·å¤„ç†å®ƒã€‚æ— éœ€æœç´¢å’Œè¯»å–æ•°æ®å—
    - åªéœ€ç´¢å¼•åˆ°æ˜ å°„åŒºåŸŸã€‚
- en: But memory mapping has trade-offs. The OS does apply readahead to memory-mapped
    files, but the optimization works differently than with `read()`. With `read()`,
    the kernel knows exactly how much data you want upfront. With mmap, the kernel
    must detect your access pattern through page faults - you access a page, the kernel
    loads it and potentially prefetches nearby pages. This reactive approach adds
    latency for initial accesses that `read()` avoids. For pure sequential streaming,
    a read stream is usually faster.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å†…å­˜æ˜ å°„ä¹Ÿæœ‰æƒè¡¡ã€‚æ“ä½œç³»ç»Ÿç¡®å®ä¼šå¯¹å†…å­˜æ˜ å°„æ–‡ä»¶åº”ç”¨é¢„è¯»ï¼Œä½†ä¼˜åŒ–æ–¹å¼ä¸`read()`ä¸åŒã€‚ä½¿ç”¨`read()`æ—¶ï¼Œå†…æ ¸ç¡®åˆ‡åœ°çŸ¥é“ä½ äº‹å…ˆéœ€è¦å¤šå°‘æ•°æ®ã€‚ä½¿ç”¨mmapæ—¶ï¼Œå†…æ ¸å¿…é¡»é€šè¿‡é¡µé¢é”™è¯¯æ¥æ£€æµ‹ä½ çš„è®¿é—®æ¨¡å¼
    - ä½ è®¿é—®ä¸€ä¸ªé¡µé¢ï¼Œå†…æ ¸åŠ è½½å®ƒå¹¶å¯èƒ½é¢„å–é™„è¿‘çš„é¡µé¢ã€‚è¿™ç§ååº”å¼æ–¹æ³•ä¸ºåˆå§‹è®¿é—®æ·»åŠ äº†å»¶è¿Ÿï¼Œè€Œ`read()`å¯ä»¥é¿å…ã€‚å¯¹äºçº¯é¡ºåºæµï¼Œè¯»å–æµé€šå¸¸æ›´å¿«ã€‚
- en: For streaming, memory mapping rarely makes sense. For random access workloads
    (like a database), it can work well. Just be aware of the trade-offs and measure
    performance for your specific use case.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæµå¤„ç†ï¼Œå†…å­˜æ˜ å°„å¾ˆå°‘æœ‰æ„ä¹‰ã€‚å¯¹äºéšæœºè®¿é—®å·¥ä½œè´Ÿè½½ï¼ˆå¦‚æ•°æ®åº“ï¼‰ï¼Œå®ƒå¯ä»¥å·¥ä½œå¾—å¾ˆå¥½ã€‚åªéœ€æ³¨æ„æƒè¡¡å¹¶æµ‹é‡ç‰¹å®šç”¨ä¾‹çš„æ€§èƒ½ã€‚
- en: Avoiding Unnecessary Buffer Copies in Your Code
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¿å…åœ¨æ‚¨çš„ä»£ç ä¸­ä¸å¿…è¦çš„ç¼“å†²åŒºå¤åˆ¶
- en: Even when Node.js can't use OS-level zero-copy, you can avoid unnecessary copies
    in your own code. Let's talk about common patterns that introduce extra copies
    and how to eliminate them.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿Node.jsæ— æ³•ä½¿ç”¨æ“ä½œç³»ç»Ÿçº§åˆ«çš„é›¶å¤åˆ¶ï¼Œä½ ä»ç„¶å¯ä»¥åœ¨è‡ªå·±çš„ä»£ç ä¸­é¿å…ä¸å¿…è¦çš„å¤åˆ¶ã€‚è®©æˆ‘ä»¬è°ˆè°ˆå¼•å…¥é¢å¤–å¤åˆ¶çš„å¸¸è§æ¨¡å¼ä»¥åŠå¦‚ä½•æ¶ˆé™¤å®ƒä»¬ã€‚
- en: 'One frequent culprit is `Buffer.concat()`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¸¸è§çš„ç½ªé­ç¥¸é¦–æ˜¯`Buffer.concat()`ï¼š
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This pattern collects chunks into an array, then concatenates them. The `Buffer.concat()`
    call allocates a new buffer and copies all chunks into it. That's an extra copy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¨¡å¼å°†æ•°æ®å—æ”¶é›†åˆ°ä¸€ä¸ªæ•°ç»„ä¸­ï¼Œç„¶åè¿æ¥å®ƒä»¬ã€‚`Buffer.concat()`è°ƒç”¨åˆ†é…ä¸€ä¸ªæ–°çš„ç¼“å†²åŒºå¹¶å°†æ‰€æœ‰æ•°æ®å—å¤åˆ¶åˆ°å…¶ä¸­ã€‚è¿™æ˜¯ä¸€ä¸ªé¢å¤–çš„å¤åˆ¶ã€‚
- en: 'If you''re going to process the data as one contiguous buffer, this copy is
    unavoidable. But if you can process chunks individually, skip the concatenation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ‰“ç®—å°†æ•°æ®ä½œä¸ºè¿ç»­çš„ç¼“å†²åŒºè¿›è¡Œå¤„ç†ï¼Œè¿™ä¸ªå¤åˆ¶æ˜¯ä¸å¯é¿å…çš„ã€‚ä½†å¦‚æœä½ å¯ä»¥å•ç‹¬å¤„ç†æ•°æ®å—ï¼Œåˆ™å¯ä»¥è·³è¿‡è¿æ¥ï¼š
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Process each chunk as it arrives. No intermediate buffer, no concatenation copy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§åˆ°è¾¾çš„é¡ºåºå¤„ç†æ¯ä¸ªæ•°æ®å—ã€‚æ²¡æœ‰ä¸­é—´ç¼“å†²åŒºï¼Œæ²¡æœ‰è¿æ¥å¤åˆ¶ã€‚
- en: 'Another anti-pattern is converting buffers to strings and back:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªåæ¨¡å¼æ˜¯å°†ç¼“å†²åŒºè½¬æ¢ä¸ºå­—ç¬¦ä¸²ç„¶åå†è½¬æ¢å›æ¥ï¼š
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each conversion allocates new memory and copies data. If your processing can
    work directly on buffers (using `buffer.indexOf()`, `buffer.subarray()`, etc.),
    avoid string conversion.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡è½¬æ¢éƒ½ä¼šåˆ†é…æ–°çš„å†…å­˜å¹¶å¤åˆ¶æ•°æ®ã€‚å¦‚æœä½ çš„å¤„ç†å¯ä»¥ç›´æ¥åœ¨ç¼“å†²åŒºä¸Šè¿›è¡Œï¼ˆä½¿ç”¨`buffer.indexOf()`ã€`buffer.subarray()`ç­‰ï¼‰ï¼Œè¯·é¿å…å­—ç¬¦ä¸²è½¬æ¢ã€‚
- en: 'Buffer slicing is a zero-copy operation when used correctly:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ­£ç¡®ä½¿ç”¨æ—¶ï¼Œç¼“å†²åŒºåˆ‡ç‰‡æ˜¯ä¸€ç§é›¶å¤åˆ¶æ“ä½œï¼š
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This doesn't copy data. It creates a view of the original buffer from byte 10
    to byte 50\. The slice shares the underlying memory with the original buffer.
    Modifying the slice modifies the original buffer. This is zero-copy slicing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ä¼šå¤åˆ¶æ•°æ®ã€‚å®ƒä»å­—èŠ‚10åˆ°å­—èŠ‚50åˆ›å»ºåŸå§‹ç¼“å†²åŒºçš„è§†å›¾ã€‚åˆ‡ç‰‡ä¸åŸå§‹ç¼“å†²åŒºå…±äº«åº•å±‚å†…å­˜ã€‚ä¿®æ”¹åˆ‡ç‰‡ä¼šä¿®æ”¹åŸå§‹ç¼“å†²åŒºã€‚è¿™æ˜¯é›¶å¤åˆ¶åˆ‡ç‰‡ã€‚
- en: 'Note: Use `subarray()` instead of `slice()`. `Buffer.slice()` has the same
    behavior, but it''s deprecated (DEP0158) because it''s inconsistent with `TypedArray.prototype.slice()`
    which creates a copy. As of Node.js v25, calling `slice()` emits runtime deprecation
    warnings. Use `subarray()` instead - it''s the recommended API and won''t trigger
    warnings.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šä½¿ç”¨`subarray()`è€Œä¸æ˜¯`slice()`ã€‚`Buffer.slice()`å…·æœ‰ç›¸åŒçš„è¡Œä¸ºï¼Œä½†å®ƒå·²è¢«å¼ƒç”¨ï¼ˆDEP0158ï¼‰ï¼Œå› ä¸ºå®ƒä¸`TypedArray.prototype.slice()`ä¸ä¸€è‡´ï¼Œåè€…ä¼šåˆ›å»ºä¸€ä¸ªå‰¯æœ¬ã€‚ä»Node.js
    v25å¼€å§‹ï¼Œè°ƒç”¨`slice()`ä¼šå‘å‡ºè¿è¡Œæ—¶å¼ƒç”¨è­¦å‘Šã€‚è¯·ä½¿ç”¨`subarray()`ä»£æ›¿ - å®ƒæ˜¯æ¨èçš„APIï¼Œä¸ä¼šè§¦å‘è­¦å‘Šã€‚
- en: But if you then modify the original buffer's length or reallocate it, the slice
    becomes invalid. And if you slice tiny fragments from many large buffers and keep
    those slices alive, you're preventing the large buffers from being garbage collected.
    The small slice holds a reference to the large buffer, keeping it in memory.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ ä¿®æ”¹åŸå§‹ç¼“å†²åŒºçš„é•¿åº¦æˆ–é‡æ–°åˆ†é…å®ƒï¼Œåˆ‡ç‰‡å°±å˜å¾—æ— æ•ˆäº†ã€‚å¦‚æœä½ ä»è®¸å¤šå¤§ç¼“å†²åŒºä¸­åˆ‡å‡ºå¾®å°çš„ç‰‡æ®µå¹¶ä¿æŒè¿™äº›åˆ‡ç‰‡å­˜æ´»ï¼Œä½ å°†é˜»æ­¢å¤§ç¼“å†²åŒºè¢«åƒåœ¾å›æ”¶ã€‚å°çš„åˆ‡ç‰‡æŒæœ‰å¯¹å¤§ç¼“å†²åŒºçš„å¼•ç”¨ï¼Œä½¿å…¶ä¿æŒåœ¨å†…å­˜ä¸­ã€‚
- en: 'The safe pattern: slice for temporary processing, then copy if you need to
    retain the data:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰å…¨æ¨¡å¼ï¼šåˆ‡ç‰‡è¿›è¡Œä¸´æ—¶å¤„ç†ï¼Œç„¶åå¦‚æœéœ€è¦ä¿ç•™æ•°æ®å°±è¿›è¡Œå¤åˆ¶ï¼š
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'One more pattern: avoid `Buffer.from(buffer)` unless you explicitly need a
    copy. If you just need to pass a buffer to another function, pass the original:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæ¨¡å¼ï¼šé™¤éä½ æ˜ç¡®éœ€è¦å¤åˆ¶ï¼Œå¦åˆ™è¯·é¿å…ä½¿ç”¨`Buffer.from(buffer)`ã€‚å¦‚æœä½ åªéœ€è¦å°†ç¼“å†²åŒºä¼ é€’ç»™å¦ä¸€ä¸ªå‡½æ•°ï¼Œè¯·ä¼ é€’åŸå§‹ç¼“å†²åŒºï¼š
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The writable stream doesn't modify the buffer (unless it's a very unusual stream),
    so there's no reason to copy it.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å¯å†™æµä¸ä¼šä¿®æ”¹ç¼“å†²åŒºï¼ˆé™¤éå®ƒæ˜¯ä¸€ä¸ªéå¸¸ä¸å¯»å¸¸çš„æµï¼‰ï¼Œå› æ­¤æ²¡æœ‰å¿…è¦å¤åˆ¶å®ƒã€‚
- en: Every `Buffer.concat()`, `Buffer.from()`, and `buffer.toString()` potentially
    allocates and copies. Only do these operations when the semantics of your code
    require them, not out of habit. And remember to use `subarray()` instead of `slice()`
    for zero-copy views.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ª`Buffer.concat()`ã€`Buffer.from()`å’Œ`buffer.toString()`éƒ½å¯èƒ½åˆ†é…å’Œå¤åˆ¶ã€‚åªæœ‰åœ¨ä½ çš„ä»£ç è¯­ä¹‰éœ€è¦æ—¶æ‰æ‰§è¡Œè¿™äº›æ“ä½œï¼Œè€Œä¸æ˜¯å‡ºäºä¹ æƒ¯ã€‚å¹¶ä¸”è¯·è®°ä½ï¼Œä½¿ç”¨`subarray()`è€Œä¸æ˜¯`slice()`æ¥è·å–é›¶æ‹·è´è§†å›¾ã€‚
- en: Scatter/Gather I/O
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•£åˆ—/æ”¶é›†I/O
- en: Scatter/gather I/O is a technique for reducing the number of syscalls when working
    with multiple buffers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æ•£åˆ—/æ”¶é›†I/Oæ˜¯ä¸€ç§åœ¨å¤„ç†å¤šä¸ªç¼“å†²åŒºæ—¶å‡å°‘ç³»ç»Ÿè°ƒç”¨æ¬¡æ•°çš„æŠ€æœ¯ã€‚
- en: 'In traditional I/O, if you want to write three buffers to a file, you make
    three write syscalls:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼ ç»Ÿçš„I/Oä¸­ï¼Œå¦‚æœä½ æƒ³å°†ä¸‰ä¸ªç¼“å†²åŒºå†™å…¥æ–‡ä»¶ï¼Œä½ éœ€è¦è¿›è¡Œä¸‰æ¬¡å†™å…¥ç³»ç»Ÿè°ƒç”¨ï¼š
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Each syscall has overhead. The CPU switches from user mode to kernel mode, the
    kernel processes the request, and the CPU switches back to user mode. For small
    writes, this overhead can exceed the actual I/O cost.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªç³»ç»Ÿè°ƒç”¨éƒ½æœ‰å¼€é”€ã€‚CPUä»ç”¨æˆ·æ¨¡å¼åˆ‡æ¢åˆ°å†…æ ¸æ¨¡å¼ï¼Œå†…æ ¸å¤„ç†è¯·æ±‚ï¼Œç„¶åCPUåˆ‡æ¢å›ç”¨æˆ·æ¨¡å¼ã€‚å¯¹äºå°å†™å…¥ï¼Œè¿™ä¸ªå¼€é”€å¯èƒ½è¶…è¿‡å®é™…çš„I/Oæˆæœ¬ã€‚
- en: Let's quantify this overhead. A syscall typically takes 50-200 nanoseconds just
    for the mode switch, depending on CPU and OS. Then there's the kernel's work of
    processing the request - validating parameters, setting up I/O, etc. For a simple
    write, this might be another 100-500 nanoseconds. So each syscall has a baseline
    cost of roughly 150-700 nanoseconds before any actual data movement happens.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é‡åŒ–è¿™ä¸ªå¼€é”€ã€‚ç³»ç»Ÿè°ƒç”¨é€šå¸¸ä»…éœ€è¦50-200çº³ç§’æ¥åˆ‡æ¢æ¨¡å¼ï¼Œè¿™å–å†³äºCPUå’Œæ“ä½œç³»ç»Ÿã€‚ç„¶åæ˜¯å†…æ ¸å¤„ç†è¯·æ±‚çš„å·¥ä½œâ€”â€”éªŒè¯å‚æ•°ã€è®¾ç½®I/Oç­‰ã€‚å¯¹äºç®€å•çš„å†™å…¥ï¼Œè¿™å¯èƒ½æ˜¯å¦å¤–100-500çº³ç§’ã€‚å› æ­¤ï¼Œæ¯ä¸ªç³»ç»Ÿè°ƒç”¨åœ¨å‘ç”Ÿä»»ä½•å®é™…æ•°æ®ç§»åŠ¨ä¹‹å‰çš„åŸºç¡€æˆæœ¬å¤§çº¦ä¸º150-700çº³ç§’ã€‚
- en: If you're writing three 1KB buffers, that's three syscalls, so 450-2100 nanoseconds
    of pure overhead. The actual writing of 3KB might take only a few hundred nanoseconds
    on a fast SSD. The syscall overhead can exceed the I/O cost.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ­£åœ¨å†™å…¥ä¸‰ä¸ª1KBçš„ç¼“å†²åŒºï¼Œé‚£ä¹ˆå°±æ˜¯ä¸‰ä¸ªç³»ç»Ÿè°ƒç”¨ï¼Œæ‰€ä»¥çº¯å¼€é”€ä¸º450-2100çº³ç§’ã€‚åœ¨å¿«é€Ÿçš„SSDä¸Šå®é™…å†™å…¥3KBå¯èƒ½åªéœ€è¦å‡ ç™¾çº³ç§’ã€‚ç³»ç»Ÿè°ƒç”¨å¼€é”€å¯èƒ½è¶…è¿‡I/Oæˆæœ¬ã€‚
- en: Now scale this up. If you're writing 1000 small buffers, you're spending 150-700
    microseconds just on syscall overhead. That might not sound like much, but in
    a high-throughput system processing millions of writes per second, this overhead
    adds up to real CPU time.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°†è¿™ä¸ªæ¦‚å¿µæ”¾å¤§ã€‚å¦‚æœä½ æ­£åœ¨ç¼–å†™1000ä¸ªå°ç¼“å†²åŒºï¼Œä½ å°†èŠ±è´¹150-700å¾®ç§’ä»…ä»…ç”¨äºç³»ç»Ÿè°ƒç”¨å¼€é”€ã€‚è¿™å¯èƒ½å¬èµ·æ¥ä¸å¤šï¼Œä½†åœ¨ä¸€ä¸ªæ¯ç§’å¤„ç†æ•°ç™¾ä¸‡æ¬¡å†™å…¥çš„é«˜ååé‡ç³»ç»Ÿä¸­ï¼Œè¿™ä¸ªå¼€é”€ç´¯ç§¯èµ·æ¥å°±æ˜¯å®é™…çš„CPUæ—¶é—´ã€‚
- en: Scatter/gather I/O lets you pass multiple buffers to a single syscall. For writes
    (gather), you're gathering data from multiple buffers and writing it in one operation.
    For reads (scatter), you're scattering incoming data into multiple buffers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ•£åˆ—/æ”¶é›†I/Oå…è®¸ä½ å°†å¤šä¸ªç¼“å†²åŒºä¼ é€’ç»™å•ä¸ªç³»ç»Ÿè°ƒç”¨ã€‚å¯¹äºå†™å…¥ï¼ˆæ”¶é›†ï¼‰ï¼Œä½ ä»å¤šä¸ªç¼“å†²åŒºæ”¶é›†æ•°æ®å¹¶åœ¨ä¸€ä¸ªæ“ä½œä¸­å†™å…¥ã€‚å¯¹äºè¯»å–ï¼ˆæ•£åˆ—ï¼‰ï¼Œä½ å°†ä¼ å…¥æ•°æ®æ•£åˆ—åˆ°å¤šä¸ªç¼“å†²åŒºä¸­ã€‚
- en: On Linux, the `writev()` syscall handles gather writes. You pass an array of
    buffers (technically, an array of iovec structures pointing to buffers), and the
    kernel writes all of them in one operation. This is much more efficient than separate
    write calls.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Linuxä¸Šï¼Œ`writev()`ç³»ç»Ÿè°ƒç”¨å¤„ç†æ”¶é›†å†™å…¥ã€‚ä½ ä¼ é€’ä¸€ä¸ªç¼“å†²åŒºæ•°ç»„ï¼ˆæŠ€æœ¯ä¸Šæ˜¯ä¸€ä¸ªæŒ‡å‘ç¼“å†²åŒºçš„iovecç»“æ„æ•°ç»„çš„æ•°ç»„ï¼‰ï¼Œå†…æ ¸åœ¨ä¸€ä¸ªæ“ä½œä¸­å†™å…¥æ‰€æœ‰è¿™äº›ç¼“å†²åŒºã€‚è¿™æ¯”å•ç‹¬çš„å†™å…¥è°ƒç”¨è¦é«˜æ•ˆå¾—å¤šã€‚
- en: The scatter counterpart is `readv()`, which reads data into multiple buffers.
    You specify an array of buffers, and the kernel fills them in order. If the read
    provides more data than the first buffer can hold, it continues into the second
    buffer, and so on. This is useful when you know the structure of incoming data
    - for example, a fixed-size header followed by variable-length payload. You can
    scatter-read into a header buffer and a payload buffer in one syscall.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ•£åˆ—çš„å¯¹åº”æ“ä½œæ˜¯`readv()`ï¼Œå®ƒå°†æ•°æ®è¯»å–åˆ°å¤šä¸ªç¼“å†²åŒºä¸­ã€‚ä½ æŒ‡å®šä¸€ä¸ªç¼“å†²åŒºæ•°ç»„ï¼Œå†…æ ¸æŒ‰é¡ºåºå¡«å……å®ƒä»¬ã€‚å¦‚æœè¯»å–çš„æ•°æ®å¤šäºç¬¬ä¸€ä¸ªç¼“å†²åŒºèƒ½å®¹çº³çš„ï¼Œå®ƒå°†ç»§ç»­å†™å…¥ç¬¬äºŒä¸ªç¼“å†²åŒºï¼Œä¾æ­¤ç±»æ¨ã€‚å½“ä½ çŸ¥é“ä¼ å…¥æ•°æ®çš„ç»“æ„æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨â€”â€”ä¾‹å¦‚ï¼Œä¸€ä¸ªå›ºå®šå¤§å°çš„å¤´éƒ¨åè·Ÿå¯å˜é•¿åº¦çš„æœ‰æ•ˆè´Ÿè½½ã€‚ä½ å¯ä»¥åœ¨ä¸€ä¸ªç³»ç»Ÿè°ƒç”¨ä¸­å°†æ•°æ®æ•£åˆ—è¯»å–åˆ°å¤´éƒ¨ç¼“å†²åŒºå’Œæœ‰æ•ˆè´Ÿè½½ç¼“å†²åŒºä¸­ã€‚
- en: Node.js exposes gather writes through the `_writev()` method on writable streams,
    but scatter reads aren't directly exposed in the stream API because the stream
    abstraction doesn't know in advance how many buffers to scatter into. However,
    the lower-level `fs.readv()` and `fs.writev()` functions are available if you
    need them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js é€šè¿‡å¯å†™æµä¸Šçš„ `_writev()` æ–¹æ³•å…¬å¼€æ”¶é›†å†™å…¥ï¼Œä½†æµ API ä¸­æ²¡æœ‰ç›´æ¥å…¬å¼€åˆ†æ•£è¯»å–ï¼Œå› ä¸ºæµæŠ½è±¡äº‹å…ˆä¸çŸ¥é“è¦å°†å¤šå°‘ç¼“å†²åŒºåˆ†æ•£åˆ°ã€‚ç„¶è€Œï¼Œå¦‚æœä½ éœ€è¦ï¼Œè¾ƒä½çº§åˆ«çš„
    `fs.readv()` å’Œ `fs.writev()` å‡½æ•°æ˜¯å¯ç”¨çš„ã€‚
- en: Node.js writable streams expose this via the `_writev()` method. If you're implementing
    a custom writable stream and you override `_writev()`, Node will batch writes
    and call your `_writev()` with an array of chunks instead of calling `_write()`
    multiple times.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js çš„å¯å†™æµé€šè¿‡ `_writev()` æ–¹æ³•å…¬å¼€è¿™ä¸€ç‚¹ã€‚å¦‚æœä½ æ­£åœ¨å®ç°è‡ªå®šä¹‰å¯å†™æµå¹¶é‡å†™ `_writev()`ï¼ŒNode å°†æ‰¹é‡å†™å…¥ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªåŒ…å«å—çš„æ•°ç»„è°ƒç”¨ä½ çš„
    `_writev()` è€Œä¸æ˜¯å¤šæ¬¡è°ƒç”¨ `_write()`ã€‚
- en: 'Here''s how it works. When a writable stream is corked or when multiple chunks
    are written rapidly, Node buffers them. If the stream implements `_writev()`,
    Node calls it with all buffered chunks:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å®ƒçš„å·¥ä½œåŸç†ã€‚å½“ä¸€ä¸ªå¯å†™æµè¢«å°å µæˆ–å¤šä¸ªå—å¿«é€Ÿå†™å…¥æ—¶ï¼ŒNode å°†å®ƒä»¬ç¼“å†²ã€‚å¦‚æœæµå®ç°äº† `_writev()`ï¼ŒNode ä¼šç”¨æ‰€æœ‰ç¼“å†²çš„å—è°ƒç”¨å®ƒï¼š
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `chunks` array contains objects with `chunk` and `encoding` properties.
    You extract the chunks and pass them to a syscall that supports vectored I/O.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`chunks` æ•°ç»„åŒ…å«å…·æœ‰ `chunk` å’Œ `encoding` å±æ€§çš„å¯¹è±¡ã€‚ä½ æå–å—å¹¶å°†å®ƒä»¬ä¼ é€’ç»™æ”¯æŒå‘é‡ I/O çš„ç³»ç»Ÿè°ƒç”¨ã€‚'
- en: 'The benefit: instead of N syscalls for N chunks, you make one syscall. For
    a stream writing many small chunks (like a HTTP response with many small writes),
    this can cut syscall overhead a lot.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜ç‚¹ï¼šå¯¹äº N ä¸ªå—è¿›è¡Œ N æ¬¡ç³»ç»Ÿè°ƒç”¨ï¼Œä½ åªéœ€è¿›è¡Œä¸€æ¬¡ç³»ç»Ÿè°ƒç”¨ã€‚å¯¹äºå†™å…¥è®¸å¤šå°å—ï¼ˆå¦‚å¸¦æœ‰è®¸å¤šå°å†™å…¥çš„ HTTP å“åº”ï¼‰çš„æµï¼Œè¿™å¯ä»¥å¤§å¤§å‡å°‘ç³»ç»Ÿè°ƒç”¨å¼€é”€ã€‚
- en: 'But there''s a nuance. Node only calls `_writev()` when multiple chunks are
    buffered. If chunks arrive slowly (one at a time with gaps between), Node calls
    `_write()` for each. To force batching, you can cork the stream:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æœ‰ä¸€ä¸ªç»†å¾®å·®åˆ«ã€‚Node åªåœ¨å¤šä¸ªå—è¢«ç¼“å†²æ—¶è°ƒç”¨ `_writev()`ã€‚å¦‚æœå—ç¼“æ…¢åˆ°è¾¾ï¼ˆä¸€æ¬¡ä¸€ä¸ªï¼Œä¸­é—´æœ‰é—´éš”ï¼‰ï¼ŒNode ä¼šä¸ºæ¯ä¸ªå—è°ƒç”¨ `_write()`ã€‚ä¸ºäº†å¼ºåˆ¶æ‰¹å¤„ç†ï¼Œä½ å¯ä»¥å°å µæµï¼š
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Corking suppresses `_write()` calls and buffers everything. Uncorking flushes
    the buffer, calling `_writev()` with all buffered chunks.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å°å µæŠ‘åˆ¶ `_write()` è°ƒç”¨å¹¶ç¼“å†²ä¸€åˆ‡ã€‚è§£å µåˆ·æ–°ç¼“å†²åŒºï¼Œä½¿ç”¨æ‰€æœ‰ç¼“å†²çš„å—è°ƒç”¨ `_writev()`ã€‚
- en: This is useful when you know you're about to write multiple chunks. Cork before
    the writes, uncork after, and you get batching even if `_write()` would normally
    be called.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ çŸ¥é“ä½ å³å°†å†™å…¥å¤šä¸ªå—æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚åœ¨å†™å…¥ä¹‹å‰å°å µï¼Œå†™å…¥ä¹‹åè§£å µï¼Œå³ä½¿ `_write()` é€šå¸¸ä¼šè¢«è°ƒç”¨ï¼Œä½ ä¹Ÿèƒ½å¾—åˆ°æ‰¹å¤„ç†ã€‚
- en: For scatter reads (the opposite direction), Node doesn't expose `readv()` directly
    because readable streams pull data on demand, and it's not clear how many buffers
    to scatter into. But if you're using low-level `fs` APIs, you can use `fs.readv()`
    to read into multiple buffers in one syscall.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†æ•£è¯»å–ï¼ˆç›¸åçš„æ–¹å‘ï¼‰ï¼ŒNode æ²¡æœ‰ç›´æ¥å…¬å¼€ `readv()`ï¼Œå› ä¸ºå¯è¯»æµæŒ‰éœ€æ‹‰å–æ•°æ®ï¼Œå¹¶ä¸”ä¸æ¸…æ¥šè¦å°†å¤šå°‘ç¼“å†²åŒºåˆ†æ•£åˆ°ã€‚ä½†æ˜¯ï¼Œå¦‚æœä½ ä½¿ç”¨ä½çº§ `fs`
    APIï¼Œä½ å¯ä»¥ä½¿ç”¨ `fs.readv()` åœ¨ä¸€æ¬¡ç³»ç»Ÿè°ƒç”¨ä¸­å°†æ•°æ®è¯»å–åˆ°å¤šä¸ªç¼“å†²åŒºä¸­ã€‚
- en: Batching multiple I/O operations into one syscall reduces overhead. Scatter/gather
    I/O is the mechanism for doing this with multiple buffers.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å¤šä¸ª I/O æ“ä½œæ‰¹é‡åˆ°ä¸€ä¸ªç³»ç»Ÿè°ƒç”¨ä¸­å¯ä»¥å‡å°‘å¼€é”€ã€‚åˆ†æ•£/æ”¶é›† I/O æ˜¯ä½¿ç”¨å¤šä¸ªç¼“å†²åŒºæ‰§è¡Œæ­¤æ“ä½œçš„æœºåˆ¶ã€‚
- en: Implementing _writev() for Maximum Throughput
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°ç”¨äºæœ€å¤§ååé‡çš„ `_writev()`
- en: Here's an example of implementing `_writev()` to optimize a writable stream.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå®ç° `_writev()` ä»¥ä¼˜åŒ–å¯å†™æµçš„ç¤ºä¾‹ã€‚
- en: Suppose you're writing to a network socket and you want to minimize syscalls.
    The socket's default behavior calls `write()` for each chunk. If chunks are small,
    you're making many syscalls.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ æ­£åœ¨å‘ç½‘ç»œå¥—æ¥å­—å†™å…¥ï¼Œå¹¶å¸Œæœ›æœ€å°åŒ–ç³»ç»Ÿè°ƒç”¨ã€‚å¥—æ¥å­—çš„é»˜è®¤è¡Œä¸ºä¸ºæ¯ä¸ªå—è°ƒç”¨ `write()`ã€‚å¦‚æœå—å¾ˆå°ï¼Œä½ ä¼šè¿›è¡Œè®¸å¤šç³»ç»Ÿè°ƒç”¨ã€‚
- en: 'By implementing `_writev()`, you batch writes:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å®ç° `_writev()`ï¼Œä½ æ‰¹é‡å†™å…¥ï¼š
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This `_writev()` concatenates buffers, which does introduce a copy. But it's
    still a net win if the alternative is many small syscalls.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª `_writev()` è¿æ¥ç¼“å†²åŒºï¼Œè¿™ç¡®å®å¼•å…¥äº†å¤åˆ¶ã€‚ä½†å¦‚æœæ›¿ä»£æ–¹æ¡ˆæ˜¯è®¸å¤šå°ç³»ç»Ÿè°ƒç”¨ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªå‡€èµ¢ã€‚
- en: Here's the trade-off. Without `_writev()`, you make N syscalls for N chunks.
    Syscalls are expensive. With `_writev()` that concatenates, you make one copy
    (concatenation) and one syscall. If syscall overhead exceeds copy overhead, batching
    wins.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æƒè¡¡ã€‚æ²¡æœ‰ `_writev()`ï¼Œä½ éœ€è¦å¯¹ N ä¸ªå—è¿›è¡Œ N æ¬¡ç³»ç»Ÿè°ƒç”¨ã€‚ç³»ç»Ÿè°ƒç”¨å¾ˆæ˜‚è´µã€‚ä½¿ç”¨å¯ä»¥è¿æ¥çš„ `_writev()`ï¼Œä½ åªéœ€è¿›è¡Œä¸€æ¬¡å¤åˆ¶ï¼ˆè¿æ¥ï¼‰å’Œä¸€æ¬¡ç³»ç»Ÿè°ƒç”¨ã€‚å¦‚æœç³»ç»Ÿè°ƒç”¨å¼€é”€è¶…è¿‡å¤åˆ¶å¼€é”€ï¼Œæ‰¹å¤„ç†å°±èµ¢äº†ã€‚
- en: 'But you can do better. If your underlying I/O mechanism supports true vectored
    writes (like `writev()` on a file descriptor), you can avoid the concatenation:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä½ å¯ä»¥åšå¾—æ›´å¥½ã€‚å¦‚æœä½ çš„åº•å±‚ I/O æœºåˆ¶æ”¯æŒçœŸæ­£çš„å‘é‡å†™å…¥ï¼ˆå¦‚æ–‡ä»¶æè¿°ç¬¦ä¸Šçš„ `writev()`ï¼‰ï¼Œä½ å¯ä»¥é¿å…è¿æ¥ï¼š
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now you're passing multiple buffers to a vectored I/O syscall. The kernel writes
    them all without you copying them into a single buffer. This is true gather I/O.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ æ­£åœ¨å°†å¤šä¸ªç¼“å†²åŒºä¼ é€’ç»™å‘é‡I/Oç³»ç»Ÿè°ƒç”¨ã€‚å†…æ ¸å°†å®ƒä»¬å…¨éƒ¨å†™å…¥ï¼Œè€Œä¸éœ€è¦ä½ å°†å®ƒä»¬å¤åˆ¶åˆ°ä¸€ä¸ªå•ç‹¬çš„ç¼“å†²åŒºä¸­ã€‚è¿™æ˜¯çœŸæ­£çš„èšé›†I/Oã€‚
- en: For sockets, Node's net.Socket doesn't expose vectored writes at the JavaScript
    level, but libuv (which underlies Node) uses them internally where supported.
    By implementing `_writev()` and letting the socket's internal write handle batching,
    you benefit from libuv's optimizations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¥—æ¥å­—ï¼ŒNodeçš„net.Socketåœ¨JavaScriptçº§åˆ«ä¸æš´éœ²å‘é‡å†™å…¥ï¼Œä½†libuvï¼ˆNodeçš„åŸºç¡€ï¼‰åœ¨æ”¯æŒçš„åœ°æ–¹å†…éƒ¨ä½¿ç”¨å®ƒä»¬ã€‚é€šè¿‡å®ç°`_writev()`å¹¶è®©å¥—æ¥å­—çš„å†…éƒ¨å†™å…¥å¤„ç†æ‰¹å¤„ç†ï¼Œä½ å¯ä»¥ä»libuvçš„ä¼˜åŒ–ä¸­å—ç›Šã€‚
- en: The key is to always implement `_writev()` when your destination supports batched
    writes. Even if you have to concatenate buffers, it's often faster than multiple
    syscalls.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®æ˜¯å§‹ç»ˆåœ¨ç›®æ ‡æ”¯æŒæ‰¹å¤„ç†å†™å…¥æ—¶å®ç°`_writev()`ã€‚å³ä½¿ä½ å¿…é¡»è¿æ¥ç¼“å†²åŒºï¼Œè¿™é€šå¸¸ä¹Ÿæ¯”å¤šä¸ªç³»ç»Ÿè°ƒç”¨æ›´å¿«ã€‚
- en: 'One more pattern: adaptive batching. If chunks are large, batching doesn''t
    help much. If chunks are small, batching matters a lot. You can track chunk sizes
    and cork/uncork dynamically:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæ¨¡å¼ï¼šè‡ªé€‚åº”æ‰¹å¤„ç†ã€‚å¦‚æœæ•°æ®å—å¾ˆå¤§ï¼Œæ‰¹å¤„ç†å¸®åŠ©ä¸å¤§ã€‚å¦‚æœæ•°æ®å—å¾ˆå°ï¼Œæ‰¹å¤„ç†éå¸¸é‡è¦ã€‚ä½ å¯ä»¥è·Ÿè¸ªæ•°æ®å—å¤§å°å’ŒåŠ¨æ€åœ°cork/uncorkï¼š
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This corks on the first small write and uncorks once writes settle. Using the
    write callback ensures we uncork even if some writes fail. The `Math.max(0, ...)`
    guards against the counter going negative if something unexpected happens.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨ç¬¬ä¸€æ¬¡å°å†™å…¥æ—¶corkï¼Œå¹¶åœ¨å†™å…¥ç¨³å®šåuncorkã€‚ä½¿ç”¨å†™å…¥å›è°ƒç¡®ä¿å³ä½¿åœ¨æŸäº›å†™å…¥å¤±è´¥çš„æƒ…å†µä¸‹ä¹Ÿèƒ½uncorkã€‚`Math.max(0, ...)`é˜²æ­¢è®¡æ•°å™¨åœ¨å‘ç”Ÿæ„å¤–æƒ…å†µæ—¶å˜ä¸ºè´Ÿæ•°ã€‚
- en: 'This is a simplified heuristic. In production, you''d track timing and chunk
    sizes more carefully, and add cleanup for stream close/error events. But the idea
    is solid: batch small writes, skip batching for large writes.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å¯å‘å¼æ–¹æ³•ã€‚åœ¨ç”Ÿäº§ä¸­ï¼Œä½ ä¼šæ›´ä»”ç»†åœ°è·Ÿè¸ªæ—¶é—´å’Œæ•°æ®å—å¤§å°ï¼Œå¹¶ä¸ºæµå…³é—­/é”™è¯¯äº‹ä»¶æ·»åŠ æ¸…ç†ã€‚ä½†è¿™ä¸ªæƒ³æ³•æ˜¯ç¨³å›ºçš„ï¼šæ‰¹å¤„ç†å°å†™å…¥ï¼Œå¯¹äºå¤§å†™å…¥åˆ™è·³è¿‡æ‰¹å¤„ç†ã€‚
- en: Buffer Pooling
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºæ± 
- en: Every time you allocate a buffer, you're asking V8 to allocate memory. Every
    allocation is tracked by the garbage collector. When buffers are no longer referenced,
    the GC reclaims them. Frequent allocations create GC pressure - the garbage collector
    has to run more often, pausing your application to reclaim memory.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡ä½ åˆ†é…ä¸€ä¸ªç¼“å†²åŒºï¼Œä½ éƒ½åœ¨è¯·æ±‚V8åˆ†é…å†…å­˜ã€‚æ¯æ¬¡åˆ†é…éƒ½ä¼šè¢«åƒåœ¾æ”¶é›†å™¨è·Ÿè¸ªã€‚å½“ç¼“å†²åŒºä¸å†è¢«å¼•ç”¨æ—¶ï¼ŒGCä¼šå›æ”¶å®ƒä»¬ã€‚é¢‘ç¹çš„åˆ†é…ä¼šåˆ›å»ºGCå‹åŠ›â€”â€”åƒåœ¾æ”¶é›†å™¨å¿…é¡»æ›´é¢‘ç¹åœ°è¿è¡Œï¼Œæš‚åœä½ çš„åº”ç”¨ç¨‹åºä»¥å›æ”¶å†…å­˜ã€‚
- en: For high-throughput streams, buffer allocation overhead can become a bottleneck.
    If you're processing gigabytes of data in small chunks, you might allocate millions
    of buffers, each creating GC work.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé«˜ååé‡æµï¼Œç¼“å†²åŒºåˆ†é…å¼€é”€å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚å¦‚æœä½ åœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶ä½¿ç”¨å°å—æ•°æ®ï¼Œä½ å¯èƒ½ä¼šåˆ†é…æ•°ç™¾ä¸‡ä¸ªç¼“å†²åŒºï¼Œæ¯ä¸ªç¼“å†²åŒºéƒ½ä¼šåˆ›å»ºGCå·¥ä½œã€‚
- en: Understanding the mechanics matters here. When you allocate a buffer with `Buffer.alloc(size)`,
    V8 doesn't just hand you memory. It has to find a free memory region (possibly
    triggering GC if memory is fragmented), initialize metadata to track the allocation,
    potentially zero the memory for security, and link the buffer into its tracking
    structures.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£å…¶æœºåˆ¶åœ¨è¿™é‡Œå¾ˆé‡è¦ã€‚å½“ä½ ä½¿ç”¨`Buffer.alloc(size)`åˆ†é…ç¼“å†²åŒºæ—¶ï¼ŒV8ä¸ä»…ä»…ç»™ä½ å†…å­˜ã€‚å®ƒå¿…é¡»æ‰¾åˆ°ä¸€ä¸ªç©ºé—²çš„å†…å­˜åŒºåŸŸï¼ˆå¦‚æœå†…å­˜ç¢ç‰‡åŒ–å¯èƒ½ä¼šè§¦å‘GCï¼‰ï¼Œåˆå§‹åŒ–å…ƒæ•°æ®ä»¥è·Ÿè¸ªåˆ†é…ï¼Œå¯èƒ½ä¸ºé›¶å†…å­˜ä»¥ä¿éšœå®‰å…¨ï¼Œå¹¶å°†ç¼“å†²åŒºé“¾æ¥åˆ°å…¶è·Ÿè¸ªç»“æ„ä¸­ã€‚
- en: For a small buffer (say 1KB), this allocation might take 500-2000 nanoseconds,
    depending on heap state. If you're allocating one buffer per chunk and processing
    100,000 chunks per second, that's 50-200 milliseconds per second spent just on
    allocation overhead - 5-20% of CPU time wasted on bookkeeping.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªå°ç¼“å†²åŒºï¼ˆæ¯”å¦‚1KBï¼‰ï¼Œè¿™ä¸ªåˆ†é…å¯èƒ½éœ€è¦500-2000çº³ç§’ï¼Œå…·ä½“å–å†³äºå †çŠ¶æ€ã€‚å¦‚æœä½ æ¯ç§’å¤„ç†100,000ä¸ªæ•°æ®å—ï¼Œæ¯ä¸ªæ•°æ®å—åˆ†é…ä¸€ä¸ªç¼“å†²åŒºï¼Œé‚£ä¹ˆä½ æ¯ç§’ä»…åœ¨åˆ†é…å¼€é”€ä¸Šå°±ä¼šèŠ±è´¹50-200æ¯«ç§’â€”â€”5-20%çš„CPUæ—¶é—´æµªè´¹åœ¨ç°¿è®°ä¸Šã€‚
- en: When those buffers become unreachable, the garbage collector has to reclaim
    them. V8's GC has multiple generations (young generation, old generation), and
    objects move between generations based on age. Buffers that live for many GC cycles
    get promoted to the old generation, which is more expensive to collect. Frequent
    buffer allocations and deallocations thrash the GC, causing longer and more frequent
    pause times.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¿™äº›ç¼“å†²åŒºå˜å¾—ä¸å¯è¾¾æ—¶ï¼Œåƒåœ¾æ”¶é›†å™¨å¿…é¡»å›æ”¶å®ƒä»¬ã€‚V8çš„GCæœ‰å¤šä¸ªä»£ï¼ˆå¹´è½»ä»£ã€è€å¹´ä»£ï¼‰ï¼Œå¯¹è±¡æ ¹æ®å¹´é¾„åœ¨ä»£ä¹‹é—´ç§»åŠ¨ã€‚å­˜æ´»å¤šä¸ªGCå‘¨æœŸçš„ç¼“å†²åŒºä¼šè¢«æå‡åˆ°è€å¹´ä»£ï¼Œè¿™æ›´æ˜‚è´µä¸”éš¾ä»¥æ”¶é›†ã€‚é¢‘ç¹çš„ç¼“å†²åŒºåˆ†é…å’Œé‡Šæ”¾ä¼šç ´åGCï¼Œå¯¼è‡´æ›´é•¿çš„æš‚åœæ—¶é—´å’Œæ›´é¢‘ç¹çš„æš‚åœã€‚
- en: Buffer pooling is the technique of reusing buffers to avoid repeated allocations.
    Instead of allocating a new buffer for each chunk, you allocate a pool of buffers
    upfront and reuse them.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºæ± åŒ–æ˜¯ä¸€ç§é‡å¤ä½¿ç”¨ç¼“å†²åŒºä»¥é¿å…é‡å¤åˆ†é…çš„æŠ€æœ¯ã€‚è€Œä¸æ˜¯ä¸ºæ¯ä¸ªæ•°æ®å—åˆ†é…ä¸€ä¸ªæ–°çš„ç¼“å†²åŒºï¼Œä½ é¢„å…ˆåˆ†é…ä¸€ä¸ªç¼“å†²åŒºæ± å¹¶é‡å¤ä½¿ç”¨å®ƒä»¬ã€‚
- en: 'The core idea is simple: allocate N buffers at startup, keep them in a pool,
    and hand them out when needed. When a buffer is no longer needed, return it to
    the pool instead of letting it be garbage collected. This transforms repeated
    allocations into simple pool management - popping and pushing from an array, which
    is orders of magnitude faster than GC operations.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒæ€æƒ³å¾ˆç®€å•ï¼šåœ¨å¯åŠ¨æ—¶åˆ†é…Nä¸ªç¼“å†²åŒºï¼Œå°†å®ƒä»¬ä¿å­˜åœ¨æ± ä¸­ï¼Œå¹¶åœ¨éœ€è¦æ—¶åˆ†å‘ã€‚å½“ä¸€ä¸ªç¼“å†²åŒºä¸å†éœ€è¦æ—¶ï¼Œå°†å…¶è¿”å›åˆ°æ± ä¸­è€Œä¸æ˜¯è®©å®ƒè¢«åƒåœ¾å›æ”¶ã€‚è¿™æŠŠé‡å¤åˆ†é…è½¬æ¢æˆäº†ç®€å•çš„æ± ç®¡ç†â€”â€”ä»æ•°ç»„ä¸­å¼¹å‡ºå’Œæ¨å…¥ï¼Œè¿™æ¯”GCæ“ä½œå¿«å¾—å¤šã€‚
- en: The difficulty is in managing buffer lifetimes. You can only reuse a buffer
    when you're certain it's no longer referenced anywhere else. If you return a buffer
    to the pool while some other code still holds a reference, that code will see
    its data corrupted when the buffer is reused. This is a use-after-free bug, just
    in JavaScript instead of C.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å›°éš¾ä¹‹å¤„åœ¨äºç®¡ç†ç¼“å†²åŒºçš„ç”Ÿå‘½å‘¨æœŸã€‚åªæœ‰å½“ä½ ç¡®å®šç¼“å†²åŒºä¸å†è¢«å…¶ä»–ä»»ä½•åœ°æ–¹å¼•ç”¨æ—¶ï¼Œæ‰èƒ½é‡å¤ä½¿ç”¨ç¼“å†²åŒºã€‚å¦‚æœä½ åœ¨æŸä¸ªå…¶ä»–ä»£ç ä»ç„¶æŒæœ‰å¼•ç”¨æ—¶å°†ç¼“å†²åŒºè¿”å›åˆ°æ± ä¸­ï¼Œé‚£ä¹ˆå½“ç¼“å†²åŒºè¢«é‡å¤ä½¿ç”¨æ—¶ï¼Œè¯¥ä»£ç å°†çœ‹åˆ°å…¶æ•°æ®è¢«æŸåã€‚è¿™æ˜¯ä¸€ä¸ªåœ¨JavaScriptä¸­è€Œä¸æ˜¯Cä¸­çš„ä½¿ç”¨åé‡Šæ”¾çš„æ¼æ´ã€‚
- en: 'The safe approach is to only pool buffers with well-defined, scoped lifetimes.
    For example, buffers used for a single I/O operation: read into buffer, process
    immediately, return to pool. As long as processing doesn''t retain a reference
    to the buffer, it''s safe to reuse.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰å…¨çš„æ–¹æ³•æ˜¯åªå°†å…·æœ‰æ˜ç¡®ã€èŒƒå›´æœ‰é™çš„ç¼“å†²åŒºæ± åŒ–ã€‚ä¾‹å¦‚ï¼Œç”¨äºå•ä¸ªI/Oæ“ä½œçš„ç¼“å†²åŒºï¼šè¯»å–åˆ°ç¼“å†²åŒºä¸­ï¼Œç«‹å³å¤„ç†ï¼Œç„¶åè¿”å›æ± ä¸­ã€‚åªè¦å¤„ç†è¿‡ç¨‹ä¸­ä¸ä¿ç•™å¯¹ç¼“å†²åŒºçš„å¼•ç”¨ï¼Œå°±å¯ä»¥å®‰å…¨åœ°é‡å¤ä½¿ç”¨ã€‚
- en: 'Here''s the simplest form: a single reusable buffer:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æœ€ç®€å•çš„å½¢å¼ï¼šä¸€ä¸ªå¯é‡å¤ä½¿ç”¨çš„ç¼“å†²åŒºï¼š
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You allocate one 64KB buffer. For each incoming chunk, you copy it into the
    reusable buffer, process it, and then reuse the buffer for the next chunk.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åˆ†é…ä¸€ä¸ª64KBçš„ç¼“å†²åŒºã€‚å¯¹äºæ¯ä¸ªä¼ å…¥çš„æ•°æ®å—ï¼Œä½ å°†å…¶å¤åˆ¶åˆ°å¯é‡å¤ä½¿ç”¨çš„ç¼“å†²åŒºä¸­ï¼Œå¤„ç†å®ƒï¼Œç„¶åä¸ºä¸‹ä¸€ä¸ªæ•°æ®å—é‡å¤ä½¿ç”¨è¯¥ç¼“å†²åŒºã€‚
- en: This eliminates per-chunk allocations. You allocate once, reuse many times,
    reducing GC pressure.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¶ˆé™¤äº†æ¯ä¸ªæ•°æ®å—çš„åˆ†é…ã€‚ä½ åªåˆ†é…ä¸€æ¬¡ï¼Œé‡å¤ä½¿ç”¨å¤šæ¬¡ï¼Œå‡å°‘GCå‹åŠ›ã€‚
- en: 'But there''s an important detail: `Buffer.allocUnsafe()`. This allocates a
    buffer without zeroing its memory. Normal `Buffer.alloc()` zeroes the buffer,
    which ensures no leftover data from previous uses. Zeroing costs CPU cycles. If
    you''re about to overwrite the entire buffer anyway, zeroing is wasted work.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æœ‰ä¸€ä¸ªé‡è¦çš„ç»†èŠ‚ï¼š`Buffer.allocUnsafe()`ã€‚è¿™åˆ†é…äº†ä¸€ä¸ªç¼“å†²åŒºè€Œä¸å¯¹å…¶å†…å­˜è¿›è¡Œé›¶åŒ–ã€‚æ­£å¸¸çš„`Buffer.alloc()`ä¼šé›¶åŒ–ç¼“å†²åŒºï¼Œè¿™ç¡®ä¿äº†æ²¡æœ‰æ¥è‡ªå…ˆå‰ä½¿ç”¨çš„é—ç•™æ•°æ®ã€‚é›¶åŒ–ä¼šæ¶ˆè€—CPUå‘¨æœŸã€‚å¦‚æœä½ å³å°†è¦†ç›–æ•´ä¸ªç¼“å†²åŒºï¼Œé›¶åŒ–æ˜¯æµªè´¹çš„å·¥ä½œã€‚
- en: '`Buffer.allocUnsafe()` skips zeroing. The buffer contains whatever data was
    in that memory region before. This is faster but dangerous. If you don''t overwrite
    the entire buffer, you might leak sensitive data from previous operations.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.allocUnsafe()`è·³è¿‡äº†é›¶åŒ–ã€‚ç¼“å†²åŒºåŒ…å«åœ¨è¯¥å†…å­˜åŒºåŸŸä¹‹å‰çš„æ•°æ®ã€‚è¿™æ›´å¿«ä½†æ›´å±é™©ã€‚å¦‚æœä½ æ²¡æœ‰è¦†ç›–æ•´ä¸ªç¼“å†²åŒºï¼Œä½ å¯èƒ½ä¼šä»å…ˆå‰çš„æ“ä½œä¸­æ³„éœ²æ•æ„Ÿæ•°æ®ã€‚'
- en: 'The safe pattern: use `Buffer.allocUnsafe()` when you''re immediately overwriting
    the buffer, and slice the buffer to the actual data length to avoid exposing uninitialized
    memory:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰å…¨æ¨¡å¼ï¼šå½“ä½ ç«‹å³è¦†ç›–ç¼“å†²åŒºæ—¶ä½¿ç”¨`Buffer.allocUnsafe()`ï¼Œå¹¶å°†ç¼“å†²åŒºåˆ‡å‰²åˆ°å®é™…æ•°æ®é•¿åº¦ä»¥é¿å…æš´éœ²æœªåˆå§‹åŒ–çš„å†…å­˜ï¼š
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The slice contains only the bytes you wrote. The rest of the buffer (which might
    have garbage data) isn't exposed.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ‡ç‰‡åªåŒ…å«ä½ å†™å…¥çš„å­—èŠ‚ã€‚ç¼“å†²åŒºçš„å…¶ä½™éƒ¨åˆ†ï¼ˆå¯èƒ½åŒ…å«åƒåœ¾æ•°æ®ï¼‰ä¸ä¼šè¢«æš´éœ²ã€‚
- en: 'For more flexible pooling, maintain a pool of buffers and check them out/in:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´çµæ´»çš„æ± åŒ–ï¼Œç»´æŠ¤ä¸€ä¸ªç¼“å†²åŒºæ± å¹¶æ£€æŸ¥å®ƒä»¬ï¼š
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You allocate a pool of buffers upfront. When you need a buffer, you pop one
    from the pool. When you're done, you return it to the pool. If the pool is empty,
    you allocate a new buffer (falling back to normal allocation). If the pool is
    too large (to avoid hoarding memory), you discard returned buffers instead of
    pooling them.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ é¢„å…ˆåˆ†é…ä¸€ä¸ªç¼“å†²åŒºæ± ã€‚å½“ä½ éœ€è¦ç¼“å†²åŒºæ—¶ï¼Œä»æ± ä¸­å–å‡ºä¸€ä¸ªã€‚å½“ä½ å®Œæˆæ—¶ï¼Œå°†å…¶è¿”å›åˆ°æ± ä¸­ã€‚å¦‚æœæ± ä¸ºç©ºï¼Œåˆ™åˆ†é…ä¸€ä¸ªæ–°çš„ç¼“å†²åŒºï¼ˆå›é€€åˆ°æ­£å¸¸åˆ†é…ï¼‰ã€‚å¦‚æœæ± å¤ªå¤§ï¼ˆä¸ºäº†é¿å…å†…å­˜å›¤ç§¯ï¼‰ï¼Œåˆ™ä¸¢å¼ƒè¿”å›çš„ç¼“å†²åŒºè€Œä¸æ˜¯å°†å®ƒä»¬æ± åŒ–ã€‚
- en: '**Security warning:** This basic pool doesn''t zero buffers on release. If
    your application handles sensitive data (passwords, tokens, PII), previous contents
    remain in memory and could leak to subsequent users of the same buffer. For security-sensitive
    applications, either zero the buffer before releasing (`buffer.fill(0)`) or don''t
    pool buffers that held sensitive data.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®‰å…¨è­¦å‘Š**ï¼šè¿™ä¸ªåŸºæœ¬æ± åœ¨é‡Šæ”¾æ—¶ä¸ä¼šå°†ç¼“å†²åŒºæ¸…é›¶ã€‚å¦‚æœä½ çš„åº”ç”¨ç¨‹åºå¤„ç†æ•æ„Ÿæ•°æ®ï¼ˆå¯†ç ã€ä»¤ç‰Œã€PIIï¼‰ï¼Œåˆ™ä¹‹å‰çš„å†…å®¹å°†ä¿ç•™åœ¨å†…å­˜ä¸­ï¼Œå¯èƒ½ä¼šæ³„éœ²ç»™åç»­ä½¿ç”¨ç›¸åŒç¼“å†²åŒºçš„ç”¨æˆ·ã€‚å¯¹äºå®‰å…¨æ•æ„Ÿçš„åº”ç”¨ç¨‹åºï¼Œåœ¨é‡Šæ”¾å‰å°†ç¼“å†²åŒºæ¸…é›¶ï¼ˆ`buffer.fill(0)`ï¼‰æˆ–è€…ä¸è¦å°†åŒ…å«æ•æ„Ÿæ•°æ®çš„ç¼“å†²åŒºæ± åŒ–ã€‚'
- en: This cuts allocations way down. Instead of allocating per chunk, you allocate
    a small pool upfront and reuse those buffers thousands of times.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¤§å¤§å‡å°‘äº†åˆ†é…ã€‚ä½ ä¸å†æŒ‰å—åˆ†é…ï¼Œè€Œæ˜¯é¢„å…ˆåˆ†é…ä¸€ä¸ªå°æ± ï¼Œå¹¶é‡å¤ä½¿ç”¨è¿™äº›ç¼“å†²åŒºæ•°åƒæ¬¡ã€‚
- en: 'Use it in a readable stream:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯è¯»çš„æµä¸­ä½¿ç”¨å®ƒï¼š
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You acquire a buffer, use it for reading, copy the relevant portion with `Buffer.from()`,
    push that copy, and release the original buffer back to the pool. The buffer is
    reused for the next read.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è·å–ä¸€ä¸ªç¼“å†²åŒºï¼Œç”¨å®ƒè¿›è¡Œè¯»å–ï¼Œä½¿ç”¨ `Buffer.from()` å¤åˆ¶ç›¸å…³éƒ¨åˆ†ï¼Œæ¨é€é‚£ä¸ªå‰¯æœ¬ï¼Œå¹¶å°†åŸå§‹ç¼“å†²åŒºé‡Šæ”¾å›æ± ã€‚è¯¥ç¼“å†²åŒºå°†è¢«ç”¨äºä¸‹ä¸€æ¬¡è¯»å–ã€‚
- en: The explicit copy with `Buffer.from()` is to be noted here. `subarray()` creates
    a view that shares memory with the original buffer - it does NOT copy data. If
    you pushed the subarray directly and released the buffer, downstream consumers
    might see corrupted data when the pool reuses that buffer for another read. Always
    copy before releasing a pooled buffer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œè¦æ³¨æ„ä½¿ç”¨ `Buffer.from()` çš„æ˜¾å¼å¤åˆ¶ã€‚`subarray()` åˆ›å»ºäº†ä¸€ä¸ªä¸åŸå§‹ç¼“å†²åŒºå…±äº«å†…å­˜çš„è§†å›¾ - å®ƒä¸ä¼šå¤åˆ¶æ•°æ®ã€‚å¦‚æœä½ ç›´æ¥æ¨é€å­æ•°ç»„å¹¶é‡Šæ”¾ç¼“å†²åŒºï¼Œå½“æ± é‡æ–°ä½¿ç”¨è¯¥ç¼“å†²åŒºè¿›è¡Œå¦ä¸€ä¸ªè¯»å–æ—¶ï¼Œä¸‹æ¸¸æ¶ˆè´¹è€…å¯èƒ½ä¼šçœ‹åˆ°æŸåçš„æ•°æ®ã€‚åœ¨é‡Šæ”¾æ± åŒ–ç¼“å†²åŒºä¹‹å‰å§‹ç»ˆè¿›è¡Œå¤åˆ¶ã€‚
- en: You're still controlling where allocations happen. Instead of allocating a buffer
    in `_read()` each time (which V8 has to manage), you're allocating smaller copies
    of just the bytes you read. The pool buffers are large and reused, reducing pressure
    on the allocator.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä»ç„¶æ§åˆ¶ç€åˆ†é…å‘ç”Ÿçš„ä½ç½®ã€‚ä½ ä¸å†éœ€è¦åœ¨æ¯æ¬¡ `_read()` ä¸­åˆ†é…ä¸€ä¸ªç¼“å†²åŒºï¼ˆV8 å¿…é¡»ç®¡ç†è¿™äº›ç¼“å†²åŒºï¼‰ï¼Œè€Œæ˜¯åˆ†é…ä½ è¯»å–çš„å­—èŠ‚çš„å°å‰¯æœ¬ã€‚æ± åŒ–ç¼“å†²åŒºå¾ˆå¤§ä¸”å¯é‡å¤ä½¿ç”¨ï¼Œä»è€Œå‡è½»äº†åˆ†é…å™¨çš„å‹åŠ›ã€‚
- en: For truly zero-copy pooling, you'd need to pass pool buffers directly downstream
    and ensure they're released after consumption. This means coordinating with the
    consumer, which gets messy. In practice, pooling is most useful when you control
    both ends of the data flow (like a custom protocol implementation).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºçœŸæ­£çš„é›¶å¤åˆ¶æ± åŒ–ï¼Œä½ éœ€è¦ç›´æ¥å°†æ± ç¼“å†²åŒºä¼ é€’ç»™ä¸‹æ¸¸ï¼Œå¹¶ç¡®ä¿åœ¨æ¶ˆè´¹åé‡Šæ”¾å®ƒä»¬ã€‚è¿™æ„å‘³ç€éœ€è¦ä¸æ¶ˆè´¹è€…åè°ƒï¼Œè¿™ä¼šå˜å¾—å¾ˆå¤æ‚ã€‚åœ¨å®è·µä¸­ï¼Œå½“ä½ å¯ä»¥æ§åˆ¶æ•°æ®æµçš„ä¸¤ä¸ªç«¯ç‚¹æ—¶ï¼ˆå¦‚è‡ªå®šä¹‰åè®®å®ç°ï¼‰ï¼Œæ± åŒ–æœ€æœ‰ç”¨ã€‚
- en: Buffer pooling reduces allocations, which reduces GC pressure. Use `Buffer.allocUnsafe()`
    for buffers you're about to overwrite, and be careful with slicing to avoid leaking
    uninitialized data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºæ± åŒ–å‡å°‘äº†åˆ†é…ï¼Œä»è€Œå‡å°‘äº† GC å‹åŠ›ã€‚å¯¹äºå³å°†è¦†ç›–çš„ç¼“å†²åŒºä½¿ç”¨ `Buffer.allocUnsafe()`ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨åˆ‡ç‰‡æ—¶è¦å°å¿ƒï¼Œä»¥é¿å…æ³„éœ²æœªåˆå§‹åŒ–çš„æ•°æ®ã€‚
- en: Batching Writes for Efficiency
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰¹é‡å†™å…¥ä»¥æé«˜æ•ˆç‡
- en: We touched on cork/uncork earlier for batching writes. Now we'll examine when
    and how to use it strategically.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰æåˆ°äº†ä½¿ç”¨ cork/uncork æ¥æ‰¹é‡å†™å…¥ã€‚ç°åœ¨æˆ‘ä»¬å°†æ¢è®¨ä½•æ—¶ä»¥åŠå¦‚ä½•ç­–ç•¥æ€§åœ°ä½¿ç”¨å®ƒã€‚
- en: Corking a writable stream tells it to buffer writes instead of flushing them
    immediately. Uncorking flushes the buffered writes, ideally in a single batched
    operation (via `_writev()` if implemented).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Corking a writable stream tells it to buffer writes instead of flushing them
    immediately. Uncorking flushes the buffered writes, ideally in a single batched
    operation (via `_writev()` if implemented).
- en: 'The benefit: reducing the number of write operations. The cost: increased latency
    (data sits in the buffer until uncorked).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜ç‚¹ï¼šå‡å°‘å†™æ“ä½œçš„æ•°é‡ã€‚ä»£ä»·ï¼šå¢åŠ å»¶è¿Ÿï¼ˆæ•°æ®åœ¨ç¼“å†²åŒºä¸­ç­‰å¾…ç›´åˆ° uncorkï¼‰ã€‚
- en: 'The key is knowing when to cork. If you''re about to write a burst of small
    chunks, cork before the burst and uncork after:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®åœ¨äºçŸ¥é“ä½•æ—¶ corkã€‚å¦‚æœä½ å³å°†å†™å…¥ä¸€ç³»åˆ—å°å—ï¼Œåˆ™åœ¨çˆ†å‘ä¹‹å‰ corkï¼Œåœ¨çˆ†å‘ä¹‹å uncorkï¼š
- en: '[PRE18]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: All writes buffer, then flush together. If `_writev()` is implemented, they're
    written in one syscall.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰å†™å…¥éƒ½ä¼šç¼“å†²ï¼Œç„¶åä¸€èµ·åˆ·æ–°ã€‚å¦‚æœå®ç°äº† `_writev()`ï¼Œå®ƒä»¬ä¼šåœ¨ä¸€ä¸ªç³»ç»Ÿè°ƒç”¨ä¸­å†™å…¥ã€‚
- en: 'But there''s a problem with this pattern: if `processItem()` throws an exception,
    `uncork()` never gets called and the stream stays corked. Always pair cork with
    uncork in a try/finally:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™ç§æ¨¡å¼å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šå¦‚æœ `processItem()` æŠ›å‡ºå¼‚å¸¸ï¼Œ`uncork()` æ°¸è¿œä¸ä¼šè¢«è°ƒç”¨ï¼Œæµä¿æŒ corked çŠ¶æ€ã€‚å§‹ç»ˆåœ¨ try/finally
    ä¸­æˆå¯¹ä½¿ç”¨ cork å’Œ uncorkï¼š
- en: '[PRE19]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `finally` block ensures uncork even if an error occurs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`finally` å—ç¡®ä¿å³ä½¿å‘ç”Ÿé”™è¯¯ä¹Ÿä¼š uncorkã€‚'
- en: 'One subtlety: Node allows multiple cork calls. Each cork increments an internal
    counter. Each uncork decrements it. The stream only flushes when the counter reaches
    zero:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç»†å¾®ä¹‹å¤„ï¼šNode å…è®¸å¤šæ¬¡è°ƒç”¨ corkã€‚æ¯æ¬¡ cork éƒ½ä¼šå¢åŠ ä¸€ä¸ªå†…éƒ¨è®¡æ•°å™¨ã€‚æ¯æ¬¡ uncork éƒ½ä¼šå‡å°‘å®ƒã€‚åªæœ‰å½“è®¡æ•°å™¨è¾¾åˆ°é›¶æ—¶ï¼Œæµæ‰ä¼šåˆ·æ–°ï¼š
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This lets you nest cork regions. The innermost cork/uncork doesn't trigger a
    flush; only the outermost uncork does.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è®©ä½ å¯ä»¥åµŒå¥—å°å µåŒºåŸŸã€‚æœ€å†…å±‚çš„å°å µ/è§£å°ä¸ä¼šè§¦å‘åˆ·æ–°ï¼›åªæœ‰æœ€å¤–å±‚çš„è§£å°æ‰ä¼šã€‚
- en: 'This is useful for complex control flow where multiple functions might cork/uncork:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹äºå¤æ‚çš„æ§åˆ¶æµå¾ˆæœ‰ç”¨ï¼Œå…¶ä¸­å¤šä¸ªå‡½æ•°å¯èƒ½ä¼šå°å µ/è§£å°ï¼š
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Both `writeHeader` and `writeBody` cork/uncork, but because they're nested within
    an outer cork, their uncorks don't flush. The final uncork flushes everything.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`writeHeader` å’Œ `writeBody` éƒ½ä¼šå°å µ/è§£å°ï¼Œä½†å®ƒä»¬åµŒå¥—åœ¨å¤–éƒ¨å°å µä¸­ï¼Œå› æ­¤å®ƒä»¬çš„è§£å°ä¸ä¼šåˆ·æ–°ã€‚æœ€åçš„è§£å°åˆ·æ–°ä¸€åˆ‡ã€‚'
- en: 'When not to cork: if writes are already large (megabytes), corking doesn''t
    help. The overhead of buffering might exceed any batching benefit. Cork is most
    useful for many small writes.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åº”è¯¥å°å µçš„æƒ…å†µï¼šå¦‚æœå†™å…¥å·²ç»å¾ˆå¤§ï¼ˆå…†å­—èŠ‚ï¼‰ï¼Œå°å µæ²¡æœ‰å¸®åŠ©ã€‚ç¼“å†²çš„å¼€é”€å¯èƒ½è¶…è¿‡ä»»ä½•æ‰¹å¤„ç†çš„å¥½å¤„ã€‚å°å µå¯¹äºè®¸å¤šå°å†™å…¥æœ€æœ‰ç”¨ã€‚
- en: Also, don't cork indefinitely. If you're processing a long-running stream and
    you cork at the start, data buffers until the end. This consumes memory and increases
    latency. Cork only around bursts of writes, not for the entire stream lifetime.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä¸è¦æ— é™æœŸåœ°å°å µã€‚å¦‚æœä½ æ­£åœ¨å¤„ç†é•¿æ—¶é—´è¿è¡Œçš„æµå¹¶åœ¨å¼€å§‹æ—¶å°å µï¼Œæ•°æ®ç¼“å†²åŒºä¼šä¸€ç›´ç§¯ç´¯åˆ°æœ«å°¾ã€‚è¿™ä¼šæ¶ˆè€—å†…å­˜å¹¶å¢åŠ å»¶è¿Ÿã€‚åªåœ¨å†™å…¥çˆ†å‘æ—¶å°å µï¼Œè€Œä¸æ˜¯æ•´ä¸ªæµçš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸã€‚
- en: 'A pattern for adaptive corking: cork when writes are frequent, uncork when
    they slow down:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”å°å µçš„æ¨¡å¼ï¼šå½“å†™å…¥é¢‘ç¹æ—¶å°å µï¼Œå½“å†™å…¥å˜æ…¢æ—¶è§£å°ï¼š
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If writes happen within 10ms of each other, cork. If 10ms pass without writes,
    uncork. This batches rapid writes while flushing promptly when writes slow down.
    Note that we clear and reset the timer on each write - without this, rapid writes
    would accumulate many pending timers.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå†™å…¥åœ¨å½¼æ­¤ä¹‹é—´10æ¯«ç§’å†…å‘ç”Ÿï¼Œåˆ™å°å µã€‚å¦‚æœ10æ¯«ç§’å†…æ²¡æœ‰å†™å…¥ï¼Œåˆ™è§£å°ã€‚è¿™ä¼šå°†å¿«é€Ÿå†™å…¥æ‰¹å¤„ç†åœ¨ä¸€èµ·ï¼Œå½“å†™å…¥å˜æ…¢æ—¶åŠæ—¶åˆ·æ–°ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨æ¯æ¬¡å†™å…¥æ—¶éƒ½ä¼šæ¸…é™¤å¹¶é‡ç½®è®¡æ—¶å™¨â€”â€”å¦‚æœæ²¡æœ‰è¿™æ ·åšï¼Œå¿«é€Ÿå†™å…¥ä¼šç§¯ç´¯è®¸å¤šæŒ‚èµ·çš„è®¡æ—¶å™¨ã€‚
- en: This is a heuristic. The right threshold depends on your workload. Measure and
    tune.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¯å‘å¼æ–¹æ³•ã€‚æ­£ç¡®çš„é˜ˆå€¼å–å†³äºä½ çš„å·¥ä½œè´Ÿè½½ã€‚æµ‹é‡å’Œè°ƒæ•´ã€‚
- en: Avoiding String Concatenation Overhead in Streams
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¿å…åœ¨æµä¸­å­—ç¬¦ä¸²è¿æ¥çš„å¼€é”€
- en: String concatenation in JavaScript can be memory-inefficient when accumulating
    large amounts of text. Modern engines like V8 optimize concatenation using "cons
    strings" (also called ropes) - instead of immediately copying, they create a tree
    structure pointing to the original strings. The actual copying happens later when
    the string is read or flattened.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç´¯ç§¯å¤§é‡æ–‡æœ¬æ—¶ï¼ŒJavaScriptä¸­çš„å­—ç¬¦ä¸²è¿æ¥å¯èƒ½ä¼šé€ æˆå†…å­˜æ•ˆç‡ä½ä¸‹ã€‚ç°ä»£å¼•æ“å¦‚V8ä½¿ç”¨â€œConså­—ç¬¦ä¸²â€ï¼ˆä¹Ÿç§°ä¸ºropeï¼‰æ¥ä¼˜åŒ–è¿æ¥â€”â€”è€Œä¸æ˜¯ç«‹å³å¤åˆ¶ï¼Œå®ƒä»¬åˆ›å»ºä¸€ä¸ªæŒ‡å‘åŸå§‹å­—ç¬¦ä¸²çš„æ ‘ç»“æ„ã€‚å®é™…çš„å¤åˆ¶å‘ç”Ÿåœ¨å­—ç¬¦ä¸²è¢«è¯»å–æˆ–æ‰å¹³åŒ–æ—¶ã€‚
- en: But this optimization has limits. Cons strings add memory overhead for the tree
    structure, and flattening happens unpredictably - when you access a character,
    search the string, or pass it to native code. In stream scenarios where you're
    accumulating many chunks, these deferred copies still happen eventually, and the
    memory overhead of maintaining deep cons string trees can cause issues.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™ç§ä¼˜åŒ–æœ‰å±€é™æ€§ã€‚Conså­—ç¬¦ä¸²ä¸ºæ ‘ç»“æ„å¢åŠ äº†å†…å­˜å¼€é”€ï¼Œå¹¶ä¸”æ‰å¹³åŒ–å‘ç”Ÿå¾—ä¸å¯é¢„æµ‹â€”â€”å½“ä½ è®¿é—®ä¸€ä¸ªå­—ç¬¦ã€æœç´¢å­—ç¬¦ä¸²æˆ–å°†å®ƒä¼ é€’ç»™åŸç”Ÿä»£ç æ—¶ã€‚åœ¨æµåœºæ™¯ä¸­ï¼Œå½“ä½ ç´¯ç§¯è®¸å¤šå—æ—¶ï¼Œè¿™äº›å»¶è¿Ÿçš„å¤åˆ¶æœ€ç»ˆè¿˜æ˜¯ä¼šå‘ç”Ÿï¼Œè€Œç»´æŠ¤æ·±å±‚Conså­—ç¬¦ä¸²æ ‘çš„å†…å­˜å¼€é”€å¯èƒ½ä¼šå¯¼è‡´é—®é¢˜ã€‚
- en: 'In streams, the problematic pattern looks like this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æµä¸­ï¼Œæœ‰é—®é¢˜çš„æ¨¡å¼çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Each `+=` either creates a cons string (deferred cost) or triggers flattening
    of previous cons strings (immediate cost). For a large file with many chunks,
    you end up with either a deep tree that eventually flattens expensively, or repeated
    flattening operations that approach O(N^2) behavior.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ª `+=` è¦ä¹ˆåˆ›å»ºä¸€ä¸ªConså­—ç¬¦ä¸²ï¼ˆå»¶è¿Ÿæˆæœ¬ï¼‰è¦ä¹ˆè§¦å‘ä¹‹å‰Conså­—ç¬¦ä¸²çš„æ‰å¹³åŒ–ï¼ˆå³æ—¶æˆæœ¬ï¼‰ã€‚å¯¹äºåŒ…å«è®¸å¤šå—çš„æ–‡ä»¶ï¼Œä½ æœ€ç»ˆä¼šå¾—åˆ°ä¸€ä¸ªæ·±åº¦æ ‘ï¼Œæœ€ç»ˆä»¥æ˜‚è´µçš„æˆæœ¬æ‰å¹³åŒ–ï¼Œæˆ–è€…æ¥è¿‘O(N^2)è¡Œä¸ºçš„é‡å¤æ‰å¹³åŒ–æ“ä½œã€‚
- en: 'The fix: use an array to collect chunks, then join once:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿®å¤æ–¹æ³•ï¼šä½¿ç”¨æ•°ç»„æ”¶é›†å—ï¼Œç„¶åä¸€æ¬¡æ€§è¿æ¥ï¼š
- en: '[PRE24]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Array.push()` is cheap. `Array.join()` allocates once and copies all strings
    in one pass. Linear time instead of quadratic.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`Array.push()` æˆæœ¬è¾ƒä½ã€‚`Array.join()` ä¸€æ¬¡åˆ†é…å¹¶å¤åˆ¶æ‰€æœ‰å­—ç¬¦ä¸²ã€‚çº¿æ€§æ—¶é—´è€Œä¸æ˜¯äºŒæ¬¡æ–¹æ—¶é—´ã€‚'
- en: 'Even better, if you''re working with buffers, collect buffers and concatenate
    at the end:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¥½çš„æ˜¯ï¼Œå¦‚æœä½ æ­£åœ¨å¤„ç†ç¼“å†²åŒºï¼Œæ”¶é›†ç¼“å†²åŒºå¹¶åœ¨æœ€åè¿æ¥ï¼š
- en: '[PRE25]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`Buffer.concat()` allocates once and copies all buffers. Avoid `toString()`
    until you actually need a string.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`Buffer.concat()` ä¸€æ¬¡åˆ†é…å¹¶å¤åˆ¶æ‰€æœ‰ç¼“å†²åŒºã€‚é¿å…åœ¨å®é™…ä¸Šéœ€è¦å­—ç¬¦ä¸²ä¹‹å‰ä½¿ç”¨ `toString()`ã€‚'
- en: 'If you need to process data incrementally (not accumulate the entire stream),
    process chunks directly:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ éœ€è¦å¢é‡å¤„ç†æ•°æ®ï¼ˆè€Œä¸æ˜¯ç´¯ç§¯æ•´ä¸ªæµï¼‰ï¼Œç›´æ¥å¤„ç†å—ï¼š
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: No concatenation, no accumulation, just per-chunk processing.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰è¿æ¥ï¼Œæ²¡æœ‰ç´¯ç§¯ï¼Œåªæ˜¯æŒ‰å—å¤„ç†ã€‚
- en: 'Another anti-pattern: converting buffers to strings for simple operations:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªåæ¨¡å¼ï¼šå°†ç¼“å†²åŒºè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œç®€å•æ“ä½œï¼š
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Buffers have methods for searching:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºæœ‰ç”¨äºæœç´¢çš„æ–¹æ³•ï¼š
- en: '[PRE28]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: No conversion, no string allocation, just a buffer scan.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰è½¬æ¢ï¼Œæ²¡æœ‰å­—ç¬¦ä¸²åˆ†é…ï¼Œåªæ˜¯ç¼“å†²åŒºæ‰«æã€‚
- en: Strings are immutable, and concatenation creates new strings. For streams, minimize
    conversions and concatenations. Work with buffers when possible, and collect chunks
    in arrays when accumulation is necessary.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: å­—ç¬¦ä¸²æ˜¯ä¸å¯å˜çš„ï¼Œè¿æ¥æ“ä½œä¼šåˆ›å»ºæ–°çš„å­—ç¬¦ä¸²ã€‚å¯¹äºæµï¼Œå°½å¯èƒ½å‡å°‘è½¬æ¢å’Œè¿æ¥æ“ä½œã€‚å½“éœ€è¦ç´¯ç§¯æ—¶ï¼Œå°½å¯èƒ½ä½¿ç”¨ç¼“å†²åŒºï¼Œå¹¶åœ¨æ•°ç»„ä¸­æ”¶é›†å—ã€‚
- en: The stream.read(0) Hack
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµçš„ `read(0)` æ¼æ´
- en: 'An obscure but occasionally useful trick: calling `read(0)` on a readable stream.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¸ä¸ºäººçŸ¥ä½†å¶å°”æœ‰ç”¨çš„æŠ€å·§ï¼šåœ¨å¯è¯»æµä¸Šè°ƒç”¨ `read(0)`ã€‚
- en: Normally, `read(size)` pulls `size` bytes from the stream's internal buffer.
    But `read(0)` doesn't pull any data. Instead, it triggers the stream's internal
    buffer check, potentially calling `_read()` if certain conditions are met.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œ`read(size)` ä»æµçš„å†…éƒ¨ç¼“å†²åŒºä¸­æ‹‰å– `size` å­—èŠ‚ã€‚ä½† `read(0)` ä¸ä¼šæ‹‰å–ä»»ä½•æ•°æ®ã€‚ç›¸åï¼Œå®ƒè§¦å‘æµçš„å†…éƒ¨ç¼“å†²åŒºæ£€æŸ¥ï¼Œå¦‚æœæ»¡è¶³æŸäº›æ¡ä»¶ï¼Œå¯èƒ½ä¼šè°ƒç”¨
    `_read()`ã€‚
- en: '`_read()` will only be called when **both** conditions are true:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰å½“ **ä¸¤ä¸ª** æ¡ä»¶éƒ½æ»¡è¶³æ—¶ï¼Œ`_read()` æ‰ä¼šè¢«è°ƒç”¨ï¼š
- en: The internal buffer is below `highWaterMark`
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å†…éƒ¨ç¼“å†²åŒºä½äº `highWaterMark`
- en: The stream is not currently in the middle of a `_read()` call
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æµå½“å‰ä¸åœ¨ `_read()` è°ƒç”¨è¿‡ç¨‹ä¸­
- en: 'This is useful when you''re using a stream in paused mode and you want to initiate
    buffer filling:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨ä½ åœ¨æš‚åœæ¨¡å¼ä¸‹ä½¿ç”¨æµå¹¶å¸Œæœ›å¯åŠ¨ç¼“å†²åŒºå¡«å……æ—¶å¾ˆæœ‰ç”¨ï¼š
- en: '[PRE29]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Calling `read(0)` tells the stream "check if you need to read more data," without
    actually consuming anything. If the buffer is low and no read is in progress,
    `_read()` is called to start filling the buffer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ `read(0)` å‘Šè¯‰æµâ€œæ£€æŸ¥æ˜¯å¦éœ€è¦è¯»å–æ›´å¤šæ•°æ®â€ï¼Œä½†å®é™…ä¸Šå¹¶æ²¡æœ‰æ¶ˆè€—ä»»ä½•å†…å®¹ã€‚å¦‚æœç¼“å†²åŒºä½ä¸”æ²¡æœ‰è¿›è¡Œè¯»å–æ“ä½œï¼Œåˆ™ä¼šè°ƒç”¨ `_read()`
    ä»¥å¼€å§‹å¡«å……ç¼“å†²åŒºã€‚
- en: 'One important caveat: `_read()` implementations are almost always asynchronous.
    They call `this.push()` later, after I/O completes. So `read(0)` merely *initiates*
    a read request; it doesn''t synchronously fill the buffer. If you need data to
    be available, you must wait for the async operation to complete:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé‡è¦çš„æ³¨æ„äº‹é¡¹ï¼š`_read()` å®ç°å‡ ä¹æ€»æ˜¯å¼‚æ­¥çš„ã€‚å®ƒä»¬åœ¨ I/O å®Œæˆåç¨åè°ƒç”¨ `this.push()`ã€‚æ‰€ä»¥ `read(0)` ä»…ä»…æ˜¯
    *å¯åŠ¨* ä¸€ä¸ªè¯»å–è¯·æ±‚ï¼›å®ƒä¸ä¼šåŒæ­¥å¡«å……ç¼“å†²åŒºã€‚å¦‚æœä½ éœ€è¦æ•°æ®å¯ç”¨ï¼Œä½ å¿…é¡»ç­‰å¾…å¼‚æ­¥æ“ä½œå®Œæˆï¼š
- en: '[PRE30]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This is a niche optimization. Most code doesn't need it. But if you're writing
    low-level stream plumbing and you need fine-grained control over when `_read()`
    is called, `read(0)` is the tool.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå°ä¼—ä¼˜åŒ–ã€‚å¤§å¤šæ•°ä»£ç ä¸éœ€è¦å®ƒã€‚ä½†å¦‚æœä½ æ­£åœ¨ç¼–å†™ä½çº§æµç®¡é“ï¼Œå¹¶ä¸”éœ€è¦ç²¾ç»†æ§åˆ¶ä½•æ—¶è°ƒç”¨ `_read()`ï¼Œ`read(0)` å°±æ˜¯è¿™ä¸ªå·¥å…·ã€‚
- en: 'For debugging: if `read(0)` doesn''t trigger `_read()`, it doesn''t necessarily
    indicate a bug. It might mean the buffer is already at or above `highWaterMark`,
    or that a previous `_read()` is still in progress. Check `readable.readableLength`
    against the stream''s `highWaterMark` to understand the current state.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè°ƒè¯•ï¼šå¦‚æœ `read(0)` æ²¡æœ‰è§¦å‘ `_read()`ï¼Œè¿™å¹¶ä¸ä¸€å®šæ„å‘³ç€æœ‰é”™è¯¯ã€‚å®ƒå¯èƒ½æ„å‘³ç€ç¼“å†²åŒºå·²ç»è¾¾åˆ°æˆ–è¶…è¿‡ `highWaterMark`ï¼Œæˆ–è€…ä¹‹å‰çš„ä¸€ä¸ª
    `_read()` æ“ä½œä»åœ¨è¿›è¡Œä¸­ã€‚å°† `readable.readableLength` ä¸æµçš„ `highWaterMark` è¿›è¡Œæ¯”è¾ƒï¼Œä»¥äº†è§£å½“å‰çŠ¶æ€ã€‚
- en: Treat this as an advanced technique. For normal stream usage, you won't touch
    `read(0)`. But it's part of the stream API, and knowing it exists can help when
    you're deep in the internals.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤è§†ä¸ºä¸€ç§é«˜çº§æŠ€æœ¯ã€‚å¯¹äºæ­£å¸¸çš„æµä½¿ç”¨ï¼Œä½ ä¸ä¼šæ¥è§¦åˆ° `read(0)`ã€‚ä½†å®ƒå±äºæµ API çš„ä¸€éƒ¨åˆ†ï¼Œäº†è§£å®ƒçš„å­˜åœ¨å¯ä»¥åœ¨æ·±å…¥äº†è§£å†…éƒ¨æ—¶æœ‰æ‰€å¸®åŠ©ã€‚
- en: Avoiding Intermediate Transforms
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¿å…ä¸­é—´è½¬æ¢
- en: Each transform in a pipeline adds overhead. The data passes through the transform's
    buffering, the `_transform()` method is called, and the output is buffered again.
    For complex pipelines with many stages, this overhead compounds.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡é“ä¸­çš„æ¯ä¸ªè½¬æ¢éƒ½ä¼šå¢åŠ å¼€é”€ã€‚æ•°æ®é€šè¿‡è½¬æ¢çš„ç¼“å†²åŒºï¼Œè°ƒç”¨ `_transform()` æ–¹æ³•ï¼Œç„¶åå†æ¬¡ç¼“å†²è¾“å‡ºã€‚å¯¹äºå…·æœ‰è®¸å¤šé˜¶æ®µçš„å¤æ‚ç®¡é“ï¼Œè¿™ç§å¼€é”€ä¼šç´¯ç§¯ã€‚
- en: 'If you can combine transformations, you reduce stages and improve performance:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯ä»¥å°†è½¬æ¢ç»„åˆèµ·æ¥ï¼Œä½ å°±å¯ä»¥å‡å°‘é˜¶æ®µå¹¶æé«˜æ€§èƒ½ï¼š
- en: '[PRE31]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The combined transform does the same work in one pass. Less buffering, fewer
    method calls, less overhead.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ç»„åˆè½¬æ¢åœ¨ä¸€æ¬¡éå†ä¸­å®Œæˆç›¸åŒçš„å·¥ä½œã€‚å‡å°‘ç¼“å†²ï¼Œå‡å°‘æ–¹æ³•è°ƒç”¨ï¼Œå‡å°‘å¼€é”€ã€‚
- en: 'The trade-off is modularity. Separate transforms are easier to test and reuse.
    A combined transform is a monolith. Choose based on your priorities: performance
    vs. maintainability.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ä»·æ˜¯æ¨¡å—åŒ–ã€‚ç‹¬ç«‹çš„è½¬æ¢æ›´å®¹æ˜“æµ‹è¯•å’Œé‡ç”¨ã€‚ç»„åˆè½¬æ¢æ˜¯ä¸€ä¸ªå•ä½“ã€‚æ ¹æ®ä½ çš„ä¼˜å…ˆçº§é€‰æ‹©ï¼šæ€§èƒ½ä¸å¯ç»´æŠ¤æ€§ã€‚
- en: If performance matters and you have a pipeline with many simple transforms,
    consider combining them. If maintainability matters and transforms are reused
    across pipelines, keep them separate.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ€§èƒ½å¾ˆé‡è¦ï¼Œå¹¶ä¸”ä½ æœ‰è®¸å¤šç®€å•è½¬æ¢çš„ç®¡é“ï¼Œè€ƒè™‘å°†å®ƒä»¬ç»„åˆã€‚å¦‚æœå¯ç»´æŠ¤æ€§å¾ˆé‡è¦ï¼Œå¹¶ä¸”è½¬æ¢åœ¨å¤šä¸ªç®¡é“ä¸­é‡å¤ä½¿ç”¨ï¼Œåˆ™ä¿æŒå®ƒä»¬åˆ†å¼€ã€‚
- en: 'One pattern: lazy combination. Start with separate transforms for clarity.
    If profiling shows pipeline overhead is high, combine the hot path:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æ¨¡å¼ï¼šæ‡’ç»„åˆã€‚ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œå¼€å§‹æ—¶ä½¿ç”¨å•ç‹¬çš„è½¬æ¢ã€‚å¦‚æœåˆ†ææ˜¾ç¤ºç®¡é“å¼€é”€å¾ˆé«˜ï¼Œåˆ™ç»„åˆçƒ­ç‚¹è·¯å¾„ï¼š
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You've encapsulated the combined logic, and the pipeline has one fewer stage.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å·²ç»å°è£…äº†ç»„åˆé€»è¾‘ï¼Œç®¡é“å°‘äº†ä¸€ä¸ªé˜¶æ®µã€‚
- en: 'Another optimization: eliminate no-op transforms. Sometimes transforms pass
    data unchanged under certain conditions:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§ä¼˜åŒ–ï¼šæ¶ˆé™¤æ— æ“ä½œè½¬æ¢ã€‚æœ‰æ—¶åœ¨ç‰¹å®šæ¡ä»¶ä¸‹è½¬æ¢ä¼šä¸æ”¹å˜æ•°æ®ï¼š
- en: '[PRE33]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: If the transform isn't needed, skip it entirely. Don't include a passthrough
    transform "just in case."
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè½¬æ¢ä¸éœ€è¦ï¼Œå°±å®Œå…¨è·³è¿‡å®ƒã€‚ä¸è¦åŒ…å«ä¸€ä¸ªâ€œä»¥é˜²ä¸‡ä¸€â€çš„æ—è·¯è½¬æ¢ã€‚
- en: Every pipeline stage has a cost. Minimize stages when performance matters. Combine
    transforms judiciously to balance speed and code clarity.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªç®¡é“é˜¶æ®µéƒ½æœ‰æˆæœ¬ã€‚å½“æ€§èƒ½å¾ˆé‡è¦æ—¶ï¼Œå°½é‡å‡å°‘é˜¶æ®µã€‚è°¨æ…åœ°ç»„åˆè½¬æ¢ä»¥å¹³è¡¡é€Ÿåº¦å’Œä»£ç æ¸…æ™°åº¦ã€‚
- en: Optimizing readable.readableFlowing for Manual Control
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–readable.readableFlowingä»¥è¿›è¡Œæ‰‹åŠ¨æ§åˆ¶
- en: The `readable.readableFlowing` property tells you whether the stream is in flowing
    mode (true), paused mode (false), or hasn't been set yet (null).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`readable.readableFlowing`å±æ€§å‘Šè¯‰ä½ æµæ˜¯å¦å¤„äºæµåŠ¨æ¨¡å¼ï¼ˆtrueï¼‰ã€æš‚åœæ¨¡å¼ï¼ˆfalseï¼‰æˆ–å°šæœªè®¾ç½®ï¼ˆnullï¼‰ã€‚'
- en: 'You can use this for manual flow control. Suppose you''re consuming a stream
    and you want to pause based on external conditions:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ç”¨è¿™ä¸ªæ¥æ‰‹åŠ¨æ§åˆ¶æµã€‚å‡è®¾ä½ æ­£åœ¨æ¶ˆè´¹ä¸€ä¸ªæµï¼Œå¹¶ä¸”ä½ æƒ³æ ¹æ®å¤–éƒ¨æ¡ä»¶æš‚åœï¼š
- en: '[PRE34]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Checking `readableFlowing` before calling `resume()` avoids redundant resume
    calls. If the stream is already flowing, `resume()` does nothing, but checking
    first saves a method call.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒç”¨`resume()`ä¹‹å‰æ£€æŸ¥`readableFlowing`å¯ä»¥é¿å…é‡å¤çš„æ¢å¤è°ƒç”¨ã€‚å¦‚æœæµå·²ç»æµåŠ¨ï¼Œ`resume()`ä¸ä¼šåšä»»ä½•äº‹æƒ…ï¼Œä½†å…ˆæ£€æŸ¥å¯ä»¥èŠ‚çœä¸€ä¸ªæ–¹æ³•è°ƒç”¨ã€‚
- en: This is micro-optimization territory, but in tight loops processing millions
    of chunks, small savings add up.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å±äºå¾®ä¼˜åŒ–é¢†åŸŸï¼Œä½†åœ¨å¤„ç†æ•°ç™¾ä¸‡ä¸ªå—æ—¶ç´§å¯†å¾ªç¯ä¸­ï¼Œå°çš„èŠ‚çœä¼šç´¯ç§¯èµ·æ¥ã€‚
- en: 'Another pattern: conditionally switching modes based on flow state:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æ¨¡å¼ï¼šæ ¹æ®æµçŠ¶æ€æœ‰æ¡ä»¶åœ°åˆ‡æ¢æ¨¡å¼ï¼š
- en: '[PRE35]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This lets you adapt to the stream's current state without assumptions.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è®©ä½ èƒ½å¤Ÿé€‚åº”æµçš„å½“å‰çŠ¶æ€ï¼Œè€Œä¸åšå‡è®¾ã€‚
- en: The `readableFlowing` property is also useful for debugging. If a stream isn't
    emitting data, check `readableFlowing`. If it's `false`, the stream is paused.
    If it's `null`, no listener has triggered flowing mode yet. If it's `true`, the
    stream is flowing, and the issue is elsewhere.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`readableFlowing`å±æ€§å¯¹äºè°ƒè¯•ä¹Ÿå¾ˆæœ‰ç”¨ã€‚å¦‚æœä¸€ä¸ªæµæ²¡æœ‰å‘å‡ºæ•°æ®ï¼Œæ£€æŸ¥`readableFlowing`ã€‚å¦‚æœå®ƒæ˜¯`false`ï¼Œåˆ™æµå·²æš‚åœã€‚å¦‚æœå®ƒæ˜¯`null`ï¼Œåˆ™è¿˜æ²¡æœ‰è§¦å‘æµåŠ¨æ¨¡å¼çš„ç›‘å¬å™¨ã€‚å¦‚æœå®ƒæ˜¯`true`ï¼Œåˆ™æµæ­£åœ¨æµåŠ¨ï¼Œé—®é¢˜åœ¨å…¶ä»–åœ°æ–¹ã€‚'
- en: For most code, you won't manipulate `readableFlowing` directly. But it's a handy
    introspection tool when building or debugging streaming systems.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤§å¤šæ•°ä»£ç ï¼Œä½ ä¸ä¼šç›´æ¥æ“ä½œ`readableFlowing`ã€‚ä½†åœ¨æ„å»ºæˆ–è°ƒè¯•æµç³»ç»Ÿæ—¶ï¼Œå®ƒæ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„å†…çœå·¥å…·ã€‚
- en: Performance Profiling
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€§èƒ½åˆ†æ
- en: All these optimizations - zero-copy, buffer pooling, cork/uncork - only matter
    if they improve performance for your workload. The only way to know is to measure.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›ä¼˜åŒ–â€”â€”é›¶æ‹·è´ã€ç¼“å†²åŒºæ± ã€å¡ä½/æ‹”å¡â€”â€”åªæœ‰åœ¨å®ƒä»¬æé«˜ä½ çš„å·¥ä½œè´Ÿè½½æ€§èƒ½æ—¶æ‰æœ‰æ„ä¹‰ã€‚å”¯ä¸€çŸ¥é“çš„æ–¹æ³•æ˜¯æµ‹é‡ã€‚
- en: 'Start with a baseline. Run your stream pipeline without optimizations and measure
    throughput:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ä»åŸºçº¿å¼€å§‹ã€‚åœ¨ä¸è¿›è¡Œä¼˜åŒ–çš„æƒ…å†µä¸‹è¿è¡Œä½ çš„æµç®¡é“å¹¶æµ‹é‡ååé‡ï¼š
- en: '[PRE36]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Record the baseline throughput. Then apply one optimization at a time and remeasure.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å½•åŸºçº¿ååé‡ã€‚ç„¶åä¸€æ¬¡åº”ç”¨ä¸€ä¸ªä¼˜åŒ–å¹¶é‡æ–°æµ‹é‡ã€‚
- en: 'For example, implement `_writev()`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå®ç°`_writev()`ï¼š
- en: '[PRE37]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Run the pipeline with the optimized writer and measure throughput. If it's higher,
    the optimization helps. If it's the same or lower, it doesn't (or it's not the
    bottleneck).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¼˜åŒ–åçš„å†™å…¥å™¨è¿è¡Œç®¡é“å¹¶æµ‹é‡ååé‡ã€‚å¦‚æœå®ƒæ›´é«˜ï¼Œåˆ™ä¼˜åŒ–æœ‰å¸®åŠ©ã€‚å¦‚æœç›¸åŒæˆ–æ›´ä½ï¼Œåˆ™æ²¡æœ‰å¸®åŠ©ï¼ˆæˆ–è€…å®ƒä¸æ˜¯ç“¶é¢ˆï¼‰ã€‚
- en: 'Measure memory usage too:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿæµ‹é‡å†…å­˜ä½¿ç”¨ï¼š
- en: '[PRE38]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If buffer pooling reduces heap usage without hurting throughput, it's a win.
    If it reduces throughput more than it saves memory, it's not worth it.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç¼“å†²åŒºæ± å‡å°‘äº†å †çš„ä½¿ç”¨è€Œæ²¡æœ‰æŸå®³ååé‡ï¼Œé‚£å°±æ˜¯èƒœåˆ©ã€‚å¦‚æœå®ƒå‡å°‘çš„ååé‡æ¯”èŠ‚çœçš„å†…å­˜å¤šï¼Œé‚£å°±æ²¡æœ‰æ„ä¹‰ã€‚
- en: 'Use Node''s built-in profiler to find hotspots:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Nodeçš„å†…ç½®åˆ†æå™¨æ‰¾åˆ°çƒ­ç‚¹ï¼š
- en: '[PRE39]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Look for time spent in buffer operations, syscalls, or transform methods. If
    50% of time is in `Buffer.concat()`, that's your bottleneck. Optimize that.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å¯»æ‰¾åœ¨ç¼“å†²åŒºæ“ä½œã€ç³»ç»Ÿè°ƒç”¨æˆ–è½¬æ¢æ–¹æ³•ä¸­èŠ±è´¹çš„æ—¶é—´ã€‚å¦‚æœ50%çš„æ—¶é—´èŠ±åœ¨`Buffer.concat()`ä¸Šï¼Œé‚£å°±æ˜¯ä½ çš„ç“¶é¢ˆã€‚ä¼˜åŒ–è¿™ä¸€ç‚¹ã€‚
- en: The key is to profile your actual workload, not synthetic benchmarks. If you're
    processing JSON, test with JSON. If you're serving files, test with real file
    sizes. Optimizations that help with small chunks might hurt with large chunks,
    or vice versa.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®æ˜¯è¦å¯¹ä½ çš„å®é™…å·¥ä½œè´Ÿè½½è¿›è¡Œæ€§èƒ½åˆ†æï¼Œè€Œä¸æ˜¯åˆæˆåŸºå‡†æµ‹è¯•ã€‚å¦‚æœä½ æ­£åœ¨å¤„ç†JSONï¼Œç”¨JSONè¿›è¡Œæµ‹è¯•ã€‚å¦‚æœä½ æ­£åœ¨æä¾›æ–‡ä»¶æœåŠ¡ï¼Œç”¨å®é™…æ–‡ä»¶å¤§å°è¿›è¡Œæµ‹è¯•ã€‚æœ‰åŠ©äºå°å—ä¼˜åŒ–çš„ä¼˜åŒ–å¯èƒ½å¯¹å¤§å—æœ‰å®³ï¼Œåä¹‹äº¦ç„¶ã€‚
- en: 'And remember: premature optimization is the root of all evil. Optimize after
    measuring, not before. Many "obvious" optimizations don''t help in practice.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼šè¿‡æ—©ä¼˜åŒ–æ˜¯ä¸‡æ¶ä¹‹æºã€‚åœ¨æµ‹é‡ä¹‹åä¼˜åŒ–ï¼Œè€Œä¸æ˜¯åœ¨ä¹‹å‰ã€‚è®¸å¤šâ€œæ˜¾è€Œæ˜“è§â€çš„ä¼˜åŒ–åœ¨å®è·µä¸­å¹¶æ²¡æœ‰å¸®åŠ©ã€‚
- en: Real-World Performance Patterns
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çœŸå®ä¸–ç•Œçš„æ€§èƒ½æ¨¡å¼
- en: Before complete examples, we need to establish when each optimization technique
    provides real value in production systems.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®Œæ•´çš„ç¤ºä¾‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç¡®å®šæ¯ç§ä¼˜åŒ–æŠ€æœ¯ä½•æ—¶åœ¨ç”Ÿäº§ç³»ç»Ÿä¸­æä¾›å®é™…ä»·å€¼ã€‚
- en: '**Zero-copy concepts matter in high-bandwidth, low-transform scenarios.** If
    you''re building a static file server, video streaming service, or HTTP proxy,
    minimizing copies is crucial. However, remember that Node.js streams don''t automatically
    use kernel-level zero-copy like `sendfile()`. The optimizations we''ve discussed
    - avoiding `Buffer.concat()`, using `subarray()` for views, implementing `_writev()`
    - help reduce copies in user space. For true kernel-level zero-copy at CDN scale
    (10,000+ requests/second for large files), specialized servers like nginx are
    typically more appropriate than Node.js.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨é«˜é€Ÿç‡ã€ä½è½¬æ¢åœºæ™¯ä¸­ï¼Œé›¶æ‹·è´æ¦‚å¿µå¾ˆé‡è¦ã€‚** å¦‚æœä½ æ­£åœ¨æ„å»ºé™æ€æ–‡ä»¶æœåŠ¡å™¨ã€è§†é¢‘æµæœåŠ¡æˆ–HTTPä»£ç†ï¼Œæœ€å°åŒ–æ‹·è´æ˜¯è‡³å…³é‡è¦çš„ã€‚ç„¶è€Œï¼Œè¯·è®°ä½ï¼ŒNode.jsæµä¸ä¼šè‡ªåŠ¨ä½¿ç”¨åƒ`sendfile()`è¿™æ ·çš„å†…æ ¸çº§é›¶æ‹·è´ã€‚æˆ‘ä»¬è®¨è®ºè¿‡çš„ä¼˜åŒ–
    - é¿å…ä½¿ç”¨`Buffer.concat()`ï¼Œä½¿ç”¨`subarray()`è¿›è¡Œè§†å›¾ï¼Œå®ç°`_writev()` - æœ‰åŠ©äºå‡å°‘ç”¨æˆ·ç©ºé—´ä¸­çš„æ‹·è´ã€‚å¯¹äºCDNè§„æ¨¡ï¼ˆæ¯ç§’10,000+è¯·æ±‚çš„å¤§å‹æ–‡ä»¶ï¼‰çš„çœŸæ­£å†…æ ¸çº§é›¶æ‹·è´ï¼Œé€šå¸¸æ¯”Node.jsæ›´é€‚åˆçš„æ˜¯åƒnginxè¿™æ ·çš„ä¸“ç”¨æœåŠ¡å™¨ã€‚'
- en: But if you're building an API that returns JSON responses (typically under 100KB),
    zero-copy won't help. The responses are small, they're generated dynamically (requiring
    transform), and they don't flow through file-to-socket pipes. Traditional I/O
    is fine here, and optimization effort should focus elsewhere - like JSON serialization
    or database queries.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ æ­£åœ¨æ„å»ºä¸€ä¸ªè¿”å›JSONå“åº”çš„APIï¼ˆé€šå¸¸å°äº100KBï¼‰ï¼Œé›¶æ‹·è´å°±å¸®ä¸ä¸Šå¿™äº†ã€‚å“åº”å¾ˆå°ï¼Œå®ƒä»¬æ˜¯åŠ¨æ€ç”Ÿæˆçš„ï¼ˆéœ€è¦è½¬æ¢ï¼‰ï¼Œå¹¶ä¸”ä¸é€šè¿‡æ–‡ä»¶åˆ°å¥—æ¥å­—ç®¡é“æµåŠ¨ã€‚åœ¨è¿™é‡Œï¼Œä¼ ç»Ÿçš„I/Oæ˜¯åˆé€‚çš„ï¼Œä¼˜åŒ–åŠªåŠ›åº”è¯¥é›†ä¸­åœ¨å…¶ä»–åœ°æ–¹
    - æ¯”å¦‚JSONåºåˆ—åŒ–æˆ–æ•°æ®åº“æŸ¥è¯¢ã€‚
- en: '**Scatter/gather I/O (writev) matters when you have many small writes.** HTTP
    response headers are a classic example. You write status line, multiple header
    lines, and then body - potentially dozens of small chunks. Without writev(), that''s
    dozens of syscalls. With writev(), it''s one or a few. For a high-volume HTTP
    server, this can improve response latency by 10-30% just by reducing syscall overhead.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ•£åˆ—/èšé›†I/Oï¼ˆwritevï¼‰åœ¨æœ‰è®¸å¤šå°å†™æ“ä½œæ—¶å¾ˆé‡è¦ã€‚** HTTPå“åº”å¤´å°±æ˜¯ä¸€ä¸ªç»å…¸çš„ä¾‹å­ã€‚ä½ å†™ä¸‹çŠ¶æ€è¡Œï¼Œå¤šä¸ªå¤´éƒ¨è¡Œï¼Œç„¶åæ˜¯ä¸»ä½“ - å¯èƒ½æ˜¯æ•°åä¸ªå°å—ã€‚æ²¡æœ‰writev()ï¼Œé‚£å°±æ˜¯æ•°åä¸ªç³»ç»Ÿè°ƒç”¨ã€‚æœ‰äº†writev()ï¼Œå°±åªæœ‰ä¸€ä¸ªæˆ–å‡ ä¸ªã€‚å¯¹äºé«˜æµé‡çš„HTTPæœåŠ¡å™¨ï¼Œè¿™å¯ä»¥é€šè¿‡å‡å°‘ç³»ç»Ÿè°ƒç”¨å¼€é”€æ¥æé«˜å“åº”å»¶è¿Ÿ10-30%ã€‚'
- en: But if your writes are already large (like streaming 64KB chunks from a file),
    writev() won't help. Each chunk is written in one syscall anyway. The overhead
    you're saving is negligible compared to the actual I/O cost.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ çš„å†™æ“ä½œå·²ç»å¾ˆå¤§ï¼ˆæ¯”å¦‚ä»æ–‡ä»¶ä¸­æµå¼ä¼ è¾“64KBå—ï¼‰ï¼Œwritev()å°±å¸®ä¸ä¸Šå¿™äº†ã€‚æ¯ä¸ªå—ä»ç„¶æ˜¯åœ¨ä¸€ä¸ªç³»ç»Ÿè°ƒç”¨ä¸­å†™å…¥çš„ã€‚ä½ èŠ‚çœçš„å¼€é”€ä¸å®é™…çš„I/Oæˆæœ¬ç›¸æ¯”å¾®ä¸è¶³é“ã€‚
- en: '**Buffer pooling pays off when allocation rate is extreme.** If you''re processing
    binary protocols with many small messages (think network packet parsing, IoT sensor
    data, financial tick data), you might allocate millions of small buffers per second.
    Pooling can reduce GC pause time by 50-80%, transforming a system that struggles
    to maintain 10,000 msg/sec into one that easily handles 100,000 msg/sec.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨åˆ†é…ç‡æç«¯æ—¶ï¼Œç¼“å†²åŒºæ± åŒ–æ˜¯æœ‰ç›Šçš„ã€‚** å¦‚æœä½ æ­£åœ¨å¤„ç†å…·æœ‰è®¸å¤šå°æ¶ˆæ¯çš„äºŒè¿›åˆ¶åè®®ï¼ˆä¾‹å¦‚ç½‘ç»œæ•°æ®åŒ…è§£æã€ç‰©è”ç½‘ä¼ æ„Ÿå™¨æ•°æ®ã€é‡‘èäº¤æ˜“æ•°æ®ï¼‰ï¼Œä½ å¯èƒ½æ¯ç§’åˆ†é…æ•°ç™¾ä¸‡ä¸ªå°ç¼“å†²åŒºã€‚æ± åŒ–å¯ä»¥å°†GCæš‚åœæ—¶é—´å‡å°‘50-80%ï¼Œå°†ä¸€ä¸ªéš¾ä»¥ç»´æŒ10,000
    msg/secçš„ç³»ç»Ÿè½¬å˜ä¸ºä¸€ä¸ªå¯ä»¥è½»æ¾å¤„ç†100,000 msg/secçš„ç³»ç»Ÿã€‚'
- en: But if your stream processing allocates buffers at a modest rate (say, 1,000
    per second for a typical web application), pooling won't provide measurable benefit.
    The GC can easily handle that rate, and the complexity of pool management isn't
    justified.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ çš„æµå¤„ç†ä»¥é€‚ä¸­çš„é€Ÿç‡åˆ†é…ç¼“å†²åŒºï¼ˆæ¯”å¦‚ï¼Œå¯¹äºä¸€ä¸ªå…¸å‹çš„Webåº”ç”¨ï¼Œæ¯ç§’1,000ä¸ªï¼‰ï¼Œæ± åŒ–ä¸ä¼šæä¾›å¯æµ‹é‡çš„å¥½å¤„ã€‚åƒåœ¾æ”¶é›†å™¨å¯ä»¥è½»æ¾å¤„ç†é‚£ä¸ªé€Ÿç‡ï¼Œè€Œæ± åŒ–ç®¡ç†å¤æ‚æ€§æ˜¯ä¸åˆç†çš„ã€‚
- en: '**Cork/uncork optimization helps with bursty write patterns.** If your application
    processes batch jobs - reading 1000 records from a database, transforming them,
    and writing results - corking around the batch gives a clear win. You might reduce
    write operations from 1000 to 10-50, boosting throughput a lot.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**Cork/uncork ä¼˜åŒ–æœ‰åŠ©äºçªå‘å†™å…¥æ¨¡å¼ã€‚** å¦‚æœä½ çš„åº”ç”¨ç¨‹åºå¤„ç†æ‰¹å¤„ç†ä½œä¸š - ä»æ•°æ®åº“ä¸­è¯»å– 1000 æ¡è®°å½•ï¼Œè½¬æ¢å®ƒä»¬ï¼Œå¹¶å†™å…¥ç»“æœ
    - åœ¨æ‰¹å¤„ç†å‘¨å›´è¿›è¡Œ corking å¯ä»¥å¸¦æ¥æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚ä½ å¯èƒ½å°†å†™æ“ä½œä» 1000 å‡å°‘åˆ° 10-50ï¼Œä»è€Œå¤§å¹…æé«˜ååé‡ã€‚'
- en: But for steady-state streaming (like tailing a log file and forwarding lines),
    cork/uncork doesn't help. Data flows continuously, and batching doesn't reduce
    total I/O operations, it just delays them. In some cases, it might even increase
    latency for no throughput benefit.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¯¹äºç¨³æ€æµå¼ä¼ è¾“ï¼ˆå¦‚è·Ÿè¸ªæ—¥å¿—æ–‡ä»¶å¹¶è½¬å‘è¡Œï¼‰ï¼Œcork/uncork æ²¡æœ‰å¸®åŠ©ã€‚æ•°æ®æŒç»­æµåŠ¨ï¼Œæ‰¹å¤„ç†ä¸ä¼šå‡å°‘æ€»çš„ I/O æ“ä½œï¼Œå®ƒåªæ˜¯å»¶è¿Ÿäº†å®ƒä»¬ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå®ƒç”šè‡³å¯èƒ½åœ¨æ²¡æœ‰æé«˜ååé‡çš„æƒ…å†µä¸‹å¢åŠ å»¶è¿Ÿã€‚
- en: 'The pattern here is clear: measure your workload characteristics first, then
    apply optimizations that match those characteristics. Don''t optimize for scenarios
    you don''t have.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æ¨¡å¼å¾ˆæ¸…æ™°ï¼šé¦–å…ˆæµ‹é‡ä½ çš„å·¥ä½œè´Ÿè½½ç‰¹å¾ï¼Œç„¶ååº”ç”¨åŒ¹é…è¿™äº›ç‰¹å¾çš„ä¼˜åŒ–ã€‚ä¸è¦ä¸ºäº†ä½ æ²¡æœ‰çš„åœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚
- en: 'A decision framework:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ¡†æ¶ï¼š
- en: '**Profile your application under realistic load.** Use `perf` on Linux, Instruments
    on macOS, or Node''s built-in profiler. Identify where CPU time is actually going.
    If you see high CPU usage but low disk/network utilization, CPU overhead (copying,
    syscalls) is your bottleneck. If disk/network is saturated and CPU is low, I/O
    throughput is your limit, not CPU overhead.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åœ¨ç°å®è´Ÿè½½ä¸‹åˆ†æä½ çš„åº”ç”¨ç¨‹åºã€‚** åœ¨ Linux ä¸Šä½¿ç”¨ `perf`ï¼Œåœ¨ macOS ä¸Šä½¿ç”¨ Instrumentsï¼Œæˆ–åœ¨ Node çš„å†…ç½®åˆ†æå™¨ä¸­è¿›è¡Œåˆ†æã€‚ç¡®å®š
    CPU æ—¶é—´å®é™…ä¸ŠèŠ±åœ¨äº†å“ªé‡Œã€‚å¦‚æœä½ çœ‹åˆ°é«˜ CPU ä½¿ç”¨ç‡ä½†ç£ç›˜/ç½‘ç»œåˆ©ç”¨ç‡ä½ï¼ŒCPU è´Ÿè½½ï¼ˆå¤åˆ¶ã€ç³»ç»Ÿè°ƒç”¨ï¼‰æ˜¯ä½ çš„ç“¶é¢ˆã€‚å¦‚æœç£ç›˜/ç½‘ç»œé¥±å’Œä¸” CPU ä½ï¼ŒI/O
    ååé‡æ˜¯ä½ çš„é™åˆ¶ï¼Œè€Œä¸æ˜¯ CPU è´Ÿè½½ã€‚'
- en: '**Measure allocation rates and GC impact.** Use `process.memoryUsage()` and
    track heap growth. Use `--trace-gc` to see GC pause times. If allocations are
    extreme (multi-MB/sec) and GC pauses are long (`>10ms`), buffer pooling might
    help. If allocations are modest and GC pauses are tiny (`<1ms`), pooling won''t
    provide benefit.'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æµ‹é‡åˆ†é…ç‡å’Œåƒåœ¾å›æ”¶å½±å“ã€‚** ä½¿ç”¨ `process.memoryUsage()` å¹¶è·Ÿè¸ªå †å¢é•¿ã€‚ä½¿ç”¨ `--trace-gc` æ¥æŸ¥çœ‹åƒåœ¾å›æ”¶æš‚åœæ—¶é—´ã€‚å¦‚æœåˆ†é…æç«¯ï¼ˆå¤š
    MB/ç§’ï¼‰ä¸”åƒåœ¾å›æ”¶æš‚åœæ—¶é—´é•¿ï¼ˆ`>10ms`ï¼‰ï¼Œç¼“å†²åŒºæ± åŒ–å¯èƒ½æœ‰æ‰€å¸®åŠ©ã€‚å¦‚æœåˆ†é…é€‚åº¦ä¸”åƒåœ¾å›æ”¶æš‚åœæ—¶é—´çŸ­ï¼ˆ`<1ms`ï¼‰ï¼Œæ± åŒ–ä¸ä¼šæä¾›ä»»ä½•å¥½å¤„ã€‚'
- en: '**Count your syscalls.** Use `strace` on Linux or `dtruss` on macOS to see
    how many syscalls your application makes. If you see thousands of tiny write()
    calls when you expected a few large ones, writev() or cork/uncork can help. If
    you see mostly large I/O operations, batching won''t matter.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®¡ç®—ä½ çš„ç³»ç»Ÿè°ƒç”¨æ¬¡æ•°ã€‚** åœ¨ Linux ä¸Šä½¿ç”¨ `strace` æˆ–åœ¨ macOS ä¸Šä½¿ç”¨ `dtruss` æ¥æŸ¥çœ‹ä½ çš„åº”ç”¨ç¨‹åºæ‰§è¡Œäº†å¤šå°‘ç³»ç»Ÿè°ƒç”¨ã€‚å¦‚æœä½ çœ‹åˆ°é¢„æœŸåªæœ‰å‡ ä¸ªå¤§å†™()è°ƒç”¨æ—¶å‡ºç°äº†æˆåƒä¸Šä¸‡çš„å¾®å°å†™()è°ƒç”¨ï¼Œwritev()
    æˆ– cork/uncork å¯ä»¥æœ‰æ‰€å¸®åŠ©ã€‚å¦‚æœä½ çœ‹åˆ°ä¸»è¦æ˜¯å¤§ I/O æ“ä½œï¼Œæ‰¹å¤„ç†å°±ä¸ä¼šå¾ˆé‡è¦ã€‚'
- en: '**Benchmark with and without optimizations.** The only way to know if an optimization
    helps is to measure. Implement the optimization, benchmark throughput and latency,
    and compare to baseline. If throughput improves by 20% and latency doesn''t increase,
    it''s a win. If there''s no change or performance degrades, remove the optimization.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åœ¨æœ‰å’Œæ²¡æœ‰ä¼˜åŒ–çš„æƒ…å†µä¸‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚** çŸ¥é“ä¼˜åŒ–æ˜¯å¦æœ‰å¸®åŠ©çš„å”¯ä¸€æ–¹æ³•æ˜¯è¿›è¡Œæµ‹é‡ã€‚å®ç°ä¼˜åŒ–ï¼ŒåŸºå‡†æµ‹è¯•ååé‡å’Œå»¶è¿Ÿï¼Œå¹¶ä¸åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœååé‡æé«˜äº†
    20%ï¼Œè€Œå»¶è¿Ÿæ²¡æœ‰å¢åŠ ï¼Œé‚£ä¹ˆå°±æ˜¯èƒœåˆ©ã€‚å¦‚æœæ²¡æœ‰å˜åŒ–æˆ–æ€§èƒ½ä¸‹é™ï¼Œåˆ™åˆ é™¤ä¼˜åŒ–ã€‚'
- en: This methodical approach prevents premature optimization. You might think buffer
    pooling will help, but profiling shows allocation isn't a bottleneck. You might
    assume zero-copy is necessary, but you're transforming every byte and can't use
    it anyway. Measure, then optimize.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æœ‰ç³»ç»Ÿçš„æ–¹æ³•å¯ä»¥é˜²æ­¢è¿‡æ—©ä¼˜åŒ–ã€‚ä½ å¯èƒ½è®¤ä¸ºç¼“å†²åŒºæ± åŒ–ä¼šæœ‰å¸®åŠ©ï¼Œä½†åˆ†æè¡¨æ˜åˆ†é…å¹¶ä¸æ˜¯ç“¶é¢ˆã€‚ä½ å¯èƒ½è®¤ä¸ºé›¶æ‹·è´æ˜¯å¿…è¦çš„ï¼Œä½†ä½ æ­£åœ¨è½¬æ¢æ¯ä¸€ä¸ªå­—èŠ‚ï¼Œè€Œä¸”æ— è®ºå¦‚ä½•éƒ½ä¸èƒ½ä½¿ç”¨å®ƒã€‚å…ˆæµ‹é‡ï¼Œç„¶åä¼˜åŒ–ã€‚
- en: 'Practical Example: Optimized File Copy Pipeline'
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®é™…ç¤ºä¾‹ï¼šä¼˜åŒ–æ–‡ä»¶å¤åˆ¶ç®¡é“
- en: Here's an optimized file copy pipeline that combines these techniques.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªç»“åˆäº†è¿™äº›æŠ€æœ¯çš„ä¼˜åŒ–æ–‡ä»¶å¤åˆ¶ç®¡é“çš„ç¤ºä¾‹ã€‚
- en: 'We''ll copy a large file using larger buffers for throughput, `_writev()` for
    batching, and efficient buffer handling:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨æ›´å¤§çš„ç¼“å†²åŒºä»¥æé«˜ååé‡ï¼Œä½¿ç”¨ `_writev()` è¿›è¡Œæ‰¹å¤„ç†ï¼Œä»¥åŠé«˜æ•ˆçš„ç¼“å†²åŒºå¤„ç†æ¥å¤åˆ¶ä¸€ä¸ªå¤§æ–‡ä»¶ï¼š
- en: '[PRE40]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This pipeline uses 64KB buffers and implements `_writev()` for batching multiple
    chunks into single write operations. Note that 64KB is actually the default for
    `fs.createReadStream()` and `fs.createWriteStream()` as of Node.js 22 - we're
    being explicit here for clarity, but you could omit `highWaterMark` entirely for
    file streams. The base `stream.Readable` class still defaults to 16KB.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç®¡é“ä½¿ç”¨ 64KB ç¼“å†²åŒºå¹¶å®ç°äº† `_writev()` ä»¥å°†å¤šä¸ªå—æ‰¹å¤„ç†ä¸ºå•ä¸ªå†™æ“ä½œã€‚è¯·æ³¨æ„ï¼Œ64KB å®é™…ä¸Šæ˜¯ Node.js 22 çš„ `fs.createReadStream()`
    å’Œ `fs.createWriteStream()` çš„é»˜è®¤å€¼ - æˆ‘ä»¬åœ¨è¿™é‡Œæ˜ç¡®æŒ‡å‡ºä»¥å¢å¼ºæ¸…æ™°åº¦ï¼Œä½†å¯¹äºæ–‡ä»¶æµï¼Œä½ å¯ä»¥å®Œå…¨çœç•¥ `highWaterMark`ã€‚åŸºæœ¬çš„
    `stream.Readable` ç±»ä»ç„¶é»˜è®¤ä¸º 16KBã€‚
- en: 'For file-to-file copies where you don''t need to process the data, consider
    using `fs.copyFile()` directly, which can use OS-level optimizations:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸éœ€è¦å¤„ç†æ•°æ®çš„æ–‡ä»¶åˆ°æ–‡ä»¶å¤åˆ¶ï¼Œè€ƒè™‘ç›´æ¥ä½¿ç”¨ `fs.copyFile()`ï¼Œè¿™å¯ä»¥ä½¿ç”¨æ“ä½œç³»ç»Ÿçº§åˆ«çš„ä¼˜åŒ–ï¼š
- en: '[PRE41]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The key takeaway: combine techniques. Use larger buffers for high throughput,
    implement `_writev()` for batching, and use `fs.copyFile()` when you don''t need
    stream processing.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®è¦ç‚¹ï¼šç»“åˆæŠ€æœ¯ã€‚ä½¿ç”¨æ›´å¤§çš„ç¼“å†²åŒºä»¥å®ç°é«˜ååé‡ï¼Œå®ç° `_writev()` ä»¥è¿›è¡Œæ‰¹å¤„ç†ï¼Œå¹¶åœ¨ä¸éœ€è¦æµå¤„ç†æ—¶ä½¿ç”¨ `fs.copyFile()`ã€‚
- en: Measurement and Debugging Techniques
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµ‹é‡å’Œè°ƒè¯•æŠ€æœ¯
- en: Figuring out whether your optimizations are working takes more than eyeballing
    throughput numbers. Here are some ways to dig deeper.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®šä½ çš„ä¼˜åŒ–æ˜¯å¦èµ·ä½œç”¨éœ€è¦ä¸ä»…ä»…æ˜¯æŸ¥çœ‹ååé‡æ•°å­—ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ·±å…¥æŒ–æ˜çš„æ–¹æ³•ã€‚
- en: '**Syscall tracing** shows exactly how your optimizations affect system calls.
    On Linux, use `strace` with timestamping:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç³»ç»Ÿè°ƒç”¨è·Ÿè¸ª** å¯ä»¥æ˜¾ç¤ºä½ çš„ä¼˜åŒ–å¦‚ä½•å½±å“ç³»ç»Ÿè°ƒç”¨ã€‚åœ¨ Linux ä¸Šï¼Œä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„ `strace`ï¼š'
- en: '[PRE42]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `-c` flag shows a summary of syscall counts. If you implemented `_writev()`
    but still see hundreds of individual `write()` calls, something's wrong - maybe
    the stream isn't corking, or chunks aren't being buffered. If you see dozens of
    `writev()` calls with the same number of operations as your data chunks, the optimization
    is working.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`-c` æ ‡å¿—æ˜¾ç¤ºç³»ç»Ÿè°ƒç”¨è®¡æ•°çš„æ‘˜è¦ã€‚å¦‚æœä½ å®ç°äº† `_writev()` ä½†ä»ç„¶çœ‹åˆ°æ•°ç™¾ä¸ªå•ç‹¬çš„ `write()` è°ƒç”¨ï¼Œé‚£ä¹ˆæœ‰é—®é¢˜ - å¯èƒ½æ˜¯æµæ²¡æœ‰è¢«å¡ä½ï¼Œæˆ–è€…å—æ²¡æœ‰è¢«ç¼“å†²ã€‚å¦‚æœä½ çœ‹åˆ°æ•°åä¸ªä¸ä½ çš„æ•°æ®å—æ“ä½œæ•°ç›¸åŒçš„
    `writev()` è°ƒç”¨ï¼Œåˆ™ä¼˜åŒ–æ­£åœ¨èµ·ä½œç”¨ã€‚'
- en: 'For more detail, trace specific syscalls:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ›´å¤šç»†èŠ‚ï¼Œè·Ÿè¸ªç‰¹å®šçš„ç³»ç»Ÿè°ƒç”¨ï¼š
- en: '[PRE43]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This shows every write and writev call with parameters and return values. You
    can verify that writev() is receiving multiple buffers, not one.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¾ç¤ºäº†æ¯ä¸ªå¸¦æœ‰å‚æ•°å’Œè¿”å›å€¼çš„å†™å’Œ writev è°ƒç”¨ã€‚ä½ å¯ä»¥éªŒè¯ writev() æ˜¯å¦æ¥æ”¶å¤šä¸ªç¼“å†²åŒºï¼Œè€Œä¸æ˜¯ä¸€ä¸ªã€‚
- en: '**CPU profiling with perf** shows where CPU time is spent. On Linux:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ **perf** è¿›è¡Œ **CPU åˆ†æ** å¯ä»¥æ˜¾ç¤º CPU æ—¶é—´èŠ±è´¹åœ¨å“ªé‡Œã€‚åœ¨ Linux ä¸Šï¼š
- en: '[PRE44]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This samples the call stack 99 times per second and shows which functions consume
    the most CPU. If you see high CPU in `memcpy` or `Buffer.concat`, you have unnecessary
    copying. If you see high CPU in syscall entry/exit (`__kernel_vsyscall` on x86),
    you're syscall-bound - batching might help.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¯ç§’é‡‡æ ·è°ƒç”¨æ ˆ 99 æ¬¡ï¼Œå¹¶æ˜¾ç¤ºå“ªäº›å‡½æ•°æ¶ˆè€—äº†æœ€å¤šçš„ CPUã€‚å¦‚æœä½ åœ¨ `memcpy` æˆ– `Buffer.concat` ä¸­çœ‹åˆ°é«˜ CPUï¼Œä½ æœ‰ä¸å¿…è¦çš„å¤åˆ¶ã€‚å¦‚æœä½ åœ¨ç³»ç»Ÿè°ƒç”¨å…¥å£/é€€å‡ºï¼ˆx86
    ä¸Šçš„ `__kernel_vsyscall`ï¼‰ä¸­çœ‹åˆ°é«˜ CPUï¼Œä½ æ˜¯ç³»ç»Ÿè°ƒç”¨å—é™ - æ‰¹å¤„ç†å¯èƒ½æœ‰æ‰€å¸®åŠ©ã€‚
- en: Look for the `sendfile()` function in the profile. If it's there and consuming
    noticeable CPU (which is actually kernel time, but attributed to sendfile), zero-copy
    is active. If it's absent and you expected it, zero-copy isn't being used.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é…ç½®æ–‡ä»¶ä¸­æŸ¥æ‰¾ `sendfile()` å‡½æ•°ã€‚å¦‚æœå®ƒå­˜åœ¨å¹¶ä¸”æ¶ˆè€—äº†æ˜¾è‘—çš„ CPUï¼ˆè¿™å®é™…ä¸Šæ˜¯å†…æ ¸æ—¶é—´ï¼Œä½†å½’å› äº sendfileï¼‰ï¼Œåˆ™é›¶æ‹·è´æ­£åœ¨æ¿€æ´»ã€‚å¦‚æœå®ƒä¸å­˜åœ¨è€Œä½ æœŸæœ›å®ƒå­˜åœ¨ï¼Œåˆ™æ²¡æœ‰ä½¿ç”¨é›¶æ‹·è´ã€‚
- en: '**Memory profiling with Chrome DevTools or V8''s heap profiler** shows allocation
    patterns. Connect to Node with `--inspect` and open Chrome DevTools. Take heap
    snapshots before and after processing a stream. If you see millions of small Buffer
    objects in the heap, allocations are excessive - pooling might help. You can also
    use `node --heap-prof your-script.js` to generate a heap profile file, then load
    it in DevTools for analysis.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ **Chrome DevTools æˆ– V8 çš„å †åˆ†æå™¨** è¿›è¡Œ **å†…å­˜åˆ†æ** å¯ä»¥æ˜¾ç¤ºåˆ†é…æ¨¡å¼ã€‚ä½¿ç”¨ `--inspect` è¿æ¥åˆ° Node
    å¹¶æ‰“å¼€ Chrome DevToolsã€‚åœ¨å¤„ç†æµå‰åè¿›è¡Œå †å¿«ç…§ã€‚å¦‚æœä½ åœ¨å †ä¸­çœ‹åˆ°æ•°ç™¾ä¸‡ä¸ªå°çš„ Buffer å¯¹è±¡ï¼Œåˆ™åˆ†é…è¿‡å¤š - æ± åŒ–å¯èƒ½æœ‰æ‰€å¸®åŠ©ã€‚ä½ è¿˜å¯ä»¥ä½¿ç”¨
    `node --heap-prof your-script.js` ç”Ÿæˆå †åˆ†ææ–‡ä»¶ï¼Œç„¶ååœ¨ DevTools ä¸­åŠ è½½å®ƒè¿›è¡Œåˆ†æã€‚
- en: The "Allocation Timeline" view shows allocation rate over time. Spiky allocation
    patterns indicate burst allocations - maybe during certain processing phases.
    This helps identify where to apply pooling.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: â€œåˆ†é…æ—¶é—´çº¿â€è§†å›¾æ˜¾ç¤ºäº†éšæ—¶é—´å˜åŒ–çš„åˆ†é…ç‡ã€‚å°–å³°åˆ†é…æ¨¡å¼è¡¨æ˜çªå‘åˆ†é… - å¯èƒ½æ˜¯åœ¨æŸäº›å¤„ç†é˜¶æ®µã€‚è¿™æœ‰åŠ©äºç¡®å®šåº”ç”¨æ± åŒ–çš„ä½ç½®ã€‚
- en: '**GC tracing with --trace-gc** shows garbage collection activity:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ **--trace-gc** è¿›è¡Œ **GC è·Ÿè¸ª** å¯ä»¥æ˜¾ç¤ºåƒåœ¾å›æ”¶æ´»åŠ¨ï¼š
- en: '[PRE45]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Each GC event prints timing and heap sizes. Look for frequent minor GCs (young
    generation) - a sign of high allocation rate. If buffer pooling reduces minor
    GC frequency, it's working. Look for major GCs (old generation) - these are more
    expensive. If pooling reduces major GC pause times, that's a big win.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªåƒåœ¾å›æ”¶äº‹ä»¶éƒ½ä¼šæ‰“å°æ—¶é—´å’Œå †å¤§å°ã€‚å¯»æ‰¾é¢‘ç¹çš„æ¬¡è¦åƒåœ¾å›æ”¶ï¼ˆå¹´è½»ä»£ï¼‰â€”â€”è¿™æ˜¯é«˜åˆ†é…ç‡çš„æ ‡å¿—ã€‚å¦‚æœç¼“å†²åŒºæ± å‡å°‘äº†æ¬¡è¦åƒåœ¾å›æ”¶é¢‘ç‡ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯æœ‰æ•ˆçš„ã€‚å¯»æ‰¾ä¸»è¦åƒåœ¾å›æ”¶ï¼ˆè€å¹´ä»£ï¼‰â€”â€”è¿™äº›æ›´æ˜‚è´µã€‚å¦‚æœæ± åŒ–å‡å°‘äº†ä¸»è¦åƒåœ¾å›æ”¶æš‚åœæ—¶é—´ï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„èƒœåˆ©ã€‚
- en: '**Detailed GC stats with --trace-gc-verbose** provides even more info:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨`--trace-gc-verbose`æä¾›è¯¦ç»†çš„åƒåœ¾å›æ”¶ç»Ÿè®¡ä¿¡æ¯ï¼š**'
- en: '[PRE46]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This shows GC timing, how much memory was reclaimed, how much survived, and
    promotion rates. High promotion rates (objects moving from young to old generation)
    indicate long-lived objects - not ideal for buffers that should be short-lived
    or pooled.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¾ç¤ºäº†åƒåœ¾å›æ”¶æ—¶é—´ï¼Œå›æ”¶äº†å¤šå°‘å†…å­˜ï¼Œæœ‰å¤šå°‘å†…å­˜å¹¸å­˜ï¼Œä»¥åŠæå‡ç‡ã€‚é«˜æå‡ç‡ï¼ˆå¯¹è±¡ä»å¹´è½»ä»£ç§»åŠ¨åˆ°è€å¹´ä»£ï¼‰è¡¨æ˜å¯¹è±¡ç”Ÿå‘½å‘¨æœŸé•¿â€”â€”å¯¹äºåº”è¯¥çŸ­æš‚æˆ–æ± åŒ–çš„ç¼“å†²åŒºæ¥è¯´å¹¶ä¸ç†æƒ³ã€‚
- en: '**Event loop delay measurement** tells you if I/O operations block the event
    loop. Use `perf_hooks`:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**äº‹ä»¶å¾ªç¯å»¶è¿Ÿæµ‹é‡**å‘Šè¯‰ä½ I/Oæ“ä½œæ˜¯å¦é˜»å¡äº‹ä»¶å¾ªç¯ã€‚ä½¿ç”¨`perf_hooks`ï¼š'
- en: '[PRE47]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: If p99 delay spikes when processing streams, your code is blocking the event
    loop. This could be synchronous buffer operations (Buffer.concat() on huge buffers)
    or large chunks overwhelming the processing loop. Break up the work or use smaller
    chunks.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå¤„ç†æµæ—¶p99å»¶è¿Ÿæ¿€å¢ï¼Œä½ çš„ä»£ç æ­£åœ¨é˜»å¡äº‹ä»¶å¾ªç¯ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºåŒæ­¥ç¼“å†²åŒºæ“ä½œï¼ˆåœ¨å¤§å‹ç¼“å†²åŒºä¸Šä½¿ç”¨Buffer.concat()ï¼‰æˆ–å¤§é‡æ•°æ®å—å‹å€’å¤„ç†å¾ªç¯ã€‚åˆ†è§£å·¥ä½œæˆ–ä½¿ç”¨æ›´å°çš„å—ã€‚
- en: '**Node.js internal module debugging** can be enabled with the `NODE_DEBUG`
    environment variable:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯ä»¥é€šè¿‡`NODE_DEBUG`ç¯å¢ƒå˜é‡å¯ç”¨Node.jså†…éƒ¨æ¨¡å—è°ƒè¯•ï¼š**'
- en: '[PRE48]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This enables debug output for the specified built-in modules (fs, net, stream
    in this example). It shows internal operations like file opens, socket connections,
    and stream state changes.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ç”¨äº†æŒ‡å®šå†…ç½®æ¨¡å—ï¼ˆä¾‹å¦‚fsã€netã€streamï¼‰çš„è°ƒè¯•è¾“å‡ºã€‚å®ƒæ˜¾ç¤ºäº†å†…éƒ¨æ“ä½œï¼Œå¦‚æ–‡ä»¶æ‰“å¼€ã€å¥—æ¥å­—è¿æ¥å’ŒæµçŠ¶æ€æ›´æ”¹ã€‚
- en: 'Note: The `DEBUG=*` environment variable is for the popular npm `debug` package,
    not Node.js internals. For actual syscall-level visibility, use `strace` on Linux
    or `dtruss` on macOS as shown earlier.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼š`DEBUG=*`ç¯å¢ƒå˜é‡æ˜¯ç”¨äºæµè¡Œçš„npm `debug`åŒ…ï¼Œè€Œä¸æ˜¯Node.jså†…éƒ¨ã€‚å¯¹äºå®é™…çš„ç³»ç»Ÿè°ƒç”¨çº§åˆ«å¯è§æ€§ï¼Œè¯·ä½¿ç”¨å‰é¢æ˜¾ç¤ºçš„Linuxä¸Šçš„`strace`æˆ–macOSä¸Šçš„`dtruss`ã€‚
- en: Combining these techniques gives you a complete picture. Syscall tracing shows
    I/O behavior, CPU profiling shows computational cost, memory profiling shows allocation
    impact, GC tracing shows memory management overhead, and event loop monitoring
    shows responsiveness. Together, they tell you whether your optimizations are actually
    working and where to focus next.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆè¿™äº›æŠ€æœ¯å¯ä»¥ç»™ä½ ä¸€ä¸ªå®Œæ•´çš„å›¾æ™¯ã€‚ç³»ç»Ÿè°ƒç”¨è·Ÿè¸ªæ˜¾ç¤ºI/Oè¡Œä¸ºï¼ŒCPUåˆ†ææ˜¾ç¤ºè®¡ç®—æˆæœ¬ï¼Œå†…å­˜åˆ†ææ˜¾ç¤ºåˆ†é…å½±å“ï¼Œåƒåœ¾å›æ”¶è·Ÿè¸ªæ˜¾ç¤ºå†…å­˜ç®¡ç†å¼€é”€ï¼Œäº‹ä»¶å¾ªç¯ç›‘æ§æ˜¾ç¤ºå“åº”æ€§ã€‚å…±åŒæ¥è¯´ï¼Œå®ƒä»¬å‘Šè¯‰ä½ ä½ çš„ä¼˜åŒ–æ˜¯å¦çœŸæ­£æœ‰æ•ˆï¼Œä»¥åŠä¸‹ä¸€æ­¥åº”è¯¥å…³æ³¨å“ªé‡Œã€‚
- en: When to Use These Techniques
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å½“ä½¿ç”¨è¿™äº›æŠ€æœ¯æ—¶
- en: 'Not every stream needs these optimizations. Here''s when they''re worth it:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶éæ¯ä¸ªæµéƒ½éœ€è¦è¿™äº›ä¼˜åŒ–ã€‚ä»¥ä¸‹æ˜¯å®ƒä»¬å€¼å¾—çš„æ—¶å€™ï¼š
- en: '**Minimize buffer copies when:**'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨ä»¥ä¸‹æƒ…å†µä¸‹æœ€å°åŒ–ç¼“å†²åŒºå¤åˆ¶ï¼š**'
- en: Processing large files through stream pipelines
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æµç®¡é“å¤„ç†å¤§æ–‡ä»¶
- en: Building high-throughput data processing systems
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„å»ºé«˜ååé‡æ•°æ®å¤„ç†ç³»ç»Ÿ
- en: Serving static content (though kernel-level zero-copy isn't automatic in Node.js)
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æä¾›é™æ€å†…å®¹ï¼ˆå°½ç®¡åœ¨Node.jsä¸­å†…æ ¸çº§åˆ«çš„é›¶æ‹·è´ä¸æ˜¯è‡ªåŠ¨çš„ï¼‰
- en: '**Use `fs.copyFile()` with `COPYFILE_FICLONE` for true zero-copy file duplication**
    on filesystems that support reflinks (Btrfs, XFS, APFS).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨æ”¯æŒreflinksï¼ˆBtrfsã€XFSã€APFSï¼‰çš„æ–‡ä»¶ç³»ç»Ÿä¸Šä½¿ç”¨`fs.copyFile()`ä¸`COPYFILE_FICLONE`è¿›è¡ŒçœŸæ­£çš„é›¶æ‹·è´æ–‡ä»¶å¤åˆ¶**ã€‚'
- en: '**Don''t over-optimize copying when:**'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ä¸è¦è¿‡åº¦ä¼˜åŒ–å¤åˆ¶ï¼š**'
- en: You need to transform data (copies are unavoidable)
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦è½¬æ¢æ•°æ®ï¼ˆå¤åˆ¶æ˜¯ä¸å¯é¿å…çš„ï¼‰
- en: Data is small (optimization overhead exceeds benefit)
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®é‡å°ï¼ˆä¼˜åŒ–å¼€é”€è¶…è¿‡æ”¶ç›Šï¼‰
- en: You're not CPU-bound on buffer operations (profile first!)
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ åœ¨ç¼“å†²åŒºæ“ä½œä¸Šä¸æ˜¯CPUå—é™ï¼ˆå…ˆåˆ†æï¼ï¼‰
- en: '**Use scatter/gather I/O (`_writev()`) when:**'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ä½¿ç”¨åˆ†æ•£/æ”¶é›†I/Oï¼ˆ`_writev()`ï¼‰ï¼š**'
- en: Writing many small chunks
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼–å†™è®¸å¤šå°å—å†…å®¹
- en: Syscall overhead is high (profiling confirms)
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿè°ƒç”¨å¼€é”€é«˜ï¼ˆåˆ†æç¡®è®¤ï¼‰
- en: Your destination supports vectored writes
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ çš„ç›®æ ‡æ”¯æŒå‘é‡å†™å…¥
- en: '**Skip it when:**'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨ä»¥ä¸‹æƒ…å†µä¸‹è·³è¿‡ï¼š**'
- en: Chunks are already large (batching doesn't help)
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å—å·²ç»å¾ˆå¤§ï¼ˆæ‰¹å¤„ç†æ²¡æœ‰å¸®åŠ©ï¼‰
- en: You're writing to a stream that doesn't benefit (in-memory buffers)
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ æ­£åœ¨å†™å…¥ä¸€ä¸ªæ²¡æœ‰å—ç›Šçš„æµï¼ˆå†…å­˜ç¼“å†²åŒºï¼‰
- en: '**Use buffer pooling when:**'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ä½¿ç”¨ç¼“å†²åŒºæ± ï¼š**'
- en: Allocating millions of buffers (GC pressure is high)
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†é…æ•°ç™¾ä¸‡ä¸ªç¼“å†²åŒºï¼ˆåƒåœ¾å›æ”¶å‹åŠ›é«˜ï¼‰
- en: Buffers are uniform size (easy to pool)
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºå¤§å°ç»Ÿä¸€ï¼ˆæ˜“äºæ± åŒ–ï¼‰
- en: You control buffer lifecycle (can safely reuse)
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ æ§åˆ¶ç¼“å†²åŒºç”Ÿå‘½å‘¨æœŸï¼ˆå¯ä»¥å®‰å…¨é‡ç”¨ï¼‰
- en: '**Skip it when:**'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨ä»¥ä¸‹æƒ…å†µä¸‹è·³è¿‡ï¼š**'
- en: Buffers are variable size (hard to pool efficiently)
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºå¤§å°å¯å˜ï¼ˆéš¾ä»¥é«˜æ•ˆåœ°æ± åŒ–ï¼‰
- en: Allocations aren't a bottleneck (profiling shows low GC impact)
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†é…ä¸æ˜¯ç“¶é¢ˆï¼ˆåˆ†ææ˜¾ç¤ºä½åƒåœ¾å›æ”¶å½±å“ï¼‰
- en: '**Use cork/uncork when:**'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ä½¿ç”¨å¡ä½/è§£é™¤å¡ä½ï¼š**'
- en: Writing bursts of small chunks
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†™å…¥å°å—çš„çªå‘
- en: You control the burst boundaries (start and end)
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ æ§åˆ¶ç€çªå‘è¾¹ç•Œï¼ˆå¼€å§‹å’Œç»“æŸï¼‰
- en: Latency during the burst is acceptable
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çªå‘æœŸé—´çš„å»¶è¿Ÿæ˜¯å¯ä»¥æ¥å—çš„
- en: '**Skip it when:**'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨ä»¥ä¸‹æƒ…å†µä¸‹è·³è¿‡ï¼š**'
- en: Writes are already batched naturally
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†™å…¥å·²ç»è‡ªç„¶æ‰¹å¤„ç†
- en: Latency matters (corking delays flushing)
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å»¶è¿Ÿå¾ˆé‡è¦ï¼ˆå¡ä½å»¶è¿Ÿä¼šå¯¼è‡´åˆ·æ–°ï¼‰
- en: Measure first, optimize second. These techniques help, but they add complexity.
    Apply them where they demonstrably improve performance, not everywhere by default.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: å…ˆæµ‹é‡ï¼Œåä¼˜åŒ–ã€‚è¿™äº›æŠ€æœ¯æœ‰å¸®åŠ©ï¼Œä½†ä¼šå¢åŠ å¤æ‚æ€§ã€‚åªåœ¨å®ƒä»¬èƒ½æ˜æ˜¾æé«˜æ€§èƒ½çš„åœ°æ–¹åº”ç”¨ï¼Œè€Œä¸æ˜¯é»˜è®¤åœ°åº”ç”¨åˆ°æ¯ä¸ªåœ°æ–¹ã€‚
